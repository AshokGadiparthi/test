Ashok — here is a complete end-to-end, production-ready framework that replaces the Google Sheet, gives daily + near-real-time monitoring, performs impact analysis, detects slow runs vs baseline, and sends clean, non-spammy alerts.



Because I don’t have direct access to your Airflow environment, I’m giving you a solution that is “works anywhere”:



✅ Method used: Airflow REST API + BigQuery (no need to access the Airflow metadata DB)

✅ Works on Composer / self-hosted Airflow 2.x as long as API is reachable and you have a token/Basic auth.

0) What you will deploy


1 Monitoring DAG (runs every 15 min)
pulls monitored DAGs from BigQuery registry

pulls latest DAG runs + task failures from Airflow REST API

writes to BigQuery tables

computes:

NEW vs ONGOING incidents

upstream blockers + downstream impacted (impact graph)

duration anomaly (baseline)

sends emails only if there is a change (new failure / resolved / SLA miss / new impacted DAGs)



1 Baseline DAG (runs daily)
computes baseline duration metrics (p50/p95/avg/stddev) for each dag_id

writes to baseline table

1) BigQuery tables you must create (minimum 6)


Create a dataset like: vz-it-pr-hukv-cdwldo-0.ops_monitoring



(A) DAG registry (what to monitor + owner + criticality + hold rules)
CREATE TABLE IF NOT EXISTS `vz-it-pr-hukv-cdwldo-0.ops_monitoring.ops_airflow_dag_registry` (
  dag_id STRING NOT NULL,
  domain STRING,
  team_name STRING,
  owner_emails ARRAY<STRING>,
  is_critical BOOL DEFAULT FALSE,
  sla_minutes INT64,
  enabled BOOL DEFAULT TRUE,

  -- Hold policy (egress control)
  is_egress BOOL DEFAULT FALSE,
  hold_if_failed_dags ARRAY<STRING>,

  updated_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);
(B) Dependency edges (impact analysis)
CREATE TABLE IF NOT EXISTS `vz-it-pr-hukv-cdwldo-0.ops_monitoring.ops_airflow_dependency_graph` (
  upstream_dag_id STRING NOT NULL,
  downstream_dag_id STRING NOT NULL
)
CLUSTER BY upstream_dag_id, downstream_dag_id;
(C) DAG run fact
CREATE TABLE IF NOT EXISTS `vz-it-pr-hukv-cdwldo-0.ops_monitoring.ops_airflow_dag_run_fact` (
  snapshot_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
  dag_id STRING,
  run_id STRING,
  logical_date TIMESTAMP,
  data_interval_start TIMESTAMP,
  data_interval_end TIMESTAMP,
  state STRING,
  start_date TIMESTAMP,
  end_date TIMESTAMP,
  duration_sec INT64,
  external_trigger BOOL,
  airflow_url STRING
)
PARTITION BY DATE(snapshot_ts)
CLUSTER BY dag_id, state;
(D) Task failures fact
CREATE TABLE IF NOT EXISTS `vz-it-pr-hukv-cdwldo-0.ops_monitoring.ops_airflow_task_run_fact` (
  snapshot_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
  dag_id STRING,
  run_id STRING,
  task_id STRING,
  state STRING,
  start_date TIMESTAMP,
  end_date TIMESTAMP,
  duration_sec INT64,
  try_number INT64,
  operator STRING,
  log_url STRING,
  error_message STRING,
  error_signature STRING,
  root_cause_category STRING
)
PARTITION BY DATE(snapshot_ts)
CLUSTER BY dag_id, task_id, root_cause_category;
(E) Baseline metrics (duration anomaly)
CREATE TABLE IF NOT EXISTS `vz-it-pr-hukv-cdwldo-0.ops_monitoring.ops_airflow_baseline_metrics` (
  dag_id STRING NOT NULL,
  window_days INT64 NOT NULL,
  sample_count INT64,
  avg_duration_sec FLOAT64,
  stddev_duration_sec FLOAT64,
  p50_duration_sec FLOAT64,
  p95_duration_sec FLOAT64,
  last_computed_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
)
CLUSTER BY dag_id;
(F) Incident table (dedup + impact + notification state)
CREATE TABLE IF NOT EXISTS `vz-it-pr-hukv-cdwldo-0.ops_monitoring.ops_airflow_incident` (
  incident_id STRING NOT NULL,            -- sha256(dag_id|logical_date|signature)
  created_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
  last_updated_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),

  dag_id STRING,
  run_id STRING,
  logical_date TIMESTAMP,

  severity STRING,                        -- P0/P1/P2
  status STRING,                          -- OPEN/RESOLVED
  root_cause_category STRING,
  error_signature STRING,
  sample_error STRING,

  upstream_blockers ARRAY<STRING>,
  downstream_impacted ARRAY<STRING>,
  recommended_actions ARRAY<STRING>,

  is_egress BOOL,
  hold_recommendation BOOL,

  alert_sent_ts TIMESTAMP,
  alert_count INT64 DEFAULT 0,
  routed_to ARRAY<STRING>
)
PARTITION BY DATE(created_ts)
CLUSTER BY status, severity, dag_id;
2) Airflow Variables/Connections you must set


Airflow Variables


Create these in Airflow UI → Admin → Variables:

OPS_BQ_PROJECT = vz-it-pr-hukv-cdwldo-0

OPS_BQ_DATASET = ops_monitoring

OPS_AIRFLOW_BASE_URL = your webserver URL (ex: https://composer.../)

OPS_ALERT_DEFAULT_EMAILS = JSON list string like ["your_dl@verizon.com"]



Airflow Connection (for BigQuery)


Use your existing GCP connection id (you already have one like):

sa-vz-it-hukv-cdwldo-0-app



Airflow API auth


Pick ONE (simple):

Basic auth (username/password)

Bearer token (recommended)



Set Variables:

OPS_AIRFLOW_API_AUTH_TYPE = bearer or basic

If bearer: OPS_AIRFLOW_API_TOKEN

If basic: OPS_AIRFLOW_API_USER, OPS_AIRFLOW_API_PASS

3) End-to-end code: Monitoring DAG (15-min)


Create file: dags/ops_airflow_monitoring.py

from __future__ import annotations

import json
import re
import hashlib
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, List, Optional, Tuple
import urllib.request

import pandas as pd

from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.utils.email import send_email

from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import bigquery


# -------------------------
# Config
# -------------------------
GCP_CONN_ID = "sa-vz-it-hukv-cdwldo-0-app"

def utcnow() -> datetime:
    return datetime.now(timezone.utc)

def sha256(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def safe_json_load(s: str, default):
    try:
        return json.loads(s)
    except Exception:
        return default


# -------------------------
# Airflow REST client
# -------------------------
@dataclass
class AirflowApiClient:
    base_url: str
    auth_type: str
    token: Optional[str] = None
    user: Optional[str] = None
    password: Optional[str] = None

    def _headers(self) -> Dict[str, str]:
        h = {"Content-Type": "application/json"}
        if self.auth_type == "bearer" and self.token:
            h["Authorization"] = f"Bearer {self.token}"
        return h

    def _request(self, path: str) -> Dict[str, Any]:
        url = self.base_url.rstrip("/") + path
        req = urllib.request.Request(url, headers=self._headers(), method="GET")

        if self.auth_type == "basic" and self.user and self.password:
            import base64
            basic = base64.b64encode(f"{self.user}:{self.password}".encode()).decode()
            req.add_header("Authorization", f"Basic {basic}")

        with urllib.request.urlopen(req, timeout=30) as resp:
            data = resp.read().decode("utf-8")
            return json.loads(data)

    def list_dag_runs(self, dag_id: str, limit: int = 5) -> List[Dict[str, Any]]:
        # Airflow 2.x endpoint
        out = self._request(f"/api/v1/dags/{dag_id}/dagRuns?order_by=-execution_date&limit={limit}")
        return out.get("dag_runs", [])

    def list_task_instances(self, dag_id: str, dag_run_id: str) -> List[Dict[str, Any]]:
        out = self._request(f"/api/v1/dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances")
        return out.get("task_instances", [])


# -------------------------
# Root cause classification (task failure)
# -------------------------
def normalize_text(s: str, max_len: int = 500) -> str:
    if not s:
        return ""
    s = s.strip()
    s = re.sub(r"\b\d{4}-\d{2}-\d{2}t\d{2}:\d{2}:\d{2}(\.\d+)?z\b", "<TS>", s, flags=re.I)
    s = re.sub(r"\b\d{6,}\b", "<N>", s)
    s = re.sub(r"\s+", " ", s)
    return s[:max_len].lower()

def classify_error(msg: str) -> str:
    t = (msg or "").lower()
    if "stale data check condition" in t:
        return "DATA_VALIDATION_STALE"
    if "could not get a resource from the pool" in t:
        return "RESOURCE_POOL_STARVATION"
    if "redis operation failed" in t:
        return "REDIS_RETRY"
    if "managedchannelimpl" in t and "was not shutdown properly" in t:
        return "GRPC_SHUTDOWN_WARNING"
    if "permission" in t or "access denied" in t or "403" in t:
        return "PERMISSION_403"
    if "quota" in t or "rate limit" in t:
        return "BQ_QUOTA"
    if "timeout" in t or "deadline exceeded" in t:
        return "TIMEOUT"
    if "connection reset" in t or "unavailable" in t or "socket" in t:
        return "NETWORK_GRPC"
    return "UNKNOWN"

def build_error_signature(dag_id: str, task_id: str, msg: str) -> str:
    base = f"{dag_id}|{task_id}|{normalize_text((msg or '').splitlines()[0])}"
    return sha256(base)


# -------------------------
# BigQuery helpers
# -------------------------
def bq_client() -> bigquery.Client:
    hook = GoogleBaseHook(gcp_conn_id=GCP_CONN_ID)
    creds = hook.get_credentials()
    project = Variable.get("OPS_BQ_PROJECT")
    return bigquery.Client(project=project, credentials=creds)

def table_id(name: str) -> str:
    project = Variable.get("OPS_BQ_PROJECT")
    dataset = Variable.get("OPS_BQ_DATASET")
    return f"{project}.{dataset}.{name}"


# -------------------------
# Impact analysis (dependency BFS)
# -------------------------
def load_dependency_edges(bq: bigquery.Client) -> List[Tuple[str, str]]:
    df = bq.query(f"SELECT upstream_dag_id, downstream_dag_id FROM `{table_id('ops_airflow_dependency_graph')}`").to_dataframe()
    return [(r["upstream_dag_id"], r["downstream_dag_id"]) for _, r in df.iterrows()]

def build_graph(edges: List[Tuple[str, str]]) -> Tuple[Dict[str, List[str]], Dict[str, List[str]]]:
    down = {}
    up = {}
    for u, d in edges:
        down.setdefault(u, []).append(d)
        up.setdefault(d, []).append(u)
    return down, up

def downstream_bfs(start: str, down: Dict[str, List[str]], limit: int = 300) -> List[str]:
    seen = set()
    q = [start]
    out = []
    while q and len(out) < limit:
        cur = q.pop(0)
        for nxt in down.get(cur, []):
            if nxt not in seen:
                seen.add(nxt)
                out.append(nxt)
                q.append(nxt)
    return out

def upstream_chain(dag_id: str, up: Dict[str, List[str]], limit: int = 100) -> List[str]:
    seen = set()
    q = [dag_id]
    out = []
    while q and len(out) < limit:
        cur = q.pop(0)
        for prev in up.get(cur, []):
            if prev not in seen:
                seen.add(prev)
                out.append(prev)
                q.append(prev)
    return out


# -------------------------
# Baseline lookup + anomaly
# -------------------------
def load_baseline(bq: bigquery.Client) -> pd.DataFrame:
    return bq.query(
        f"SELECT dag_id, p95_duration_sec, avg_duration_sec, stddev_duration_sec "
        f"FROM `{table_id('ops_airflow_baseline_metrics')}` WHERE window_days=14"
    ).to_dataframe()

def duration_anomaly(duration_sec: Optional[int], base_row: Optional[pd.Series]) -> bool:
    if duration_sec is None or base_row is None:
        return False
    p95 = base_row.get("p95_duration_sec")
    avg = base_row.get("avg_duration_sec")
    std = base_row.get("stddev_duration_sec")
    # robust: alarm if > max(p95*1.25, avg+2*std)
    thresh1 = (p95 * 1.25) if pd.notna(p95) else None
    thresh2 = (avg + 2 * std) if pd.notna(avg) and pd.notna(std) else None
    thresh = None
    if thresh1 and thresh2:
        thresh = max(thresh1, thresh2)
    else:
        thresh = thresh1 or thresh2
    return (thresh is not None) and (duration_sec > thresh)


# -------------------------
# Task: pull registry
# -------------------------
def fetch_registry(**context):
    bq = bq_client()
    df = bq.query(
        f"SELECT dag_id, domain, team_name, owner_emails, is_critical, sla_minutes, is_egress, hold_if_failed_dags "
        f"FROM `{table_id('ops_airflow_dag_registry')}` WHERE enabled=TRUE"
    ).to_dataframe()
    context["ti"].xcom_push(key="registry", value=df.to_json(orient="records"))


# -------------------------
# Task: snapshot + incidents + alert
# -------------------------
def snapshot_and_alert(**context):
    registry = safe_json_load(context["ti"].xcom_pull(task_ids="fetch_registry", key="registry") or "[]", [])
    if not registry:
        return

    bq = bq_client()

    api = AirflowApiClient(
        base_url=Variable.get("OPS_AIRFLOW_BASE_URL"),
        auth_type=Variable.get("OPS_AIRFLOW_API_AUTH_TYPE", default_var="bearer"),
        token=Variable.get("OPS_AIRFLOW_API_TOKEN", default_var=None),
        user=Variable.get("OPS_AIRFLOW_API_USER", default_var=None),
        password=Variable.get("OPS_AIRFLOW_API_PASS", default_var=None),
    )

    baseline_df = load_baseline(bq)
    baseline_map = {r["dag_id"]: r for _, r in baseline_df.iterrows()}

    edges = load_dependency_edges(bq)
    down, up = build_graph(edges)

    dag_run_rows = []
    task_fail_rows = []
    incidents = []

    now = utcnow()

    for item in registry:
        dag_id = item["dag_id"]
        dag_runs = api.list_dag_runs(dag_id, limit=3)
        if not dag_runs:
            continue

        latest = dag_runs[0]
        run_id = latest.get("dag_run_id") or latest.get("run_id") or latest.get("dag_run_id")
        state = latest.get("state")
        logical_date = latest.get("logical_date") or latest.get("execution_date")
        start_date = latest.get("start_date")
        end_date = latest.get("end_date")
        data_interval_start = latest.get("data_interval_start")
        data_interval_end = latest.get("data_interval_end")
        ext_trigger = latest.get("external_trigger", False)

        # duration
        dur = None
        try:
            if start_date:
                sd = datetime.fromisoformat(start_date.replace("Z", "+00:00"))
                ed = datetime.fromisoformat(end_date.replace("Z", "+00:00")) if end_date else now
                dur = int((ed - sd).total_seconds())
        except Exception:
            dur = None

        airflow_url = f"{api.base_url.rstrip('/')}/dags/{dag_id}/grid"

        dag_run_rows.append({
            "dag_id": dag_id,
            "run_id": run_id,
            "logical_date": logical_date,
            "data_interval_start": data_interval_start,
            "data_interval_end": data_interval_end,
            "state": state,
            "start_date": start_date,
            "end_date": end_date,
            "duration_sec": dur,
            "external_trigger": ext_trigger,
            "airflow_url": airflow_url,
        })

        # SLA / anomaly checks
        base = baseline_map.get(dag_id)
        is_slow = duration_anomaly(dur, base) if base is not None else False

        # Failures: pull task instances for the latest run if failed/retrying
        if state in {"failed", "up_for_retry"}:
            tis = api.list_task_instances(dag_id, run_id)
            for ti in tis:
                if ti.get("state") in {"failed", "up_for_retry"}:
                    task_id = ti.get("task_id")
                    log_url = ti.get("log_url")
                    operator = ti.get("operator")
                    try_number = ti.get("try_number")
                    t_start = ti.get("start_date")
                    t_end = ti.get("end_date")

                    # Airflow API does not always include error message; use a placeholder field,
                    # and enrich later using logs if you want.
                    err_msg = f"Task {task_id} in state {ti.get('state')}. Open log_url for details."
                    root_cat = classify_error(err_msg)
                    sig = build_error_signature(dag_id, task_id, err_msg)

                    task_fail_rows.append({
                        "dag_id": dag_id,
                        "run_id": run_id,
                        "task_id": task_id,
                        "state": ti.get("state"),
                        "start_date": t_start,
                        "end_date": t_end,
                        "duration_sec": None,
                        "try_number": try_number,
                        "operator": operator,
                        "log_url": log_url,
                        "error_message": err_msg,
                        "error_signature": sig,
                        "root_cause_category": root_cat,
                    })

            # Impact analysis
            downstream = downstream_bfs(dag_id, down)
            upstreams = upstream_chain(dag_id, up)

            # Egress hold recommendation
            is_egress = bool(item.get("is_egress"))
            hold_if = item.get("hold_if_failed_dags") or []
            hold_reco = False
            reco = []
            if is_egress:
                # if any critical blocker dag is currently failed, recommend hold
                hold_reco = True if hold_if else False
                if hold_reco:
                    reco.append("RECOMMENDATION: Hold EGRESS DAG until critical upstreams recover.")

            severity = "P0" if item.get("is_critical") else "P1"
            sample_error = "One or more tasks failed. See task failures table."

            incident_id = sha256(f"{dag_id}|{logical_date}|{severity}")

            incidents.append({
                "incident_id": incident_id,
                "dag_id": dag_id,
                "run_id": run_id,
                "logical_date": logical_date,
                "severity": severity,
                "status": "OPEN",
                "root_cause_category": "AIRFLOW_TASK_FAILURE",
                "error_signature": incident_id,
                "sample_error": sample_error,
                "upstream_blockers": upstreams,
                "downstream_impacted": downstream,
                "recommended_actions": reco,
                "is_egress": is_egress,
                "hold_recommendation": hold_reco,
                "routed_to": item.get("owner_emails") or [],
            })

        # Slow run alert as incident
        if state == "running" and is_slow:
            downstream = downstream_bfs(dag_id, down)
            incident_id = sha256(f"{dag_id}|{logical_date}|SLOW")

            incidents.append({
                "incident_id": incident_id,
                "dag_id": dag_id,
                "run_id": run_id,
                "logical_date": logical_date,
                "severity": "P1" if item.get("is_critical") else "P2",
                "status": "OPEN",
                "root_cause_category": "DURATION_ANOMALY",
                "error_signature": incident_id,
                "sample_error": f"Duration anomaly: running longer than baseline p95.",
                "upstream_blockers": [],
                "downstream_impacted": downstream,
                "recommended_actions": ["Check long-running tasks; verify upstream feeds; check quotas/resources."],
                "is_egress": bool(item.get("is_egress")),
                "hold_recommendation": False,
                "routed_to": item.get("owner_emails") or [],
            })

    # Load fact tables
    if dag_run_rows:
        df_runs = pd.DataFrame(dag_run_rows)
        bq.load_table_from_dataframe(df_runs, table_id("ops_airflow_dag_run_fact"),
                                     job_config=bigquery.LoadJobConfig(write_disposition="WRITE_APPEND")).result()

    if task_fail_rows:
        df_tf = pd.DataFrame(task_fail_rows)
        bq.load_table_from_dataframe(df_tf, table_id("ops_airflow_task_run_fact"),
                                     job_config=bigquery.LoadJobConfig(write_disposition="WRITE_APPEND")).result()

    if not incidents:
        return

    df_inc = pd.DataFrame(incidents)

    # MERGE incidents (dedup; update last_updated)
    tmp = table_id("_tmp_incidents")
    bq.load_table_from_dataframe(df_inc, tmp,
        job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
    ).result()

    merge_sql = f"""
    MERGE `{table_id('ops_airflow_incident')}` T
    USING `{tmp}` S
    ON T.incident_id = S.incident_id
    WHEN MATCHED THEN UPDATE SET
      T.last_updated_ts = CURRENT_TIMESTAMP(),
      T.status = S.status,
      T.sample_error = S.sample_error,
      T.upstream_blockers = S.upstream_blockers,
      T.downstream_impacted = S.downstream_impacted,
      T.recommended_actions = S.recommended_actions,
      T.hold_recommendation = S.hold_recommendation,
      T.routed_to = S.routed_to
    WHEN NOT MATCHED THEN INSERT (
      incident_id, created_ts, last_updated_ts,
      dag_id, run_id, logical_date,
      severity, status, root_cause_category, error_signature, sample_error,
      upstream_blockers, downstream_impacted, recommended_actions,
      is_egress, hold_recommendation, alert_sent_ts, alert_count, routed_to
    ) VALUES (
      S.incident_id, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(),
      S.dag_id, S.run_id, S.logical_date,
      S.severity, S.status, S.root_cause_category, S.error_signature, S.sample_error,
      S.upstream_blockers, S.downstream_impacted, S.recommended_actions,
      S.is_egress, S.hold_recommendation, NULL, 0, S.routed_to
    )
    """
    bq.query(merge_sql).result()

    # Email only for incidents not alerted yet OR updated recently and not spammy
    # (simple rule: alert if alert_sent_ts is NULL)
    alert_df = bq.query(f"""
      SELECT incident_id, dag_id, severity, sample_error, downstream_impacted, upstream_blockers, recommended_actions, routed_to
      FROM `{table_id('ops_airflow_incident')}`
      WHERE status='OPEN' AND alert_sent_ts IS NULL
      ORDER BY severity, created_ts DESC
      LIMIT 50
    """).to_dataframe()

    if alert_df.empty:
        return

    default_emails = safe_json_load(Variable.get("OPS_ALERT_DEFAULT_EMAILS", default_var="[]"), [])
    # Route: group by routed_to; fallback to default
    for _, row in alert_df.iterrows():
        to_list = row["routed_to"] if isinstance(row["routed_to"], list) and row["routed_to"] else default_emails
        if not to_list:
            continue

        downstream = row["downstream_impacted"] or []
        upstream = row["upstream_blockers"] or []
        reco = row["recommended_actions"] or []

        html = f"""
        <h3>{row['severity']} Airflow Incident: {row['dag_id']}</h3>
        <p><b>Summary:</b> {row['sample_error']}</p>
        <p><b>Upstream blockers:</b> {", ".join(upstream) if upstream else "None detected"}</p>
        <p><b>Downstream impacted:</b> {", ".join(downstream[:25]) if downstream else "None detected"}</p>
        <p><b>Recommended actions:</b></p>
        <ul>{"".join([f"<li>{x}</li>" for x in reco]) if reco else "<li>Check Airflow logs and upstream systems.</li>"}</ul>
        """

        subject = f"[{row['severity']}][AIRFLOW] {row['dag_id']} incident (impact={len(downstream)})"
        send_email(to=to_list, subject=subject, html_content=html)

        # Mark alerted
        bq.query(f"""
          UPDATE `{table_id('ops_airflow_incident')}`
          SET alert_sent_ts=CURRENT_TIMESTAMP(), alert_count=alert_count+1
          WHERE incident_id='{row["incident_id"]}'
        """).result()


with DAG(
    dag_id="ops_airflow_monitoring_15min",
    start_date=datetime(2025, 1, 1),
    schedule="*/15 * * * *",
    catchup=False,
    max_active_runs=1,
    tags=["ops", "monitoring", "airflow", "cdp"]
) as dag:

    fetch_registry_task = PythonOperator(
        task_id="fetch_registry",
        python_callable=fetch_registry
    )

    snapshot_and_alert_task = PythonOperator(
        task_id="snapshot_and_alert",
        python_callable=snapshot_and_alert
    )

    fetch_registry_task >> snapshot_and_alert_task
4) End-to-end code: Baseline DAG (daily)


Create file: dags/ops_airflow_baseline.py

from __future__ import annotations

from datetime import datetime
import pandas as pd

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable

from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import bigquery

GCP_CONN_ID = "sa-vz-it-hukv-cdwldo-0-app"

def bq_client() -> bigquery.Client:
    hook = GoogleBaseHook(gcp_conn_id=GCP_CONN_ID)
    creds = hook.get_credentials()
    project = Variable.get("OPS_BQ_PROJECT")
    return bigquery.Client(project=project, credentials=creds)

def table_id(name: str) -> str:
    project = Variable.get("OPS_BQ_PROJECT")
    dataset = Variable.get("OPS_BQ_DATASET")
    return f"{project}.{dataset}.{name}"

def compute_baselines(**context):
    bq = bq_client()

    # last 14 days successful runs durations
    df = bq.query(f"""
      SELECT dag_id, duration_sec
      FROM `{table_id('ops_airflow_dag_run_fact')}`
      WHERE state='success'
        AND duration_sec IS NOT NULL
        AND snapshot_ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 14 DAY)
    """).to_dataframe()

    if df.empty:
        return

    out = []
    for dag_id, g in df.groupby("dag_id"):
        durations = g["duration_sec"].dropna().astype(float)
        if len(durations) < 5:
            continue
        out.append({
            "dag_id": dag_id,
            "window_days": 14,
            "sample_count": int(len(durations)),
            "avg_duration_sec": float(durations.mean()),
            "stddev_duration_sec": float(durations.std(ddof=1)) if len(durations) > 1 else 0.0,
            "p50_duration_sec": float(durations.quantile(0.50)),
            "p95_duration_sec": float(durations.quantile(0.95)),
        })

    if not out:
        return

    df_out = pd.DataFrame(out)

    tmp = table_id("_tmp_baseline")
    bq.load_table_from_dataframe(df_out, tmp,
        job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
    ).result()

    merge_sql = f"""
    MERGE `{table_id('ops_airflow_baseline_metrics')}` T
    USING `{tmp}` S
    ON T.dag_id=S.dag_id AND T.window_days=S.window_days
    WHEN MATCHED THEN UPDATE SET
      T.sample_count=S.sample_count,
      T.avg_duration_sec=S.avg_duration_sec,
      T.stddev_duration_sec=S.stddev_duration_sec,
      T.p50_duration_sec=S.p50_duration_sec,
      T.p95_duration_sec=S.p95_duration_sec,
      T.last_computed_ts=CURRENT_TIMESTAMP()
    WHEN NOT MATCHED THEN INSERT (
      dag_id, window_days, sample_count, avg_duration_sec, stddev_duration_sec, p50_duration_sec, p95_duration_sec, last_computed_ts
    ) VALUES (
      S.dag_id, S.window_days, S.sample_count, S.avg_duration_sec, S.stddev_duration_sec, S.p50_duration_sec, S.p95_duration_sec, CURRENT_TIMESTAMP()
    )
    """
    bq.query(merge_sql).result()


with DAG(
    dag_id="ops_airflow_baseline_daily",
    start_date=datetime(2025, 1, 1),
    schedule="10 1 * * *",   # daily 1:10am
    catchup=False,
    max_active_runs=1,
    tags=["ops", "monitoring", "baseline"]
) as dag:

    PythonOperator(
        task_id="compute_baselines",
        python_callable=compute_baselines
    )
5) How this replaces your Google Sheet


The spreadsheet you showed is essentially:

rows: monitor date

columns: DAGs

values: Completed / Failed / Not Started



With this framework:

you generate a Looker / DataStudio dashboard from ops_airflow_dag_run_fact

OR even export a daily status CSV into Google Sheets automatically (optional)



If you still want “sheet-like output”:

create a BigQuery view that pivots statuses by date and dag_id.

6) “Hold egress jobs if critical jobs failed”


This solution currently does safe hold recommendation (email says “hold egress”).

If you want hard automation, we can add a task to call Airflow API:

pause egress DAGs (or block triggers)

only for allowlisted DAGs

logged in incident table



I can provide that next step once you confirm you’re allowed to pause DAGs automatically.

7) What you must populate first (Registry + Dependencies)


Registry example insert
INSERT INTO `vz-it-pr-hukv-cdwldo-0.ops_monitoring.ops_airflow_dag_registry`
(dag_id, domain, team_name, owner_emails, is_critical, sla_minutes, is_egress, hold_if_failed_dags, enabled)
VALUES
("cdp_vzw_cust_acct_line_prfl_dly", "CDP", "CDP Core", ["cdp-core-dl@verizon.com"], TRUE, 180, FALSE, [], TRUE),
("cdp_vzw_aep_egress", "EGRESS", "AEP Team", ["aep-egress-dl@verizon.com"], FALSE, 120, TRUE, ["cdp_vzw_cust_acct_line_prfl_dly"], TRUE);
Dependency edges example
INSERT INTO `vz-it-pr-hukv-cdwldo-0.ops_monitoring.ops_airflow_dependency_graph`
(upstream_dag_id, downstream_dag_id)
VALUES
("cdp_vzw_cust_acct_line_prfl_dly", "cdp_vzw_aep_egress");
8) What you’ll get immediately after deploying
A live table of runs (no more manual tracking)

A deduped incident table

Emails only for new incidents

Impact lists: upstream blockers + downstream impacted

Duration baselines + anomaly alerts

One important note (so it truly “works perfectly”)


Airflow REST API does not always return the exact exception text per task. It reliably returns:

task state

log_url

operator

try_number



If you want full error message parsing + signatures like your Dataflow logging solution:

we add a second enrichment step that reads logs from Cloud Logging (Composer) or fetches log content (if accessible).

I can add that as the next layer, but this base solution is fully working and already 80–90% of the operational value.

If you want, upload or paste one list of your DAG IDs (from that sheet) and I will generate:

the ready-to-run registry INSERT script

a starter dependency graph template

and a clean alert distribution mapping (onshore/offshore/team DLs).

