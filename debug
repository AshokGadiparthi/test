Perfect, Ashok. Below is 100% end-to-end code with hardcoded config inside code (no Google Sheet, no manual registry maintenance).



This gives you:



✅ Auto-discover ALL DAGs daily (and store tags/owners/is_paused into BigQuery)

✅ Monitor every 15 minutes (writes run facts + task failures + dedup incidents + impact best-effort + alerts)

✅ Hourly digest email (clean consolidated table)

✅ Daily baseline (p50/p95/avg/std for duration anomaly)



You only need to:

paste these files into dags/

update a few constants at top (project/dataset/base_url/auth/emails)

ensure the Airflow service account has BigQuery write permissions (DataEditor + JobUser on the dataset)

0) One-time: BigQuery dataset + permissions


Create dataset (if not existing): ops_monitoring in your project.



Permissions needed for the Composer/Airflow SA:

roles/bigquery.dataEditor on dataset

roles/bigquery.jobUser on project (or dataset, depending on org policy)

1) File 1/4 — Daily registry sync (discovers ALL DAGs)


dags/ops_registry_sync_daily.py

from __future__ import annotations

import json
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional
import urllib.request
import pandas as pd

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.common.hooks.base_google import GoogleBaseHook

from google.cloud import bigquery


# =========================
# HARD-CODED CONFIG (EDIT THESE)
# =========================
GCP_CONN_ID = "sa-vz-it-hukv-cdwldo-0-app"
BQ_PROJECT = "vz-it-pr-hukv-cdwldo-0"
BQ_DATASET = "ops_monitoring"

AIRFLOW_BASE_URL = "https://YOUR-AIRFLOW-WEBSERVER-URL"  # composer webserver base
AIRFLOW_AUTH_TYPE = "bearer"  # "bearer" or "basic" or "none"
AIRFLOW_BEARER_TOKEN = "PASTE_TOKEN_HERE"  # if bearer
AIRFLOW_BASIC_USER = ""
AIRFLOW_BASIC_PASS = ""

# Monitor ALL dags. If you want only specific tags, set TAG_FILTER = ["cdp","cassandra"]
TAG_FILTER: List[str] = []  # [] means ALL dags

# Default ownership routing if owner not present
DEFAULT_OWNER_EMAILS = ["cdp-platform-dl@yourcompany.com"]


# =========================
# Helpers
# =========================
def utcnow() -> datetime:
    return datetime.now(timezone.utc)

def table_id(name: str) -> str:
    return f"{BQ_PROJECT}.{BQ_DATASET}.{name}"

def bq_client() -> bigquery.Client:
    hook = GoogleBaseHook(gcp_conn_id=GCP_CONN_ID)
    creds = hook.get_credentials()
    return bigquery.Client(project=BQ_PROJECT, credentials=creds)

def airflow_headers() -> Dict[str, str]:
    h = {"Content-Type": "application/json"}
    if AIRFLOW_AUTH_TYPE == "bearer" and AIRFLOW_BEARER_TOKEN:
        h["Authorization"] = f"Bearer {AIRFLOW_BEARER_TOKEN}"
    return h

def airflow_get(path: str) -> Dict[str, Any]:
    url = AIRFLOW_BASE_URL.rstrip("/") + path
    req = urllib.request.Request(url, headers=airflow_headers(), method="GET")

    if AIRFLOW_AUTH_TYPE == "basic" and AIRFLOW_BASIC_USER and AIRFLOW_BASIC_PASS:
        import base64
        basic = base64.b64encode(f"{AIRFLOW_BASIC_USER}:{AIRFLOW_BASIC_PASS}".encode()).decode()
        req.add_header("Authorization", f"Basic {basic}")

    with urllib.request.urlopen(req, timeout=30) as resp:
        return json.loads(resp.read().decode("utf-8"))

def ensure_tables(**context):
    bq = bq_client()

    # Registry table stores tags + paused status
    bq.query(f"""
    CREATE TABLE IF NOT EXISTS `{table_id('ops_airflow_dag_registry')}` (
      dag_id STRING NOT NULL,
      airflow_tags ARRAY<STRING>,
      owners ARRAY<STRING>,
      is_paused BOOL,
      enabled BOOL DEFAULT TRUE,

      -- You can enrich later; for now defaults are safe
      domain STRING,
      team_name STRING,
      owner_emails ARRAY<STRING>,
      is_critical BOOL DEFAULT FALSE,
      sla_minutes INT64,
      is_egress BOOL DEFAULT FALSE,
      hold_if_failed_dags ARRAY<STRING>,

      updated_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
    )
    """).result()

    bq.query(f"""
    CREATE TABLE IF NOT EXISTS `{table_id('ops_airflow_dependency_graph')}` (
      upstream_dag_id STRING NOT NULL,
      downstream_dag_id STRING NOT NULL
    )
    CLUSTER BY upstream_dag_id, downstream_dag_id
    """).result()

def sync_registry(**context):
    bq = bq_client()

    # Airflow list dags (paginate)
    all_rows: List[Dict[str, Any]] = []
    limit = 200
    offset = 0

    while True:
        resp = airflow_get(f"/api/v1/dags?limit={limit}&offset={offset}")
        dags = resp.get("dags", []) or []
        if not dags:
            break

        for d in dags:
            dag_id = d.get("dag_id")
            tags = [t.get("name") for t in (d.get("tags") or []) if t and t.get("name")]
            owners = d.get("owners") or []
            is_paused = bool(d.get("is_paused", False))

            # If filter configured: keep only matching tags
            if TAG_FILTER:
                if not any(t.lower() in [x.lower() for x in TAG_FILTER] for t in tags):
                    continue

            all_rows.append({
                "dag_id": dag_id,
                "airflow_tags": tags,
                "owners": owners,
                "is_paused": is_paused,
                "enabled": True
            })

        if len(dags) < limit:
            break
        offset += limit

    if not all_rows:
        raise ValueError("No DAGs returned from Airflow API (or tag filter excluded all). Check AIRFLOW_BASE_URL/auth/TAG_FILTER.")

    df = pd.DataFrame(all_rows)

    # stage -> merge
    tmp = table_id("_tmp_registry_sync")
    bq.load_table_from_dataframe(
        df, tmp,
        job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
    ).result()

    merge_sql = f"""
    MERGE `{table_id('ops_airflow_dag_registry')}` T
    USING `{tmp}` S
    ON T.dag_id = S.dag_id
    WHEN MATCHED THEN UPDATE SET
      T.airflow_tags = S.airflow_tags,
      T.owners = S.owners,
      T.is_paused = S.is_paused,
      T.enabled = S.enabled,
      -- keep enrichable fields if already present:
      T.updated_ts = CURRENT_TIMESTAMP()
    WHEN NOT MATCHED THEN
      INSERT (dag_id, airflow_tags, owners, is_paused, enabled, owner_emails, updated_ts)
      VALUES (S.dag_id, S.airflow_tags, S.owners, S.is_paused, S.enabled, [], CURRENT_TIMESTAMP())
    """
    bq.query(merge_sql).result()

    # Optional: if owner_emails is empty, auto-fill from owners or default
    # (top-quality routing: always route somewhere)
    bq.query(f"""
    UPDATE `{table_id('ops_airflow_dag_registry')}`
    SET owner_emails = IF(ARRAY_LENGTH(owners)>0, owners, @default_emails),
        updated_ts = CURRENT_TIMESTAMP()
    WHERE (owner_emails IS NULL OR ARRAY_LENGTH(owner_emails)=0)
    """, job_config=bigquery.QueryJobConfig(
        query_parameters=[bigquery.ArrayQueryParameter("default_emails", "STRING", DEFAULT_OWNER_EMAILS)]
    )).result()


with DAG(
    dag_id="ops_registry_sync_daily",
    start_date=datetime(2025, 1, 1),
    schedule="0 5 * * *",  # daily 5:00am
    catchup=False,
    max_active_runs=1,
    tags=["ops", "registry", "monitoring"]
) as dag:
    PythonOperator(task_id="ensure_tables", python_callable=ensure_tables)
    PythonOperator(task_id="sync_registry", python_callable=sync_registry)
2) File 2/4 — 15-min Monitoring (facts + incidents + alert)


dags/ops_monitoring_15min.py

from __future__ import annotations

import json
import re
import hashlib
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple
import urllib.request
import pandas as pd

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.email import send_email
from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import bigquery


# =========================
# HARD-CODED CONFIG (EDIT THESE)
# =========================
GCP_CONN_ID = "sa-vz-it-hukv-cdwldo-0-app"
BQ_PROJECT = "vz-it-pr-hukv-cdwldo-0"
BQ_DATASET = "ops_monitoring"

AIRFLOW_BASE_URL = "https://YOUR-AIRFLOW-WEBSERVER-URL"
AIRFLOW_AUTH_TYPE = "bearer"
AIRFLOW_BEARER_TOKEN = "PASTE_TOKEN_HERE"
AIRFLOW_BASIC_USER = ""
AIRFLOW_BASIC_PASS = ""

# Monitor ALL enabled dags from registry. If you want tag-based monitor, set MONITOR_TAGS
MONITOR_TAGS: List[str] = []  # [] means ALL

DEFAULT_ALERT_EMAILS = ["cdp-platform-dl@yourcompany.com"]

# How many recent runs to inspect per dag
DAGRUN_LOOKBACK = 2


# =========================
# Helpers
# =========================
def utcnow() -> datetime:
    return datetime.now(timezone.utc)

def table_id(name: str) -> str:
    return f"{BQ_PROJECT}.{BQ_DATASET}.{name}"

def bq_client() -> bigquery.Client:
    hook = GoogleBaseHook(gcp_conn_id=GCP_CONN_ID)
    creds = hook.get_credentials()
    return bigquery.Client(project=BQ_PROJECT, credentials=creds)

def airflow_headers() -> Dict[str, str]:
    h = {"Content-Type": "application/json"}
    if AIRFLOW_AUTH_TYPE == "bearer" and AIRFLOW_BEARER_TOKEN:
        h["Authorization"] = f"Bearer {AIRFLOW_BEARER_TOKEN}"
    return h

def airflow_get(path: str) -> Dict[str, Any]:
    url = AIRFLOW_BASE_URL.rstrip("/") + path
    req = urllib.request.Request(url, headers=airflow_headers(), method="GET")

    if AIRFLOW_AUTH_TYPE == "basic" and AIRFLOW_BASIC_USER and AIRFLOW_BASIC_PASS:
        import base64
        basic = base64.b64encode(f"{AIRFLOW_BASIC_USER}:{AIRFLOW_BASIC_PASS}".encode()).decode()
        req.add_header("Authorization", f"Basic {basic}")

    with urllib.request.urlopen(req, timeout=30) as resp:
        return json.loads(resp.read().decode("utf-8"))


# --- signature + classification (dedupe) ---
_space = re.compile(r"\s+")
_num_noise = re.compile(r"\b\d{6,}\b")

def norm(s: str) -> str:
    if not s:
        return ""
    s = s.strip()
    s = _num_noise.sub("<N>", s)
    s = _space.sub(" ", s)
    return s[:300].lower()

def sha256(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def classify(msg: str) -> str:
    t = (msg or "").lower()
    if "permission" in t or "access denied" in t or "403" in t:
        return "PERMISSION_403"
    if "quota" in t or "rate limit" in t:
        return "BQ_QUOTA"
    if "timeout" in t or "deadline exceeded" in t:
        return "TIMEOUT"
    if "connection reset" in t or "unavailable" in t or "socket" in t:
        return "NETWORK"
    if "resource from the pool" in t:
        return "RESOURCE_POOL"
    if "stale data check condition" in t:
        return "DATA_STALE"
    return "UNKNOWN"

def severity(is_critical: bool, category: str) -> str:
    if is_critical:
        return "P0"
    if category in {"PERMISSION_403", "BQ_QUOTA", "RESOURCE_POOL", "DATA_STALE"}:
        return "P1"
    if category in {"TIMEOUT", "NETWORK"}:
        return "P2"
    return "P3"


# --- dependency graph impact (works even if empty graph) ---
def load_edges(bq: bigquery.Client) -> List[Tuple[str, str]]:
    df = bq.query(f"SELECT upstream_dag_id, downstream_dag_id FROM `{table_id('ops_airflow_dependency_graph')}`").to_dataframe()
    return [(r["upstream_dag_id"], r["downstream_dag_id"]) for _, r in df.iterrows()]

def build_graph(edges: List[Tuple[str, str]]) -> Tuple[Dict[str, List[str]], Dict[str, List[str]]]:
    down, up = {}, {}
    for u, d in edges:
        down.setdefault(u, []).append(d)
        up.setdefault(d, []).append(u)
    return down, up

def bfs(start: str, g: Dict[str, List[str]], limit: int = 300) -> List[str]:
    seen = set([start])
    q = [start]
    out = []
    while q and len(out) < limit:
        cur = q.pop(0)
        for nxt in g.get(cur, []):
            if nxt not in seen:
                seen.add(nxt)
                out.append(nxt)
                q.append(nxt)
    return out


def ensure_tables(**context):
    bq = bq_client()

    bq.query(f"""
    CREATE TABLE IF NOT EXISTS `{table_id('ops_airflow_dag_run_fact')}` (
      snapshot_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
      dag_id STRING,
      run_id STRING,
      logical_date TIMESTAMP,
      data_interval_start TIMESTAMP,
      data_interval_end TIMESTAMP,
      state STRING,
      start_date TIMESTAMP,
      end_date TIMESTAMP,
      duration_sec INT64,
      external_trigger BOOL,
      airflow_url STRING
    )
    PARTITION BY DATE(snapshot_ts)
    CLUSTER BY dag_id, state
    """).result()

    bq.query(f"""
    CREATE TABLE IF NOT EXISTS `{table_id('ops_airflow_task_run_fact')}` (
      snapshot_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
      dag_id STRING,
      run_id STRING,
      task_id STRING,
      state STRING,
      start_date TIMESTAMP,
      end_date TIMESTAMP,
      duration_sec INT64,
      try_number INT64,
      operator STRING,
      log_url STRING,
      error_message STRING,
      error_signature STRING,
      root_cause_category STRING
    )
    PARTITION BY DATE(snapshot_ts)
    CLUSTER BY dag_id, task_id, root_cause_category
    """).result()

    bq.query(f"""
    CREATE TABLE IF NOT EXISTS `{table_id('ops_airflow_incident')}` (
      incident_id STRING NOT NULL,
      created_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
      last_updated_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
      dag_id STRING,
      run_id STRING,
      logical_date TIMESTAMP,
      severity STRING,
      status STRING,
      root_cause_category STRING,
      error_signature STRING,
      sample_error STRING,
      upstream_blockers ARRAY<STRING>,
      downstream_impacted ARRAY<STRING>,
      recommended_actions ARRAY<STRING>,
      alert_sent_ts TIMESTAMP,
      alert_count INT64 DEFAULT 0,
      routed_to ARRAY<STRING>
    )
    PARTITION BY DATE(created_ts)
    CLUSTER BY status, severity, dag_id
    """).result()


def monitor(**context):
    bq = bq_client()

    # Read registry (ALL dags, optionally filter by tags)
    reg_df = bq.query(f"""
      SELECT dag_id, airflow_tags, owner_emails, is_critical, enabled
      FROM `{table_id('ops_airflow_dag_registry')}`
      WHERE enabled=TRUE
    """).to_dataframe()

    if reg_df.empty:
        raise ValueError("Registry is empty. Run ops_registry_sync_daily first.")

    if MONITOR_TAGS:
        mt = set([t.lower() for t in MONITOR_TAGS])
        reg_df = reg_df[reg_df["airflow_tags"].apply(lambda arr: any((x or "").lower() in mt for x in (arr or [])))]
        if reg_df.empty:
            raise ValueError("After MONITOR_TAGS filtering, registry has 0 DAGs.")

    edges = load_edges(bq)
    down, up = build_graph(edges)

    now = utcnow()

    dag_run_rows = []
    task_rows = []
    incident_rows = []

    for _, r in reg_df.iterrows():
        dag_id = r["dag_id"]
        owner_emails = r["owner_emails"] if isinstance(r["owner_emails"], list) and r["owner_emails"] else DEFAULT_ALERT_EMAILS
        is_critical = bool(r.get("is_critical", False))

        # latest run(s)
        resp = airflow_get(f"/api/v1/dags/{dag_id}/dagRuns?order_by=-execution_date&limit={DAGRUN_LOOKBACK}")
        runs = resp.get("dag_runs", []) or []
        if not runs:
            continue

        latest = runs[0]
        run_id = latest.get("dag_run_id") or latest.get("run_id")
        state = (latest.get("state") or "").lower()
        logical_date = latest.get("logical_date") or latest.get("execution_date")
        dis = latest.get("data_interval_start")
        die = latest.get("data_interval_end")
        start_date = latest.get("start_date")
        end_date = latest.get("end_date")
        external_trigger = bool(latest.get("external_trigger", False))
        airflow_url = f"{AIRFLOW_BASE_URL.rstrip('/')}/dags/{dag_id}/grid"

        duration_sec = None
        try:
            if start_date:
                sd = datetime.fromisoformat(start_date.replace("Z", "+00:00"))
                ed = datetime.fromisoformat(end_date.replace("Z", "+00:00")) if end_date else now
                duration_sec = int((ed - sd).total_seconds())
        except Exception:
            duration_sec = None

        dag_run_rows.append({
            "dag_id": dag_id,
            "run_id": run_id,
            "logical_date": logical_date,
            "data_interval_start": dis,
            "data_interval_end": die,
            "state": state,
            "start_date": start_date,
            "end_date": end_date,
            "duration_sec": duration_sec,
            "external_trigger": external_trigger,
            "airflow_url": airflow_url
        })

        # If failed: capture failing tasks (Airflow API provides log_url; exception text may not be present)
        if state in {"failed", "up_for_retry"}:
            ti_resp = airflow_get(f"/api/v1/dags/{dag_id}/dagRuns/{run_id}/taskInstances")
            tis = ti_resp.get("task_instances", []) or []
            failing = [ti for ti in tis if (ti.get("state") or "").lower() in {"failed", "up_for_retry"}]

            # Create a stable signature for incident using first failing task
            if failing:
                first = failing[0]
                task_id = first.get("task_id")
                log_url = first.get("log_url")
                msg = f"Task '{task_id}' is {first.get('state')}. Open log_url for details."
            else:
                task_id = "UNKNOWN_TASK"
                log_url = ""
                msg = f"DAG '{dag_id}' is {state}. No failing tasks returned by API."

            cat = classify(msg)
            sig = sha256(f"{dag_id}|{task_id}|{norm(msg)}")
            inc_id = sha256(f"{dag_id}|{logical_date}|{sig}")

            # Store task failure rows
            for ti in failing:
                tmsg = f"Task '{ti.get('task_id')}' is {ti.get('state')}. Open log_url for details."
                tcat = classify(tmsg)
                tsig = sha256(f"{dag_id}|{ti.get('task_id')}|{norm(tmsg)}")

                task_rows.append({
                    "dag_id": dag_id,
                    "run_id": run_id,
                    "task_id": ti.get("task_id"),
                    "state": (ti.get("state") or "").lower(),
                    "start_date": ti.get("start_date"),
                    "end_date": ti.get("end_date"),
                    "duration_sec": None,
                    "try_number": ti.get("try_number"),
                    "operator": ti.get("operator"),
                    "log_url": ti.get("log_url"),
                    "error_message": tmsg,
                    "error_signature": tsig,
                    "root_cause_category": tcat
                })

            # Impact analysis: best-effort from dependency graph (if edges exist)
            downstream = bfs(dag_id, down)
            upstreams = bfs(dag_id, up)

            incident_rows.append({
                "incident_id": inc_id,
                "dag_id": dag_id,
                "run_id": run_id,
                "logical_date": logical_date,
                "severity": severity(is_critical, cat),
                "status": "OPEN",
                "root_cause_category": cat,
                "error_signature": sig,
                "sample_error": msg,
                "upstream_blockers": upstreams,
                "downstream_impacted": downstream,
                "recommended_actions": ["Open task log_url, fix root cause, rerun the DAG."],
                "routed_to": owner_emails
            })

    # Write facts
    if dag_run_rows:
        bq.load_table_from_dataframe(
            pd.DataFrame(dag_run_rows),
            table_id("ops_airflow_dag_run_fact"),
            job_config=bigquery.LoadJobConfig(write_disposition="WRITE_APPEND")
        ).result()

    if task_rows:
        bq.load_table_from_dataframe(
            pd.DataFrame(task_rows),
            table_id("ops_airflow_task_run_fact"),
            job_config=bigquery.LoadJobConfig(write_disposition="WRITE_APPEND")
        ).result()

    # Merge incidents (dedupe)
    if incident_rows:
        tmp = table_id("_tmp_inc_15min")
        bq.load_table_from_dataframe(
            pd.DataFrame(incident_rows),
            tmp,
            job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
        ).result()

        bq.query(f"""
        MERGE `{table_id('ops_airflow_incident')}` T
        USING `{tmp}` S
        ON T.incident_id = S.incident_id
        WHEN MATCHED THEN UPDATE SET
          T.last_updated_ts = CURRENT_TIMESTAMP(),
          T.status = S.status,
          T.severity = S.severity,
          T.root_cause_category = S.root_cause_category,
          T.sample_error = S.sample_error,
          T.upstream_blockers = S.upstream_blockers,
          T.downstream_impacted = S.downstream_impacted,
          T.recommended_actions = S.recommended_actions,
          T.routed_to = S.routed_to
        WHEN NOT MATCHED THEN INSERT (
          incident_id, created_ts, last_updated_ts,
          dag_id, run_id, logical_date,
          severity, status, root_cause_category, error_signature, sample_error,
          upstream_blockers, downstream_impacted, recommended_actions,
          alert_sent_ts, alert_count, routed_to
        ) VALUES (
          S.incident_id, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(),
          S.dag_id, S.run_id, S.logical_date,
          S.severity, S.status, S.root_cause_category, S.error_signature, S.sample_error,
          S.upstream_blockers, S.downstream_impacted, S.recommended_actions,
          NULL, 0, S.routed_to
        )
        """).result()

    # Alert only new incidents (alert_sent_ts is NULL)
    pending = bq.query(f"""
      SELECT incident_id, dag_id, severity, root_cause_category, sample_error,
             upstream_blockers, downstream_impacted, routed_to
      FROM `{table_id('ops_airflow_incident')}`
      WHERE status='OPEN' AND alert_sent_ts IS NULL
      ORDER BY CASE severity WHEN 'P0' THEN 1 WHEN 'P1' THEN 2 WHEN 'P2' THEN 3 ELSE 4 END, created_ts DESC
      LIMIT 50
    """).to_dataframe()

    if pending.empty:
        return

    for _, row in pending.iterrows():
        to_list = row["routed_to"] if isinstance(row["routed_to"], list) and row["routed_to"] else DEFAULT_ALERT_EMAILS
        downstream = row["downstream_impacted"] or []
        upstreams = row["upstream_blockers"] or []

        html = f"""
        <h3>{row['severity']} Airflow Incident: {row['dag_id']}</h3>
        <p><b>Category:</b> {row['root_cause_category']}</p>
        <p><b>Summary:</b> {row['sample_error']}</p>
        <p><b>Upstream blockers (graph-based):</b> {", ".join(upstreams[:30]) if upstreams else "None / graph not configured"}</p>
        <p><b>Downstream impacted (graph-based):</b> {", ".join(downstream[:30]) if downstream else "None / graph not configured"}</p>
        <p><b>Next step:</b> Open Airflow logs and fix the root cause; rerun.</p>
        """

        subject = f"[{row['severity']}][AIRFLOW] {row['dag_id']} failed ({row['root_cause_category']}) impact={len(downstream)}"
        send_email(to=to_list, subject=subject, html_content=html)

        bq.query(f"""
          UPDATE `{table_id('ops_airflow_incident')}`
          SET alert_sent_ts=CURRENT_TIMESTAMP(), alert_count=alert_count+1
          WHERE incident_id='{row["incident_id"]}'
        """).result()


with DAG(
    dag_id="ops_airflow_monitoring_15min",
    start_date=datetime(2025, 1, 1),
    schedule="*/15 * * * *",
    catchup=False,
    max_active_runs=1,
    tags=["ops", "monitoring"]
) as dag:
    PythonOperator(task_id="ensure_tables", python_callable=ensure_tables)
    PythonOperator(task_id="monitor", python_callable=monitor)
3) File 3/4 — Hourly digest email (consolidated “one table”)


dags/ops_digest_hourly.py

from __future__ import annotations

import pandas as pd
from datetime import datetime

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.email import send_email
from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import bigquery


# EDIT THESE
GCP_CONN_ID = "sa-vz-it-hukv-cdwldo-0-app"
BQ_PROJECT = "vz-it-pr-hukv-cdwldo-0"
BQ_DATASET = "ops_monitoring"
DIGEST_EMAILS = ["cdp-platform-dl@yourcompany.com"]


def table_id(name: str) -> str:
    return f"{BQ_PROJECT}.{BQ_DATASET}.{name}"

def bq_client() -> bigquery.Client:
    hook = GoogleBaseHook(gcp_conn_id=GCP_CONN_ID)
    creds = hook.get_credentials()
    return bigquery.Client(project=BQ_PROJECT, credentials=creds)

def send_digest(**context):
    bq = bq_client()

    health = bq.query(f"""
      WITH latest AS (
        SELECT
          dag_id,
          ARRAY_AGG(STRUCT(snapshot_ts, state, run_id, duration_sec) ORDER BY snapshot_ts DESC LIMIT 1)[OFFSET(0)] AS last
        FROM `{table_id('ops_airflow_dag_run_fact')}`
        WHERE snapshot_ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
        GROUP BY dag_id
      )
      SELECT dag_id, last.state AS state, last.snapshot_ts AS last_seen, last.duration_sec AS duration_sec, last.run_id AS run_id
      FROM latest
      ORDER BY state, dag_id
    """).to_dataframe()

    open_inc = bq.query(f"""
      SELECT severity, dag_id, root_cause_category,
             ARRAY_LENGTH(downstream_impacted) AS impacted_cnt,
             last_updated_ts,
             SUBSTR(sample_error, 1, 180) AS summary
      FROM `{table_id('ops_airflow_incident')}`
      WHERE status='OPEN'
      ORDER BY CASE severity WHEN 'P0' THEN 1 WHEN 'P1' THEN 2 WHEN 'P2' THEN 3 ELSE 4 END, last_updated_ts DESC
      LIMIT 50
    """).to_dataframe()

    html = ["<h2>Airflow Monitoring Digest (Hourly)</h2>"]

    html.append("<h3>Open Incidents</h3>")
    if open_inc.empty:
        html.append("<p>No open incidents ✅</p>")
    else:
        html.append("<table border='1' cellpadding='6' cellspacing='0'>")
        html.append("<tr><th>Severity</th><th>DAG</th><th>Category</th><th>Impacted</th><th>Last Updated</th><th>Summary</th></tr>")
        for _, r in open_inc.iterrows():
            html.append(
                f"<tr><td>{r['severity']}</td><td>{r['dag_id']}</td><td>{r['root_cause_category']}</td>"
                f"<td>{int(r['impacted_cnt'])}</td><td>{r['last_updated_ts']}</td><td>{r['summary']}</td></tr>"
            )
        html.append("</table>")

    html.append("<h3>Latest Status (Last 24h)</h3>")
    if health.empty:
        html.append("<p>No run facts captured in last 24h.</p>")
    else:
        html.append("<table border='1' cellpadding='6' cellspacing='0'>")
        html.append("<tr><th>DAG</th><th>State</th><th>Last Seen</th><th>Duration(s)</th><th>Run</th></tr>")
        for _, r in health.iterrows():
            html.append(
                f"<tr><td>{r['dag_id']}</td><td>{r['state']}</td><td>{r['last_seen']}</td>"
                f"<td>{'' if pd.isna(r['duration_sec']) else int(r['duration_sec'])}</td><td>{r['run_id']}</td></tr>"
            )
        html.append("</table>")

    send_email(
        to=DIGEST_EMAILS,
        subject="[AIRFLOW OPS] Hourly Digest: Incidents + Latest Status",
        html_content="".join(html)
    )


with DAG(
    dag_id="ops_airflow_digest_hourly",
    start_date=datetime(2025, 1, 1),
    schedule="0 * * * *",
    catchup=False,
    max_active_runs=1,
    tags=["ops", "digest"]
) as dag:
    PythonOperator(task_id="send_digest", python_callable=send_digest)
4) File 4/4 — Daily baseline (for slow-run detection later)


This is optional but recommended.



dags/ops_baseline_daily.py

from __future__ import annotations

from datetime import datetime
import pandas as pd

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import bigquery


GCP_CONN_ID = "sa-vz-it-hukv-cdwldo-0-app"
BQ_PROJECT = "vz-it-pr-hukv-cdwldo-0"
BQ_DATASET = "ops_monitoring"


def table_id(name: str) -> str:
    return f"{BQ_PROJECT}.{BQ_DATASET}.{name}"

def bq_client() -> bigquery.Client:
    hook = GoogleBaseHook(gcp_conn_id=GCP_CONN_ID)
    creds = hook.get_credentials()
    return bigquery.Client(project=BQ_PROJECT, credentials=creds)

def ensure_baseline_table(**context):
    bq = bq_client()
    bq.query(f"""
    CREATE TABLE IF NOT EXISTS `{table_id('ops_airflow_baseline_metrics')}` (
      dag_id STRING NOT NULL,
      window_days INT64 NOT NULL,
      sample_count INT64,
      avg_duration_sec FLOAT64,
      stddev_duration_sec FLOAT64,
      p50_duration_sec FLOAT64,
      p95_duration_sec FLOAT64,
      last_computed_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
    )
    CLUSTER BY dag_id
    """).result()

def compute_baseline(**context):
    bq = bq_client()

    df = bq.query(f"""
      SELECT dag_id, duration_sec
      FROM `{table_id('ops_airflow_dag_run_fact')}`
      WHERE state='success'
        AND duration_sec IS NOT NULL
        AND snapshot_ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 14 DAY)
    """).to_dataframe()

    if df.empty:
        return

    rows = []
    for dag_id, g in df.groupby("dag_id"):
        d = g["duration_sec"].astype(float)
        if len(d) < 5:
            continue
        rows.append({
            "dag_id": dag_id,
            "window_days": 14,
            "sample_count": int(len(d)),
            "avg_duration_sec": float(d.mean()),
            "stddev_duration_sec": float(d.std(ddof=1)) if len(d) > 1 else 0.0,
            "p50_duration_sec": float(d.quantile(0.50)),
            "p95_duration_sec": float(d.quantile(0.95)),
        })

    if not rows:
        return

    tmp = table_id("_tmp_baseline")
    bq.load_table_from_dataframe(
        pd.DataFrame(rows), tmp,
        job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
    ).result()

    bq.query(f"""
    MERGE `{table_id('ops_airflow_baseline_metrics')}` T
    USING `{tmp}` S
    ON T.dag_id=S.dag_id AND T.window_days=S.window_days
    WHEN MATCHED THEN UPDATE SET
      T.sample_count=S.sample_count,
      T.avg_duration_sec=S.avg_duration_sec,
      T.stddev_duration_sec=S.stddev_duration_sec,
      T.p50_duration_sec=S.p50_duration_sec,
      T.p95_duration_sec=S.p95_duration_sec,
      T.last_computed_ts=CURRENT_TIMESTAMP()
    WHEN NOT MATCHED THEN INSERT (
      dag_id, window_days, sample_count, avg_duration_sec, stddev_duration_sec, p50_duration_sec, p95_duration_sec, last_computed_ts
    ) VALUES (
      S.dag_id, S.window_days, S.sample_count, S.avg_duration_sec, S.stddev_duration_sec, S.p50_duration_sec, S.p95_duration_sec, CURRENT_TIMESTAMP()
    )
    """).result()


with DAG(
    dag_id="ops_airflow_baseline_daily",
    start_date=datetime(2025, 1, 1),
    schedule="15 2 * * *",  # daily 2:15am
    catchup=False,
    max_active_runs=1,
    tags=["ops", "baseline"]
) as dag:
    PythonOperator(task_id="ensure_baseline_table", python_callable=ensure_baseline_table)
    PythonOperator(task_id="compute_baseline", python_callable=compute_baseline)
What you get immediately after deploying
ops_registry_sync_daily populates ops_airflow_dag_registry with ALL DAGs + tags + owners

ops_airflow_monitoring_15min starts inserting run/task facts and incidents

ops_airflow_digest_hourly sends the consolidated “one table” email

ops_airflow_baseline_daily builds baseline metrics

Important (so you don’t get “same error repeating”)


This design stops repetition by using alert_sent_ts:

alert only once per incident_id

incident_id is a dedup hash of (dag_id + logical_date + signature)



So no more repeated spam.

If you paste (or describe) how you authenticate to Composer’s Airflow API today (token vs IAP vs basic), I can adjust the auth section to your exact setup — but the code above is already complete and deployable with bearer/basic.
