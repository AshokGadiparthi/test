from datetime import datetime, timedelta
import logging
import pandas as pd
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

# -----------------------------
# Configuration
# -----------------------------
DEFAULT_ARGS = {
    'owner': 'data_engineering',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=3),
}

LATEST_RUN_WINDOW_DAYS = int(Variable.get('airflow_monitor_latest_run_window_days', 7))
BQ_DATASET = Variable.get('bq_monitoring_dataset', default_var='airflow_monitoring')
BQ_TABLE = Variable.get('bq_monitoring_table', default_var='cdp_monitor_latest')

# -----------------------------
# SQL Query (Latest Run)
# -----------------------------
METADATA_QUERY = f"""
WITH latest_runs AS (
    SELECT dag_id, MAX(execution_date) AS latest_execution_date
    FROM dag_run
    WHERE execution_date >= CURRENT_DATE - INTERVAL '{LATEST_RUN_WINDOW_DAYS} days'
    GROUP BY dag_id
)
SELECT 
    dr.dag_id,
    dr.run_id,
    dr.execution_date,
    dr.state AS dag_state,
    dr.start_date,
    dr.end_date,
    ROUND(EXTRACT(EPOCH FROM (dr.end_date - dr.start_date))::NUMERIC, 2) AS dag_duration_seconds,
    ti.task_id,
    ti.state AS task_state,
    ti.operator,
    ti.start_date AS task_start_date,
    ti.end_date AS task_end_date,
    ROUND(EXTRACT(EPOCH FROM (ti.end_date - ti.start_date))::NUMERIC, 2) AS task_duration_seconds,
    ROUND(EXTRACT(EPOCH FROM (ti.start_date - ti.queued_dttm))::NUMERIC, 2) AS queue_wait_seconds,
    ti.try_number,
    ti.max_tries,
    CURRENT_TIMESTAMP AT TIME ZONE 'UTC' AS extracted_at
FROM latest_runs lr
JOIN dag_run dr ON lr.dag_id = dr.dag_id AND lr.latest_execution_date = dr.execution_date
JOIN task_instance ti ON dr.dag_id = ti.dag_id AND dr.run_id = ti.run_id
ORDER BY dr.dag_id, ti.task_id;
"""

# -----------------------------
# Step 1: Extract Metadata
# -----------------------------
def extract_latest_metadata(**context) -> pd.DataFrame:
    logging.info("Connecting to Airflow metadata database...")
    pg_hook = PostgresHook(postgres_conn_id='airflow_db')
    conn = None
    try:
        conn = pg_hook.get_conn()
        cdp_ids_df = pd.read_sql("SELECT dag_id FROM dag_tag WHERE name = 'cdp';", conn)
        cdp_ids = cdp_ids_df['dag_id'].dropna().unique().tolist()

        if not cdp_ids:
            logging.warning("No DAGs found with tag='cdp'.")
            context['ti'].xcom_push(key='metadata_df', value=pd.DataFrame().to_json(orient='records'))
            return pd.DataFrame()

        df = pd.read_sql(METADATA_QUERY, conn)
        df = df[df['dag_id'].isin(cdp_ids)].reset_index(drop=True)
        context['ti'].xcom_push(key='metadata_df', value=df.to_json(orient='records'))
        logging.info(f"Extracted {len(df)} records for {len(df['dag_id'].unique())} CDP DAGs.")
        return df
    finally:
        if conn:
            conn.close()

# -----------------------------
# Step 2: Root Cause + Lineage Analysis
# -----------------------------
def analyze_failures(**context):
    df_json = context['ti'].xcom_pull(task_ids='extract_metadata', key='metadata_df')
    if not df_json:
        logging.warning("No metadata for RCA.")
        return

    df = pd.read_json(df_json)
    if df.empty:
        logging.info("No task data for RCA.")
        return

    failed = df[df['task_state'].isin(['failed', 'upstream_failed'])]
    if failed.empty:
        logging.info("✅ All CDP DAGs succeeded in latest run.")
        return

    rca_rows = []
    logging.info("---- RCA: Failure Insights ----")

    for dag_id in df['dag_id'].unique():
        dag_tasks = df[df['dag_id'] == dag_id].copy()
        # Build upstream mapping: task -> earlier tasks in start_date order
        upstream_map = {}
        for idx, row in dag_tasks.iterrows():
            possible_upstream = dag_tasks[(dag_tasks['task_end_date'] <= row['task_start_date'])]['task_id'].tolist()
            upstream_map[row['task_id']] = possible_upstream

        for idx, row in dag_tasks.iterrows():
            if row['task_state'] in ['failed', 'upstream_failed']:
                upstreams = upstream_map.get(row['task_id'], [])
                cause = "direct_failure"
                if upstreams:
                    upstream_failed = dag_tasks[(dag_tasks['task_id'].isin(upstreams)) &
                                                (dag_tasks['task_state'].isin(['failed', 'upstream_failed']))]
                    if not upstream_failed.empty:
                        cause = f"upstream_failed:{','.join(upstream_failed['task_id'].tolist())}"

                severity = "HIGH" if row.get('try_number', 0) > 1 else "MEDIUM"
                rca_rows.append({
                    'dag_id': dag_id,
                    'task_id': row['task_id'],
                    'task_state': row['task_state'],
                    'severity': severity,
                    'root_cause': cause,
                    'operator': row.get('operator'),
                    'task_duration_sec': row.get('task_duration_seconds'),
                    'upstream_tasks': upstreams,
                })
                logging.info(f"DAG={dag_id}, Task={row['task_id']} failed, Cause={cause}")

    if rca_rows:
        rca_df = pd.DataFrame(rca_rows)
        context['ti'].xcom_push(key='rca_df', value=rca_df.to_json(orient='records'))

# -----------------------------
# Step 3: Load to BigQuery
# -----------------------------
def load_to_bigquery(**context):
    df_json = context['ti'].xcom_pull(task_ids='extract_metadata', key='metadata_df')
    if not df_json:
        logging.warning("No data to load to BigQuery.")
        return

    df = pd.read_json(df_json)
    if df.empty:
        logging.info("No rows to load to BigQuery.")
        return

    bq_hook = BigQueryHook(gcp_conn_id='google_cloud_default', use_legacy_sql=False)
    client = bq_hook.get_client()
    table_id = f"{client.project}.{BQ_DATASET}.{BQ_TABLE}"
    logging.info(f"Loading {len(df)} rows into BigQuery table {table_id}...")
    job = client.load_table_from_dataframe(df, table_id)
    job.result()
    logging.info(f"✅ BigQuery load complete: {len(df)} rows loaded.")

# -----------------------------
# Step 4: Executive Summary
# -----------------------------
def print_executive_summary(**context):
    df_json = context['ti'].xcom_pull(task_ids='extract_metadata', key='metadata_df')
    if not df_json:
        logging.warning("No data for summary.")
        return

    df = pd.read_json(df_json)
    if df.empty:
        logging.info("No data for reporting.")
        return

    total_dags = df['dag_id'].nunique()
    total_tasks = len(df)
    failed = len(df[df['task_state'].isin(['failed', 'upstream_failed'])])
    success = len(df[df['task_state'] == 'success'])
    success_rate = (success / total_tasks) * 100 if total_tasks else 0

    logging.info("=" * 100)
    logging.info("AIRFLOW CDP EXECUTIVE SUMMARY (LATEST RUNS)")
    logging.info("=" * 100)
    logging.info(f"Monitored DAGs: {total_dags}")
    logging.info(f"Total Tasks: {total_tasks}")
    logging.info(f"Success: {success} | Failed: {failed} | Success Rate: {success_rate:.1f}%")
    logging.info("=" * 100)

# -----------------------------
# DAG Definition
# -----------------------------
with DAG(
    dag_id='airflow_cdp_monitoring_bq',
    default_args=DEFAULT_ARGS,
    description='Real-time Airflow CDP DAG monitoring with lineage, RCA, and BigQuery export',
    schedule_interval='0 * * * *',  # hourly
    start_date=datetime(2025, 1, 1),
    catchup=False,
    max_active_runs=1,
    tags=['monitoring', 'cdp', 'bigquery'],
) as dag:

    extract_metadata = PythonOperator(
        task_id='extract_metadata',
        python_callable=extract_latest_metadata,
        provide_context=True,
    )

    root_cause_analysis = PythonOperator(
        task_id='root_cause_analysis',
        python_callable=analyze_failures,
        provide_context=True,
    )

    load_bq = PythonOperator(
        task_id='load_to_bigquery',
        python_callable=load_to_bigquery,
        provide_context=True,
    )

    print_summary = PythonOperator(
        task_id='print_executive_summary',
        python_callable=print_executive_summary,
        provide_context=True,
    )

    extract_metadata >> root_cause_analysis >> load_bq >> print_summary





CREATE TABLE `your_project_id.airflow_monitoring.cdp_monitor_latest` (
    dag_id STRING NOT NULL,
    run_id STRING NOT NULL,
    execution_date TIMESTAMP,
    dag_state STRING,
    start_date TIMESTAMP,
    end_date TIMESTAMP,
    dag_duration_seconds NUMERIC,
    task_id STRING,
    task_state STRING,
    operator STRING,
    task_start_date TIMESTAMP,
    task_end_date TIMESTAMP,
    task_duration_seconds NUMERIC,
    queue_wait_seconds NUMERIC,
    try_number INT64,
    max_tries INT64,
    upstream_tasks ARRAY<STRING>,
    extracted_at TIMESTAMP
);
