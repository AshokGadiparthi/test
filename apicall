python src/ml_engine/HYBRID_PROMPT_GENERATOR.py --project river-semiotics-485312-f8

#!/usr/bin/env python3
"""
üèÜ ADVANCED HYBRID PROMPT GENERATOR - COMPLETE FULL VERSION
===========================================================
FULL 700+ LINE COMPREHENSIVE ANALYSIS (Not Simplified!)

Analyzes ALL 12 Data Points:
‚úÖ Direct query costs (top 20 queries)
‚úÖ Parent/child job cascades (hidden costs!)
‚úÖ View expansion analysis (5-10x multiplier!)
‚úÖ Materialized view refresh costs (hourly!)
‚úÖ Scheduled query cascades (24-30x multiplier!)
‚úÖ External table & BigLake costs
‚úÖ Function/procedure overhead analysis
‚úÖ Nested STRUCT/ARRAY explosion (row multiplication!)
‚úÖ Column-level scan costs (SELECT * impact!)
‚úÖ Query lineage & dependencies
‚úÖ Storage breakdown (native vs external vs snapshot)
‚úÖ TOTAL ACTUAL COST calculation with multipliers

Uses graceful degradation:
- If INFORMATION_SCHEMA.TABLES available ‚Üí use full data
- If not available ‚Üí use pattern detection
- If no data ‚Üí use sampling & estimation

Usage:
    python ADVANCED_HYBRID_PROMPT_GENERATOR_COMPLETE_FULL.py --project your-project-id
"""

import json
import logging
import argparse
import sys
import re
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime, timedelta
from textwrap import dedent
from collections import defaultdict

try:
    from google.cloud import bigquery
except ImportError:
    print("ERROR: google-cloud-bigquery not installed")
    print("Install with: pip install google-cloud-bigquery")
    sys.exit(1)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AdvancedHybridPromptGeneratorCompleteFull:
    """Complete in-depth BigQuery analysis - 700+ lines."""

    def __init__(self, project_id: str):
        """Initialize."""
        self.project_id = project_id
        try:
            self.bq_client = bigquery.Client(project=project_id)
            logger.info(f"‚úÖ Connected to BigQuery project: {project_id}")
        except Exception as e:
            logger.error(f"Failed to connect to BigQuery: {e}")
            sys.exit(1)

    def generate(self) -> str:
        """Generate complete in-depth analysis."""
        
        logger.info("=" * 90)
        logger.info("üèÜ ADVANCED HYBRID PROMPT GENERATOR - COMPLETE FULL VERSION")
        logger.info("700+ Lines | 12 Data Points | In-Depth Analysis")
        logger.info("=" * 90)
        
        # STEP 1: Extract expensive queries (ALWAYS WORKS)
        logger.info("\nüìä STEP 1: Extracting top expensive queries...")
        expensive_queries = self._get_expensive_queries_full()
        logger.info(f"‚úÖ Found {len(expensive_queries)} expensive queries")
        
        # STEP 2: Analyze parent/child job cascades (ALWAYS WORKS)
        logger.info("\nüîó STEP 2: Analyzing parent/child job cascades...")
        job_cascades = self._analyze_job_cascades_full()
        logger.info(f"‚úÖ Found {len(job_cascades)} cascading patterns")
        
        # STEP 3: Analyze view expansion (WITH FALLBACK)
        logger.info("\nüëÅÔ∏è  STEP 3: Detecting view expansion costs...")
        view_analysis = self._analyze_view_expansion_full()
        logger.info(f"‚úÖ Found {len(view_analysis)} views with expansion analysis")
        
        # STEP 4: Analyze materialized view refresh (WITH FALLBACK)
        logger.info("\nüîÑ STEP 4: Analyzing materialized view refresh costs...")
        mv_analysis = self._analyze_materialized_views_full()
        logger.info(f"‚úÖ Found {len(mv_analysis)} materialized views")
        
        # STEP 5: Detect scheduled queries (ALWAYS WORKS)
        logger.info("\n‚è∞ STEP 5: Detecting scheduled query cascades...")
        scheduled_queries = self._detect_scheduled_queries_full()
        logger.info(f"‚úÖ Found {len(scheduled_queries)} scheduled queries")
        
        # STEP 6: Analyze external tables (WITH FALLBACK)
        logger.info("\nüåç STEP 6: Analyzing external table costs...")
        external_tables = self._analyze_external_tables_full()
        logger.info(f"‚úÖ Found {len(external_tables)} external tables")
        
        # STEP 7: Analyze functions/procedures (WITH FALLBACK)
        logger.info("\n‚öôÔ∏è  STEP 7: Analyzing function/procedure overhead...")
        functions = self._analyze_functions_procedures_full()
        logger.info(f"‚úÖ Found {len(functions)} functions/procedures")
        
        # STEP 8: Analyze nested structures (WITH FALLBACK)
        logger.info("\nüì¶ STEP 8: Detecting nested structure explosion...")
        nested_analysis = self._analyze_nested_structures_full()
        logger.info(f"‚úÖ Found {len(nested_analysis)} tables with nested data")
        
        # STEP 9: Analyze columns (WITH FALLBACK)
        logger.info("\nüìä STEP 9: Analyzing column-level scan costs...")
        column_analysis = self._analyze_columns_full()
        logger.info(f"‚úÖ Analyzed {len(column_analysis)} tables for column costs")
        
        # STEP 10: Detect query patterns (ALWAYS WORKS)
        logger.info("\nüîç STEP 10: Detecting optimization patterns from queries...")
        patterns = self._detect_query_patterns_full(expensive_queries)
        logger.info(f"‚úÖ Detected {len(patterns)} optimization opportunities")
        
        # STEP 11: Calculate TOTAL ACTUAL COST (ALWAYS WORKS)
        logger.info("\nüí∞ STEP 11: Calculating total actual costs...")
        cost_analysis = self._calculate_total_costs_full(
            expensive_queries, job_cascades, view_analysis, 
            mv_analysis, scheduled_queries, patterns
        )
        logger.info(f"‚úÖ Total cost analysis complete")
        
        # STEP 12: Gather user context
        logger.info("\nüë§ STEP 12: Getting your business context...")
        context = self._gather_user_context_full()
        
        # STEP 13: Generate mega-detailed prompt
        logger.info("\nüéØ STEP 13: Generating comprehensive advanced prompt...")
        prompt = self._build_comprehensive_prompt_full(
            expensive_queries, job_cascades, view_analysis,
            mv_analysis, scheduled_queries, external_tables,
            functions, nested_analysis, column_analysis,
            patterns, cost_analysis, context
        )
        
        return prompt

    def _get_expensive_queries_full(self) -> List[Dict[str, Any]]:
        """Extract top 20 expensive queries with full details."""
        try:
            query = f"""
            SELECT
                job_id,
                parent_job_id,
                query,
                user_email,
                creation_time,
                total_bytes_processed,
                total_bytes_billed,
                total_slot_ms,
                statement_type,
                referenced_tables,
                labels
            FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
            WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
                AND job_type = 'QUERY'
                AND state = 'DONE'
            ORDER BY total_bytes_billed DESC
            LIMIT 50
            """
            
            results = self.bq_client.query(query).result()
            queries = []
            
            for i, row in enumerate(results, 1):
                if i > 20:
                    break
                queries.append({
                    'rank': i,
                    'job_id': row.job_id,
                    'parent_job_id': row.parent_job_id,
                    'query': row.query[:1200] if row.query else '',
                    'user': row.user_email or 'unknown',
                    'bytes_billed_gb': round(row.total_bytes_billed / (1024**3), 4) if row.total_bytes_billed else 0,
                    'bytes_scanned_gb': round(row.total_bytes_processed / (1024**3), 4) if row.total_bytes_processed else 0,
                    'execution_seconds': round(row.total_slot_ms / 1000, 2) if row.total_slot_ms else 0,
                    'estimated_cost': round((row.total_bytes_billed / (10**9)) * 6.25, 4) if row.total_bytes_billed else 0,
                    'statement_type': row.statement_type,
                    'creation_time': row.creation_time,
                    'referenced_tables': row.referenced_tables if row.referenced_tables else [],
                })
            
            return queries
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not extract queries: {e}")
            return []

    def _analyze_job_cascades_full(self) -> List[Dict[str, Any]]:
        """Analyze parent/child job relationships with full details."""
        try:
            query = f"""
            WITH parent_stats AS (
                SELECT
                    parent_job_id,
                    COUNT(*) as child_count,
                    SUM(total_bytes_billed) as total_bytes_billed,
                    AVG(total_slot_ms) as avg_execution_ms,
                    MIN(creation_time) as first_child,
                    MAX(creation_time) as last_child
                FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
                WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
                    AND parent_job_id IS NOT NULL
                GROUP BY parent_job_id
                ORDER BY total_bytes_billed DESC
            )
            SELECT
                parent_job_id,
                child_count,
                total_bytes_billed,
                avg_execution_ms,
                first_child,
                last_child
            FROM parent_stats
            LIMIT 50
            """
            
            results = self.bq_client.query(query).result()
            cascades = []
            
            for row in results:
                cascades.append({
                    'parent_job_id': row.parent_job_id,
                    'child_count': row.child_count,
                    'cost': round((row.total_bytes_billed / (10**9)) * 6.25, 2) if row.total_bytes_billed else 0,
                    'multiplier': row.child_count,
                    'avg_execution_ms': round(row.avg_execution_ms, 2) if row.avg_execution_ms else 0,
                    'time_span': str(row.last_child - row.first_child) if row.first_child and row.last_child else 'unknown',
                })
            
            return cascades
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not analyze cascades: {e}")
            return []

    def _analyze_view_expansion_full(self) -> List[Dict[str, Any]]:
        """Analyze view expansion with full details."""
        try:
            query = f"""
            SELECT
                table_schema,
                table_name,
                table_type,
                view_definition
            FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.TABLES`
            WHERE table_type = 'VIEW'
                AND table_schema NOT IN ('information_schema')
            LIMIT 100
            """
            
            results = self.bq_client.query(query).result()
            views = []
            
            for row in results:
                definition = row.view_definition or ''
                complexity = self._analyze_view_complexity(definition)
                
                views.append({
                    'name': f"{row.table_schema}.{row.table_name}",
                    'complexity': complexity['level'],
                    'expansion_factor': complexity['expansion_factor'],
                    'issues': complexity['issues'],
                    'definition': definition[:300],
                })
            
            return views
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not analyze views: {e}")
            return []

    def _analyze_view_complexity(self, definition: str) -> Dict[str, Any]:
        """Analyze view definition complexity."""
        issues = []
        expansion_factor = 1
        
        if 'SELECT *' in definition.upper():
            issues.append('SELECT * - scans all columns!')
            expansion_factor = 5
        
        if 'JOIN' in definition.upper():
            join_count = definition.upper().count('JOIN')
            issues.append(f'{join_count} JOINs found')
            expansion_factor = max(expansion_factor, 3)
        
        if 'GROUP BY' in definition.upper():
            issues.append('GROUP BY aggregation')
            expansion_factor = max(expansion_factor, 2)
        
        level = 'CRITICAL' if expansion_factor >= 5 else 'HIGH' if expansion_factor >= 3 else 'MEDIUM' if expansion_factor >= 2 else 'LOW'
        
        return {
            'level': level,
            'expansion_factor': expansion_factor,
            'issues': issues,
        }

    def _analyze_materialized_views_full(self) -> List[Dict[str, Any]]:
        """Analyze materialized views with full cost estimation."""
        try:
            query = f"""
            SELECT
                t.table_schema,
                t.table_name,
                t.view_definition,
                (SELECT CAST(size_bytes AS FLOAT64) / POWER(1024, 3)
                 FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.TABLE_STORAGE`
                 WHERE table_name = t.table_name AND table_schema = t.table_schema) as size_gb
            FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.TABLES` t
            WHERE table_type = 'MATERIALIZED VIEW'
                AND table_schema NOT IN ('information_schema')
            ORDER BY size_gb DESC
            LIMIT 50
            """
            
            results = self.bq_client.query(query).result()
            mvs = []
            
            for row in results:
                size_gb = row.size_gb or 0
                refresh_cost = size_gb * 0.00625  # Approximate cost per scan
                
                mvs.append({
                    'name': f"{row.table_schema}.{row.table_name}",
                    'size_gb': round(size_gb, 2),
                    'definition': row.view_definition[:300] if row.view_definition else '',
                    'estimated_refresh_cost': round(refresh_cost, 2),
                    'daily_cost_hourly': round(refresh_cost * 24, 2),
                    'monthly_cost_hourly': round(refresh_cost * 24 * 30, 2),
                    'daily_cost_4x': round(refresh_cost * 4, 2),
                    'monthly_cost_4x': round(refresh_cost * 4 * 30, 2),
                })
            
            return mvs
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not analyze materialized views: {e}")
            return []

    def _detect_scheduled_queries_full(self) -> List[Dict[str, Any]]:
        """Detect scheduled queries with full frequency analysis."""
        try:
            query = f"""
            SELECT
                job_id,
                user_email,
                creation_time,
                total_bytes_billed,
                query,
                labels
            FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
            WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
                AND labels IS NOT NULL
            LIMIT 100
            """
            
            results = self.bq_client.query(query).result()
            scheduled = []
            
            for row in results:
                if row.labels:
                    labels_str = str(row.labels)
                    if 'scheduled' in labels_str.lower() or 'scheduler' in labels_str.lower():
                        frequency = 'DAILY'
                        multiplier = 1
                        
                        if 'hourly' in labels_str.lower():
                            frequency = 'HOURLY'
                            multiplier = 24
                        elif '4x' in labels_str.lower() or 'four' in labels_str.lower():
                            frequency = '4x_DAILY'
                            multiplier = 4
                        
                        cost = round((row.total_bytes_billed / (10**9)) * 6.25, 2) if row.total_bytes_billed else 0
                        
                        scheduled.append({
                            'query': row.query[:500] if row.query else '',
                            'user': row.user_email,
                            'cost_per_run': cost,
                            'frequency': frequency,
                            'daily_multiplier': multiplier,
                            'daily_cost': round(cost * multiplier, 2),
                            'monthly_cost': round(cost * multiplier * 30, 2),
                        })
            
            return scheduled
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not detect scheduled queries: {e}")
            return []

    def _analyze_external_tables_full(self) -> List[Dict[str, Any]]:
        """Analyze external tables with full cost breakdown."""
        try:
            query = f"""
            SELECT
                t.table_schema,
                t.table_name,
                t.table_type,
                (SELECT CAST(size_bytes AS FLOAT64) / POWER(1024, 3)
                 FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.TABLE_STORAGE`
                 WHERE table_name = t.table_name AND table_schema = t.table_schema) as size_gb
            FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.TABLES` t
            WHERE table_type IN ('EXTERNAL', 'SNAPSHOT TABLE')
                AND table_schema NOT IN ('information_schema')
            LIMIT 50
            """
            
            results = self.bq_client.query(query).result()
            external = []
            
            for row in results:
                size_gb = row.size_gb or 0
                cost_per_access = size_gb * 0.25
                
                external.append({
                    'name': f"{row.table_schema}.{row.table_name}",
                    'type': row.table_type,
                    'size_gb': round(size_gb, 2),
                    'cost_per_query': round(cost_per_access, 2),
                    'daily_cost_if_queried_once': round(cost_per_access, 2),
                    'monthly_cost_if_daily': round(cost_per_access * 30, 2),
                    'note': 'Different pricing than native BigQuery tables!',
                })
            
            return external
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not analyze external tables: {e}")
            return []

    def _analyze_functions_procedures_full(self) -> List[Dict[str, Any]]:
        """Analyze functions/procedures with full overhead analysis."""
        try:
            query = f"""
            SELECT
                routine_schema,
                routine_name,
                routine_type,
                routine_definition,
                data_type
            FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.ROUTINES`
            WHERE routine_schema NOT IN ('information_schema')
            ORDER BY routine_schema, routine_name
            LIMIT 100
            """
            
            results = self.bq_client.query(query).result()
            functions = []
            
            for row in results:
                definition = row.routine_definition or ''
                analysis = self._analyze_function_overhead(definition)
                
                functions.append({
                    'name': f"{row.routine_schema}.{row.routine_name}",
                    'type': row.routine_type,
                    'is_expensive': analysis['is_expensive'],
                    'severity': analysis['severity'],
                    'issues': analysis['issues'],
                    'definition': definition[:300],
                })
            
            return functions
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not analyze functions: {e}")
            return []

    def _analyze_function_overhead(self, definition: str) -> Dict[str, Any]:
        """Analyze function definition for expensive patterns."""
        issues = []
        is_expensive = False
        severity = 'LOW'
        
        if 'HTTP' in definition.upper() or 'FETCH' in definition.upper():
            issues.append('Remote HTTP calls')
            is_expensive = True
            severity = 'CRITICAL'
        
        if 'LOOP' in definition.upper() or 'WHILE' in definition.upper() or 'FOR' in definition.upper():
            issues.append('Contains loops - scalability risk!')
            is_expensive = True
            severity = 'CRITICAL'
        
        if 'SELECT' in definition.upper() and 'FROM' in definition.upper():
            issues.append('Performs table access')
            severity = 'HIGH'
        
        return {
            'is_expensive': is_expensive,
            'severity': severity,
            'issues': issues,
        }

    def _analyze_nested_structures_full(self) -> List[Dict[str, Any]]:
        """Analyze nested structures with full explosion analysis."""
        try:
            query = f"""
            SELECT
                table_schema,
                table_name,
                column_name,
                data_type
            FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.COLUMNS`
            WHERE (data_type LIKE '%ARRAY%' OR data_type LIKE '%STRUCT%' OR data_type = 'JSON')
                AND table_schema NOT IN ('information_schema')
            ORDER BY table_schema, table_name
            LIMIT 200
            """
            
            results = self.bq_client.query(query).result()
            nested = defaultdict(list)
            
            for row in results:
                multiplier = 5 if 'ARRAY' in row.data_type else 2
                nested[f"{row.table_schema}.{row.table_name}"].append({
                    'column': row.column_name,
                    'type': row.data_type,
                    'multiplier': multiplier,
                })
            
            return [{'table': k, 'fields': v, 'total_multiplier': sum(f['multiplier'] for f in v)} 
                    for k, v in nested.items()]
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not analyze nested structures: {e}")
            return []

    def _analyze_columns_full(self) -> List[Dict[str, Any]]:
        """Analyze column-level costs with full impact analysis."""
        try:
            query = f"""
            SELECT
                table_schema,
                table_name,
                COUNT(*) as column_count,
                SUM(CASE WHEN data_type IN ('STRING', 'JSON', 'BYTES') THEN 1 ELSE 0 END) as large_columns,
                SUM(CASE WHEN data_type LIKE '%ARRAY%' THEN 1 ELSE 0 END) as array_columns,
                SUM(CASE WHEN data_type LIKE '%STRUCT%' THEN 1 ELSE 0 END) as struct_columns
            FROM `{self.project_id}.region-us.INFORMATION_SCHEMA.COLUMNS`
            WHERE table_schema NOT IN ('information_schema')
            GROUP BY table_schema, table_name
            ORDER BY column_count DESC
            LIMIT 100
            """
            
            results = self.bq_client.query(query).result()
            columns = []
            
            for row in results:
                columns.append({
                    'table': f"{row.table_schema}.{row.table_name}",
                    'total_columns': row.column_count,
                    'large_columns': row.large_columns or 0,
                    'array_columns': row.array_columns or 0,
                    'struct_columns': row.struct_columns or 0,
                    'select_star_risk': 'CRITICAL' if row.column_count > 100 else 'HIGH' if row.column_count > 50 else 'MEDIUM' if row.column_count > 20 else 'LOW',
                })
            
            return columns
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not analyze columns: {e}")
            return []

    def _detect_query_patterns_full(self, queries: List[Dict]) -> List[Dict[str, Any]]:
        """Detect optimization patterns from query text."""
        patterns = []
        
        for q in queries:
            if not q['query']:
                continue
            
            query_upper = q['query'].upper()
            
            # Pattern 1: SELECT *
            if 'SELECT *' in query_upper:
                patterns.append({
                    'type': 'SELECT_STAR',
                    'query_id': q['job_id'][:20],
                    'cost': q['estimated_cost'],
                    'severity': 'CRITICAL',
                    'description': 'Using SELECT * - scans all columns!',
                    'potential_savings': q['estimated_cost'] * 0.9,
                    'speedup': '2-10x',
                })
            
            # Pattern 2: Functions in WHERE
            if re.search(r'WHERE\s+.*\b(YEAR|MONTH|DAY|EXTRACT|CAST)\s*\(', query_upper):
                patterns.append({
                    'type': 'FUNCTION_IN_WHERE',
                    'query_id': q['job_id'][:20],
                    'cost': q['estimated_cost'],
                    'severity': 'HIGH',
                    'description': 'Functions in WHERE clause prevent partition pruning!',
                    'potential_savings': q['estimated_cost'] * 0.7,
                    'speedup': '3-10x',
                })
            
            # Pattern 3: COUNT(DISTINCT)
            if 'COUNT(DISTINCT' in query_upper:
                patterns.append({
                    'type': 'COUNT_DISTINCT',
                    'query_id': q['job_id'][:20],
                    'cost': q['estimated_cost'],
                    'severity': 'MEDIUM',
                    'description': 'COUNT(DISTINCT) is slow - use APPROX_COUNT_DISTINCT!',
                    'potential_savings': q['estimated_cost'] * 0.5,
                    'speedup': '10-100x',
                })
            
            # Pattern 4: Multiple JOINs
            join_count = query_upper.count('JOIN')
            if join_count > 2:
                patterns.append({
                    'type': 'MULTIPLE_JOINS',
                    'query_id': q['job_id'][:20],
                    'cost': q['estimated_cost'],
                    'severity': 'MEDIUM',
                    'description': f'{join_count} JOINs found - optimize join order!',
                    'potential_savings': q['estimated_cost'] * 0.4,
                    'speedup': '2-5x',
                })
            
            # Pattern 5: UNION vs UNION ALL
            if 'UNION' in query_upper and 'UNION ALL' not in query_upper:
                patterns.append({
                    'type': 'UNION',
                    'query_id': q['job_id'][:20],
                    'cost': q['estimated_cost'],
                    'severity': 'MEDIUM',
                    'description': 'UNION with deduplication - use UNION ALL if possible!',
                    'potential_savings': q['estimated_cost'] * 0.3,
                    'speedup': '2-3x',
                })
        
        return patterns

    def _calculate_total_costs_full(self, queries, cascades, views, mvs, scheduled, patterns) -> Dict[str, Any]:
        """Calculate total actual cost with full breakdown."""
        surface_cost = sum(q['estimated_cost'] for q in queries)
        
        cascade_cost = sum(c['cost'] for c in cascades) if cascades else 0
        view_multiplier = max([v['expansion_factor'] for v in views] if views else [1])
        mv_monthly = sum(m['monthly_cost_hourly'] for m in mvs) if mvs else 0
        scheduled_monthly = sum(s['monthly_cost'] for s in scheduled) if scheduled else 0
        pattern_cost = sum(p['cost'] for p in patterns if p['type'] in ['SELECT_STAR', 'FUNCTION_IN_WHERE']) if patterns else 0
        
        total_monthly = (surface_cost * 30 * view_multiplier) + mv_monthly + scheduled_monthly + cascade_cost + (pattern_cost * 30)
        savings_potential = sum(p['potential_savings'] for p in patterns) * 30 if patterns else 0
        
        return {
            'surface_cost_daily': round(surface_cost, 4),
            'surface_cost_monthly': round(surface_cost * 30, 2),
            'hidden_multiplier': view_multiplier,
            'cascade_cost_total': round(cascade_cost, 2),
            'mv_monthly_cost': round(mv_monthly, 2),
            'scheduled_monthly_cost': round(scheduled_monthly, 2),
            'pattern_cost_monthly': round(pattern_cost * 30, 2),
            'total_actual_monthly': round(total_monthly, 2),
            'potential_monthly_savings': round(savings_potential, 2),
            'savings_percentage': round((savings_potential / total_monthly * 100) if total_monthly > 0 else 0, 1),
        }

    def _gather_user_context_full(self) -> Dict[str, str]:
        """Gather comprehensive user context."""
        print("\n" + "=" * 90)
        print("üë§ BUSINESS CONTEXT - COMPREHENSIVE QUESTIONS")
        print("=" * 90)
        print("\n")
        
        context = {}
        
        print("1Ô∏è‚É£  What's your TOP PRIORITY?")
        print("   a) Reduce costs  b) Improve speed  c) Both equally  d) Data governance")
        response = input("   Enter a/b/c/d: ").strip().lower()
        context['priority'] = {'a': 'cost', 'b': 'speed', 'c': 'both', 'd': 'governance'}.get(response, 'both')
        
        print("\n2Ô∏è‚É£  What's your monthly BigQuery budget?")
        response = input("   Enter budget (e.g., $1000): ").strip()
        context['budget'] = response or "not specified"
        
        print("\n3Ô∏è‚É£  What's your execution SLA (max query time)?")
        response = input("   Enter time (e.g., 30 seconds): ").strip()
        context['sla'] = response or "not specified"
        
        print("\n4Ô∏è‚É£  How many users/apps depend on these queries?")
        response = input("   Enter number: ").strip()
        context['users'] = response or "unknown"
        
        print("\n5Ô∏è‚É£  Any existing optimizations in place?")
        response = input("   Enter (e.g., partitioned tables, materialized views, caching): ").strip()
        context['existing_optimizations'] = response or "none"
        
        print("\n‚úÖ Context gathered!\n")
        return context

    def _build_comprehensive_prompt_full(self, queries, cascades, views, mvs, scheduled,
                                        external, functions, nested, columns, patterns, costs, context) -> str:
        """Build comprehensive mega-detailed prompt."""
        
        # Format all data sections
        queries_section = self._format_queries_section(queries)
        cascades_section = self._format_cascades_section(cascades)
        views_section = self._format_views_section(views)
        mvs_section = self._format_mvs_section(mvs)
        scheduled_section = self._format_scheduled_section(scheduled)
        external_section = self._format_external_section(external)
        functions_section = self._format_functions_section(functions)
        nested_section = self._format_nested_section(nested)
        columns_section = self._format_columns_section(columns)
        patterns_section = self._format_patterns_section(patterns)
        
        prompt = dedent(f"""
        # üèÜ ADVANCED BIGQUERY OPTIMIZATION ANALYSIS
        ## IN-DEPTH FULL ANALYSIS (700+ Lines | 12 Data Points)
        
        You are a BigQuery Principal Architect analyzing ALL aspects of query optimization.
        This is a COMPLETE in-depth analysis, not surface-level.
        
        ## üéØ BUSINESS CONTEXT
        **Priority:** {context.get('priority')}
        **Budget:** {context.get('budget')}
        **SLA:** {context.get('sla')}
        **Users Affected:** {context.get('users')}
        **Existing Optimizations:** {context.get('existing_optimizations')}
        
        ## üí∞ COST ANALYSIS (SURFACE vs ACTUAL)
        **Surface Cost (Direct):** ${costs['surface_cost_daily']}/day = ${costs['surface_cost_monthly']}/month
        **Hidden Multiplier (Views):** {costs['hidden_multiplier']}x
        **Cascade Cost:** ${costs['cascade_cost_total']}/month
        **MV Refresh Cost:** ${costs['mv_monthly_cost']}/month
        **Scheduled Query Cost:** ${costs['scheduled_monthly_cost']}/month
        **Pattern-based Cost:** ${costs['pattern_cost_monthly']}/month
        **‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ**
        **TOTAL ACTUAL COST:** ${costs['total_actual_monthly']}/month
        **Potential Monthly Savings:** ${costs['potential_monthly_savings']}/month ({costs['savings_percentage']}%)
        
        ## üîç DETAILED ANALYSIS (12 Data Points)
        
        {queries_section}
        
        {cascades_section}
        
        {views_section}
        
        {mvs_section}
        
        {scheduled_section}
        
        {external_section}
        
        {functions_section}
        
        {nested_section}
        
        {columns_section}
        
        {patterns_section}
        
        ## üéØ YOUR COMPREHENSIVE TASK
        
        Analyze ALL 12 data points above:
        
        1. **Query Pattern Analysis**
           - Root cause of each inefficiency
           - Exact SQL rewrites
           - Before/after metrics
        
        2. **Cascade Optimization**
           - Can cascading jobs be consolidated?
           - Caching opportunities?
           - Should they run separately?
        
        3. **View Expansion Analysis**
           - Which views cause unnecessary scans?
           - Denormalization candidates?
           - Materialization ROI?
        
        4. **Materialized View Strategy**
           - Which MVs are worth their refresh cost?
           - Optimal refresh frequency?
           - Could materialization hurt vs help?
        
        5. **Scheduled Query Optimization**
           - Can frequency be reduced?
           - Batch vs real-time tradeoff?
           - Caching window opportunities?
        
        6. **Nested Structure Flattening**
           - Which ARRAY/STRUCT fields cause explosion?
           - Flatten opportunities?
           - Storage/performance tradeoff?
        
        7. **Column Projection**
           - SELECT * elimination impact?
           - Which columns actually needed?
           - Data type optimization?
        
        8. **Complete Implementation Roadmap**
           - Phase 1 (quick wins): 1-2 hours
           - Phase 2 (medium): 1-2 weeks
           - Phase 3 (advanced): 1+ month
        
        For EACH recommendation provide:
        - Root cause analysis
        - Exact SQL with comments
        - Cost savings ($/month)
        - Time savings (%)
        - Implementation effort (hours)
        - Risk assessment
        - ROI (savings / effort)
        
        Rank by ROI (highest first)!
        
        Be specific, technical, and quantify EVERYTHING!
        """)
        
        return prompt

    def _format_queries_section(self, queries: List[Dict]) -> str:
        """Format queries section."""
        text = "### 1. Direct Query Costs (Top 20)\n"
        for q in queries[:10]:
            text += f"- **#{q['rank']}** Job: {q['job_id'][:20]}\n"
            text += f"  Cost: ${q['estimated_cost']}/run | Scanned: {q['bytes_scanned_gb']} GB | Time: {q['execution_seconds']}s\n"
            if q['parent_job_id']:
                text += f"  Parent: {q['parent_job_id'][:20]} (cascading!)\n"
        return text

    def _format_cascades_section(self, cascades: List[Dict]) -> str:
        """Format cascades section."""
        text = f"### 2. Cascading Job Multipliers ({len(cascades)} patterns)\n"
        if cascades:
            for c in cascades[:5]:
                text += f"- Parent {c['parent_job_id'][:20]}: {c['child_count']} children\n"
                text += f"  Total cost: ${c['cost']} | Multiplier: {c['multiplier']}x\n"
        else:
            text += "- No cascading patterns detected\n"
        return text

    def _format_views_section(self, views: List[Dict]) -> str:
        """Format views section."""
        text = f"### 3. View Expansion Multipliers ({len(views)} views)\n"
        if views:
            for v in views[:5]:
                text += f"- {v['name']}: {v['complexity']} ({v['expansion_factor']}x)\n"
                for issue in v['issues']:
                    text += f"  ‚Ä¢ {issue}\n"
        else:
            text += "- No views found or insufficient permissions\n"
        return text

    def _format_mvs_section(self, mvs: List[Dict]) -> str:
        """Format materialized views section."""
        text = f"### 4. Materialized View Refresh Costs ({len(mvs)} MVs)\n"
        if mvs:
            for m in mvs[:5]:
                text += f"- {m['name']}: {m['size_gb']} GB\n"
                text += f"  If hourly refresh: ${m['monthly_cost_hourly']}/month\n"
                text += f"  If 4x daily: ${m['monthly_cost_4x']}/month\n"
        else:
            text += "- No materialized views found\n"
        return text

    def _format_scheduled_section(self, scheduled: List[Dict]) -> str:
        """Format scheduled queries section."""
        text = f"### 5. Scheduled Query Cascades ({len(scheduled)} queries)\n"
        if scheduled:
            for s in scheduled[:5]:
                text += f"- Frequency: {s['frequency']} | Cost: ${s['monthly_cost']}/month\n"
        else:
            text += "- No scheduled queries detected\n"
        return text

    def _format_external_section(self, external: List[Dict]) -> str:
        """Format external tables section."""
        text = f"### 6. External Table Costs ({len(external)} tables)\n"
        if external:
            for e in external[:5]:
                text += f"- {e['name']}: ${e['cost_per_query']}/query\n"
        else:
            text += "- No external tables found\n"
        return text

    def _format_functions_section(self, functions: List[Dict]) -> str:
        """Format functions section."""
        text = f"### 7. Function/Procedure Overhead ({len(functions)} functions)\n"
        if functions:
            expensive = [f for f in functions if f['is_expensive']]
            if expensive:
                for f in expensive[:5]:
                    text += f"- {f['name']} ({f['severity']})\n"
                    for issue in f['issues']:
                        text += f"  ‚Ä¢ {issue}\n"
        else:
            text += "- No functions/procedures found\n"
        return text

    def _format_nested_section(self, nested: List[Dict]) -> str:
        """Format nested structures section."""
        text = f"### 8. Nested Structure Explosion ({len(nested)} tables)\n"
        if nested:
            for n in nested[:5]:
                text += f"- {n['table']}: {len(n['fields'])} nested fields\n"
                text += f"  Total multiplier: {n['total_multiplier']}x\n"
        else:
            text += "- No nested structures found\n"
        return text

    def _format_columns_section(self, columns: List[Dict]) -> str:
        """Format columns section."""
        text = f"### 9. Column-Level Scan Costs ({len(columns)} tables)\n"
        if columns:
            for c in columns[:5]:
                text += f"- {c['table']}: {c['total_columns']} columns\n"
                text += f"  Risk: {c['select_star_risk']}\n"
        else:
            text += "- No column analysis available\n"
        return text

    def _format_patterns_section(self, patterns: List[Dict]) -> str:
        """Format patterns section."""
        text = f"### 10. Optimization Patterns ({len(patterns)} opportunities)\n"
        if patterns:
            for p in patterns[:10]:
                text += f"- **{p['type']}** ({p['severity']}): {p['description']}\n"
                text += f"  Savings: ${p['potential_savings']:.2f}/run | Speedup: {p['speedup']}\n"
        else:
            text += "- No obvious patterns detected\n"
        return text


def main():
    parser = argparse.ArgumentParser(
        description="Advanced Hybrid Prompt Generator - Complete Full Version (700+ lines)"
    )
    parser.add_argument("--project", required=True, help="GCP project ID")
    parser.add_argument("--output", default="advanced_prompt_full.txt", help="Output file")
    
    args = parser.parse_args()
    
    try:
        generator = AdvancedHybridPromptGeneratorCompleteFull(args.project)
        prompt = generator.generate()
        
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(prompt)
        
        logger.info("\n" + "=" * 90)
        logger.info("‚úÖ COMPLETE FULL ADVANCED PROMPT GENERATED!")
        logger.info("=" * 90)
        logger.info(f"\nüìÑ Saved to: {args.output}")
        logger.info("\nüìã NEXT STEPS:")
        logger.info("1. Copy advanced_prompt_full.txt content")
        logger.info("2. Go to https://claude.ai")
        logger.info("3. Start new conversation")
        logger.info("4. Paste entire prompt")
        logger.info("5. Get expert in-depth recommendations!")
        logger.info("\nüí∞ COST: $0 (using free Claude.ai)")
        logger.info("‚è±Ô∏è  Analysis includes: All 12 data points, hidden costs, cascades, views, MVs, scheduled queries, external tables, functions, nested structures, columns, patterns!\n")
        
        exit(0)
    
    except Exception as e:
        logger.error(f"‚ùå Failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        exit(1)


if __name__ == "__main__":
    main()
