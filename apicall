dataQuality:
  version: "1.0"
  description: >-
    Join table1â‡„table2 on col1, col2, col3 and run an equality check
    on these 10 columns
  joinKeys: [col1, col2, col3]
  comparison:
    ruleType: equality
    columns: [col1, col2, col3, col4, col5, col6, col7, col8, col9, col10]


package com.trustiq.config;

import java.util.List;

// top-level AppConfig
public class AppConfig {
  private DataQualityConfig dataQuality;
  public DataQualityConfig getDataQuality() { return dataQuality; }
  public void setDataQuality(DataQualityConfig dq) { this.dataQuality = dq; }
}

// dataQuality section
public class DataQualityConfig {
  private String version;
  private String description;
  private List<String> joinKeys;
  private ComparisonConfig comparison;
  public String getVersion() { return version; }
  public void setVersion(String v) { version = v; }
  public String getDescription() { return description; }
  public void setDescription(String d) { description = d; }
  public List<String> getJoinKeys() { return joinKeys; }
  public void setJoinKeys(List<String> j) { joinKeys = j; }
  public ComparisonConfig getComparison() { return comparison; }
  public void setComparison(ComparisonConfig c) { comparison = c; }
}

// comparison block
public class ComparisonConfig {
  private String ruleType;
  private List<String> columns;
  public String getRuleType() { return ruleType; }
  public void setRuleType(String r) { ruleType = r; }
  public List<String> getColumns() { return columns; }
  public void setColumns(List<String> c) { columns = c; }
}


package com.trustiq.config;

import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.dataformat.yaml.YAMLFactory;
import com.google.cloud.storage.*;
import java.io.InputStreamReader;
import java.io.Reader;

public class ConfigLoader {
  private static final ObjectMapper MAPPER =
    new ObjectMapper(new YAMLFactory())
      .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);

  public static AppConfig load(String gcsUri) throws Exception {
    Storage st = StorageOptions.getDefaultInstance().getService();
    Blob b = st.get( BlobId.fromGsUtilUri(gcsUri) );
    try (Reader r = new InputStreamReader(b.getContent())) {
      return MAPPER.readValue(r, AppConfig.class);
    }
  }
}


package com.trustiq.pipeline;

import com.google.api.services.bigquery.model.TableRow;
import com.google.cloud.bigquery.*;
import com.google.cloud.storage.BlobId;
import com.trustiq.config.*;
import org.apache.beam.runners.dataflow.DataflowRunner;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.*;
import org.apache.beam.sdk.options.*;
import org.apache.beam.sdk.transforms.*;
import org.apache.beam.sdk.transforms.join.*;
import org.apache.beam.sdk.values.*;
import java.time.Instant;
import java.time.LocalDate;
import java.util.*;
import java.util.stream.Collectors;

public class DataQualityPipeline {

  public interface Options extends PipelineOptions {
    @Description("GCS URI to app-config.yaml")
    @Validation.Required String getConfigGcsPath();
    void setConfigGcsPath(String v);
  }

  public static void main(String[] args) throws Exception {
    Options opts = PipelineOptionsFactory.fromArgs(args)
                          .withValidation()
                          .as(Options.class);
    opts.setRunner(DataflowRunner.class);

    // load config
    AppConfig app = ConfigLoader.load(opts.getConfigGcsPath());
    DataQualityConfig dq = app.getDataQuality();
    List<String> joinKeys = dq.getJoinKeys();
    List<String> columns  = dq.getComparison().getColumns();
    String ruleType       = dq.getComparison().getRuleType();

    // tables (could also come from your config)
    String table1 = "PROJECT:DS.table1";
    String table2 = "PROJECT:DS.table2";
    String output = "PROJECT:DS.compare_audit";

    // fetch schema from table1
    BigQuery bq = BigQueryOptions.getDefaultInstance().getService();
    TableId tid = TableId.of("PROJECT","DS","table1");
    Schema schema = bq.getTable(tid).getDefinition().getSchema();
    Map<String, LegacySQLTypeName> typeMap = schema.getFields()
      .stream().collect(Collectors.toMap(Field::getName, Field::getType));

    // set up Beam
    Pipeline p = Pipeline.create(opts);
    PCollectionView<Map<String,LegacySQLTypeName>> typeView =
      p.apply("TypeMap", Create.of(typeMap)).apply(View.asSingleton());

    // keying function
    SerializableFunction<TableRow,String> keyFn = row ->
      joinKeys.stream()
        .map(k -> Objects.toString(row.get(k),""))
        .collect(Collectors.joining("|"));

    // read & key table1
    PCollection<KV<String,TableRow>> left = p
      .apply("Read1", BigQueryIO.readTableRows()
        .from(table1)
        .withMethod(BigQueryIO.TypedRead.Method.DIRECT_READ))
      .apply("Key1", ParDo.of(new DoFn<TableRow,KV<String,TableRow>>() {
        @ProcessElement public void proc(ProcessContext c) {
          c.output(KV.of(keyFn.apply(c.element()), c.element()));
        }
      }));

    // read & key table2
    PCollection<KV<String,TableRow>> right = p
      .apply("Read2", BigQueryIO.readTableRows()
        .from(table2)
        .withMethod(BigQueryIO.TypedRead.Method.DIRECT_READ))
      .apply("Key2", ParDo.of(new DoFn<TableRow,KV<String,TableRow>>() {
        @ProcessElement public void proc(ProcessContext c) {
          c.output(KV.of(keyFn.apply(c.element()), c.element()));
        }
      }));

    // join
    final TupleTag<TableRow> L = new TupleTag<>();
    final TupleTag<TableRow> R = new TupleTag<>();
    PCollection<KV<String,CoGbkResult>> joined = KeyedPCollectionTuple
      .of(L,left).and(R,right)
      .apply(CoGroupByKey.create());

    // compare
    PCollection<KV<String,long[]>> comps = joined
      .apply("Compare", ParDo.of(new DoFn<KV<String,CoGbkResult>,KV<String,long[]>>() {
        @ProcessElement public void proc(ProcessContext c) {
          Map<String,LegacySQLTypeName> types = c.sideInput(typeView);
          CoGbkResult gbk = c.element().getValue();
          for (TableRow a : gbk.getAll(L))
            for (TableRow b : gbk.getAll(R))
              for (String col : columns) {
                Object v1 = a.get(col), v2 = b.get(col);
                boolean eq = equalsTyped(v1, v2, types.get(col));
                String rule = col + "_" + ruleType;
                c.output(KV.of(rule, new long[]{ eq?1L:0L, eq?0L:1L }));
              }
        }
      }).withSideInputs(typeView));

    // aggregate
    PCollection<KV<String,long[]>> summed = comps
      .apply(Combine.perKey(new Combine.CombineFn<long[],long[],long[]>() {
        public long[] createAccumulator()          { return new long[]{0,0}; }
        public long[] addInput(long[] a,long[] b)   { return new long[]{a[0]+b[0],a[1]+b[1]}; }
        public long[] mergeAccumulators(Iterable<long[]> it) {
          long e=0,n=0; for(long[] x:it){ e+=x[0]; n+=x[1]; } return new long[]{e,n};
        }
        public long[] extractOutput(long[] acc)     { return acc; }
      }));

    // to TableRow + write
    summed
      .apply("ToBQ", MapElements.into(TypeDescriptor.of(TableRow.class))
        .via(kv -> {
          long[] ct = kv.getValue();
          return new TableRow()
            .set("rule",             kv.getKey())
            .set("equals_count",     ct[0])
            .set("not_equals_count", ct[1])
            .set("run_date",         Instant.now().toString());
        }))
      .apply("Write", BigQueryIO.writeTableRows()
        .to(output)
        .withSchema(new TableSchema().setFields(Arrays.asList(
          new TableFieldSchema().setName("rule").setType("STRING"),
          new TableFieldSchema().setName("equals_count").setType("INTEGER"),
          new TableFieldSchema().setName("not_equals_count").setType("INTEGER"),
          new TableFieldSchema().setName("run_date").setType("TIMESTAMP")
        )))
        .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)
        .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE)
      );

    p.run().waitUntilFinish();
  }

  private static boolean equalsTyped(Object o1, Object o2, LegacySQLTypeName type) {
    if (o1==null && o2==null) return true;
    if (o1==null || o2==null) return false;
    try {
      switch(type) {
        case STRING:    return o1.toString().equals(o2.toString());
        case INTEGER:   return ((Number)o1).longValue()==((Number)o2).longValue();
        case FLOAT:     return Float.compare(((Number)o1).floatValue(),
                                             ((Number)o2).floatValue())==0;
        case DOUBLE:    return Double.compare(((Number)o1).doubleValue(),
                                              ((Number)o2).doubleValue())==0;
        case BOOLEAN:   return Boolean.parseBoolean(o1.toString())
                              == Boolean.parseBoolean(o2.toString());
        case DATE:      return LocalDate.parse(o1.toString())
                              .equals(LocalDate.parse(o2.toString()));
        case TIMESTAMP: return Instant.parse(o1.toString())
                              .equals(Instant.parse(o2.toString()));
        default:        return o1.toString().equals(o2.toString());
      }
    } catch(Exception e) {
      return false;
    }
  }
}
