from airflow import DAG
from airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator
from airflow.utils.dates import days_ago

default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
}

dag = DAG(
    'example_partitioned_bigquery_to_gcs',
    default_args=default_args,
    schedule_interval=None,
)

# Example: Suppose you want to create 50 files and have 60000 rows in total
total_rows = 60000  # Replace with the actual number of rows in your BigQuery table
desired_files = 50
rows_per_file = total_rows // desired_files

# Define the partitions (example with 5 partitions, adjust as needed)
partitions = [
    {'start_id': i * rows_per_file, 'end_id': (i + 1) * rows_per_file - 1}
    for i in range(desired_files)
]

# Create tasks for each partition
for i, partition in enumerate(partitions):
    task = BigQueryToGCSOperator(
        task_id=f'export_partition_{i}',
        source_project_dataset_table='your_project.your_dataset.your_table',
        destination_cloud_storage_uris=[f'gs://your-bucket/your-folder/partition_{i}/*.json.gz'],
        export_format='NEWLINE_DELIMITED_JSON',
        compression='GZIP',
        sql=f"""
            SELECT * FROM `your_project.your_dataset.your_table`
            WHERE id BETWEEN {partition['start_id']} AND {partition['end_id']}
        """,
        dag=dag,
    )

# This will create 50 partitions, each having approximately rows_per_file rows






SELECT 
  PARSE_TIMESTAMP("%Y-%m-%d %H:%M:%S %Ez", "2024-05-10 09:52:17.863000 +00:00", "UTC") as utc_timestamp
FROM 
  your_table;

-- If the string is just a datetime without timezone
SELECT 
  TIMESTAMP("2024-05-10T09:52:17.863000", "UTC") as utc_timestamp
FROM 
  your_table;


Cloud Scheduler: Triggers the process.
Secret Manager: Fetches the API key securely.
BigQuery: Reads the initial 100 records.
API Call: Using the fetched API key, calls the external API.
Process API Response: Handles and processes the data returned from the API.
Update BigQuery: Updates the records in BigQuery based on the API response.
Cloud Logging: Logs the operation details.

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryInsertError;
import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;
import org.apache.beam.sdk.options.Default;
import org.apache.beam.sdk.options.Description;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.SerializableFunction;
import org.apache.beam.sdk.transforms.Sum;
import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
import org.apache.beam.sdk.transforms.windowing.GlobalWindow;
import org.apache.beam.sdk.transforms.windowing.Window;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PCollectionView;
import org.apache.beam.sdk.values.TypeDescriptor;
import org.apache.beam.sdk.values.TypedValue;
import org.joda.time.Duration;

public class DataflowBatch {
    public interface MyOptions extends PipelineOptions {
        @Description("Input Pub/Sub topic")
        @Default.String("projects/YOUR_PROJECT_ID/topics/YOUR_TOPIC")
        String getInputTopic();
        void setInputTopic(String value);

        @Description("Output BigQuery table")
        @Default.String("YOUR_PROJECT_ID:YOUR_DATASET.YOUR_TABLE")
        String getOutputTable();
        void setOutputTable(String value);
    }

    public static void main(String[] args) {
        MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class);

        Pipeline pipeline = Pipeline.create(options);

        PCollection<KV<Integer, EmployeePojo>> transformedRecords = pipeline
            .apply("Read from Pub/Sub", PubsubIO.readAvros(EmployeePojo.class).fromTopic(options.getInputTopic()))
            .apply("Transform Avro to Full Name", ParDo.of(new TransformToFullNameFn()))
            .apply("Window into Global Window", Window.into(new GlobalWindow()));

        PCollection<EmployeePojo> successfullyInsertedRecords = transformedRecords
            .apply("Insert into BigQuery",
                BigQueryIO.<EmployeePojo>write()
                    .to(options.getOutputTable())
                    .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)
                    .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)
                    .withExtendedErrorInfo()
                    .withMethod(BigQueryIO.Write.Method.STORAGE_WRITE_API)
                    .withFormatFunction(new SerializableFunction<EmployeePojo, TableRow>() {
                        @Override
                        public TableRow apply(EmployeePojo input) {
                            TableRow row = new TableRow();
                            row.set("employeeId", input.getId());
                            row.set("fullName", input.getFullName());
                            return row;
                        }
                    }));

        PCollectionView<Iterable<TableRow>> successfulRecordsView = successfullyInsertedRecords
            .apply("Convert to View", ParDo.of(new ConvertToIterableFn()))
            .apply("Collect Successful Records", View.asIterable());

        PCollection<EmployeePojo> missingRecords = transformedRecords
            .apply("Identify Missing Records", ParDo.of(new IdentifyMissingRecordsFn(successfulRecordsView)));

        missingRecords
            .apply("Write Missing Records to BigQuery",
                BigQueryIO.<EmployeePojo>write()
                    .to(options.getOutputTable())
                    .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)
                    .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)
                    .withExtendedErrorInfo()
                    .withMethod(BigQueryIO.Write.Method.STORAGE_WRITE_API)
                    .withFormatFunction(new SerializableFunction<EmployeePojo, TableRow>() {
                        @Override
                        public TableRow apply(EmployeePojo input) {
                            TableRow row = new TableRow();
                            row.set("employeeId", input.getId());
                            row.set("fullName", input.getFullName());
                            return row;
                        }
                    }));

        pipeline.run().waitUntilFinish();
    }

    public static class TransformToFullNameFn extends DoFn<EmployeePojo, KV<Integer, EmployeePojo>> {
        @ProcessElement
        public void processElement(@Element EmployeePojo input, OutputReceiver<KV<Integer, EmployeePojo>> out) {
            // Transform input to include full name
            input.setFullName(input.getFirstName() + " " + input.getLastName());
            out.output(KV.of(input.getId(), input));
        }
    }

    public static class ConvertToIterableFn extends DoFn<EmployeePojo, Iterable<TableRow>> {
        @ProcessElement
        public void processElement(@Element EmployeePojo input, OutputReceiver<Iterable<TableRow>> out) {
            out.output(TypedValue.of(TableRow.class, input.toTableRow()));
        }
    }

    public static class IdentifyMissingRecordsFn extends DoFn<KV<Integer, EmployeePojo>, EmployeePojo> {
        private final PCollectionView<Iterable<TableRow>> successfulRecordsView;

        public IdentifyMissingRecordsFn(PCollectionView<Iterable<TableRow>> successfulRecordsView) {
            this.successfulRecordsView = successfulRecordsView;
        }

        @ProcessElement
        public void processElement(@Element KV<Integer, EmployeePojo> input, BoundedWindow window, OutputReceiver<EmployeePojo> out) {
            Iterable<TableRow> successfulRecords = this.successfulRecordsView.get(window);
            boolean found = false;
            for (TableRow record : successfulRecords) {
                if (record.get("employeeId").equals(input.getValue().getId())) {
                    found = true;
                    break;
                }
            }
            if (!found) {
                out.output(input.getValue());
            }
        }
    }
}




import org.apache.beam.sdk.coders.AtomicCoder;
import org.apache.beam.sdk.coders.Coder;
import org.apache.beam.sdk.coders.CoderException;

import java.io.*;

public class EmployeeCoder extends AtomicCoder<Employee> {

    public static EmployeeCoder of() {
        return new EmployeeCoder();
    }

    @Override
    public void encode(Employee value, OutputStream outStream) throws CoderException, IOException {
        if (value == null) {
            throw new CoderException("Cannot encode null values");
        }
        ObjectOutputStream oos = new ObjectOutputStream(outStream);
        oos.writeObject(value);
        oos.flush();
    }

    @Override
    public Employee decode(InputStream inStream) throws CoderException, IOException {
        ObjectInputStream ois = new ObjectInputStream(inStream);
        try {
            return (Employee) ois.readObject();
        } catch (ClassNotFoundException e) {
            throw new CoderException("Failed to decode Employee", e);
        }
    }
}

PipelineOptions options = PipelineOptionsFactory.create();
Pipeline p = Pipeline.create(options);

// Register the custom coder for the Employee class
p.getCoderRegistry().registerCoderForClass(Employee.class, EmployeeCoder.of());



public class Employee {
    private int id;
    private String name;
    private String department;

    public Employee(int id, String name, String department) {
        this.id = id;
        this.name = name;
        this.department = department;
    }

    public int getId() {
        return id;
    }

    public String getName() {
        return name;
    }

    public String getDepartment() {
        return department;
    }
}

public class TableRow {
    private Map<String, String> fields;

    public TableRow(int employeeId) {
        fields = new HashMap<>();
        fields.put("employee_id", Integer.toString(employeeId));
    }

    public int getEmployeeId() {
        return Integer.parseInt(fields.get("employee_id"));
    }
}

public static void main(String[] args) {
    PipelineOptions options = PipelineOptionsFactory.create();
    Pipeline p = Pipeline.create(options);

    // Create dummy data for Employees
    List<KV<Integer, Employee>> employeeData = Arrays.asList(
        KV.of(1, new Employee(1, "Alice", "Engineering")),
        KV.of(2, new Employee(2, "Bob", "Marketing")),
        KV.of(3, new Employee(3, "Charlie", "Engineering")),
        KV.of(4, new Employee(4, "David", "HR")),
        KV.of(5, new Employee(5, "Eve", "Finance"))
    );

    // Create dummy data for TableRows
    List<KV<Integer, TableRow>> tableRowData = Arrays.asList(
        KV.of(1, new TableRow(1)),
        KV.of(3, new TableRow(3)),
        KV.of(5, new TableRow(5))
    );

    // Create PCollections from the dummy data
    PCollection<KV<Integer, Employee>> employees = p.apply("CreateEmployeeData", Create.of(employeeData));
    PCollection<KV<Integer, TableRow>> tableRows = p.apply("CreateTableRowData", Create.of(tableRowData));

    final TupleTag<Employee> employeeTag = new TupleTag<>();
    final TupleTag<TableRow> tableRowTag = new TupleTag<>();

    PCollection<KV<Integer, CoGbkResult>> grouped =
        KeyedPCollectionTuple.of(employeeTag, employees)
                             .and(tableRowTag, tableRows)
                             .apply(CoGroupByKey.create());

    PCollection<Employee> missingEmployees = grouped.apply(ParDo.of(new DoFn<KV<Integer, CoGbkResult>, Employee>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            KV<Integer, CoGbkResult> e = c.element();
            Iterable<Employee> employeeGroup = e.getValue().getAll(employeeTag);
            Iterable<TableRow> tableRowGroup = e.getValue().getAll(tableRowTag);

            if (employeeGroup.iterator().hasNext() && !tableRowGroup.iterator().hasNext()) {
                for (Employee emp : employeeGroup) {
                    c.output(emp);
                }
            }
        }
    }));

    p.run().waitUntilFinish();
}















PCollection<Employee> missingEmployees = grouped.apply(ParDo.of(new DoFn<KV<Integer, CoGbkResult>, Employee>() {
    @ProcessElement
    public void processElement(ProcessContext c) {
        KV<Integer, CoGbkResult> e = c.element();
        Iterable<Employee> employeeGroup = e.getValue().getAll(employeeTag);
        Iterable<TableRow> tableRowGroup = e.getValue().getAll(tableRowTag);

        // If employee exists but no corresponding TableRow
        if (employeeGroup.iterator().hasNext() && !tableRowGroup.iterator().hasNext()) {
            for (Employee emp : employeeGroup) {
                c.output(emp);
            }
        }
    }
}));

PCollection<TableRow> employeeTableRows = missingEmployees.apply("ConvertToTableRow", ParDo.of(new DoFn<Employee, TableRow>() {
    @ProcessElement
    public void processElement(@Element Employee employee, OutputReceiver<TableRow> out) {
        TableRow row = new TableRow()
                .set("employee_id", employee.getId())
                .set("name", employee.getName())
                .set("department", employee.getDepartment()) // Assuming department is a field
                .set("position", employee.getPosition()); // Assuming position is a field
        out.output(row);
    }
}));

employeeTableRows.apply("WriteToBigQuery", BigQueryIO.writeTableRows()
    .to("your-project:your_dataset.your_table")
    .withSchema(getSchema()) // Define and provide the schema
    .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)
    .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND));

public static TableSchema getSchema() {
    List<TableFieldSchema> fields = new ArrayList<>();
    fields.add(new TableFieldSchema().setName("employee_id").setType("INTEGER"));
    fields.add(new TableFieldSchema().setName("name").setType("STRING"));
    fields.add(new TableFieldSchema().setName("department").setType("STRING"));
    fields.add(new TableFieldSchema().setName("position").setType("STRING"));
    return new TableSchema().setFields(fields);
}

///////////////////

missingEmployees.apply(ParDo.of(new LogEmployeeIdFn()));


public static class LogEmployeeIdFn extends DoFn<Employee, Void> {
    private static final Logger LOG = Logger.getLogger(LogEmployeeIdFn.class.getName());

    @ProcessElement
    public void processElement(@Element Employee employee, OutputReceiver<Void> out) {
        LOG.info("Missing Employee ID: " + employee.getId());
    }
}



PCollection<KV<Integer, Employee>> keyedEmployees = employees.apply(
    MapElements.via(new SimpleFunction<Employee, KV<Integer, Employee>>() {
        public KV<Integer, Employee> apply(Employee emp) {
            return KV.of(emp.getId(), emp);
        }
    }));

PCollection<KV<Integer, TableRow>> keyedTableRows = tableRows.apply(
    MapElements.via(new SimpleFunction<TableRow, KV<Integer, TableRow>>() {
        public KV<Integer, TableRow> apply(TableRow row) {
            // Assuming the TableRow has a method getEmployeeId to fetch the ID
            return KV.of(row.getEmployeeId(), row);
        }
    }));
	
final TupleTag<Employee> employeeTag = new TupleTag<>();
final TupleTag<TableRow> tableRowTag = new TupleTag<>();

PCollection<KV<Integer, CoGbkResult>> grouped =
    KeyedPCollectionTuple.of(employeeTag, keyedEmployees)
                         .and(tableRowTag, keyedTableRows)
                         .apply(CoGroupByKey.create());

PCollection<Employee> missingEmployees = grouped.apply(ParDo.of(new DoFn<KV<Integer, CoGbkResult>, Employee>() {
    @ProcessElement
    public void processElement(ProcessContext c) {
        KV<Integer, CoGbkResult> e = c.element();
        Iterable<Employee> employeeGroup = e.getValue().getAll(employeeTag);
        Iterable<TableRow> tableRowGroup = e.getValue().getAll(tableRowTag);

        // If employee exists but no corresponding TableRow
        if (!employeeGroup.iterator().hasNext() && tableRowGroup.iterator().hasNext()) {
            c.output(tableRowGroup.iterator().next());
        }

        // If TableRow exists but no corresponding Employee
        if (!tableRowGroup.iterator().hasNext() && employeeGroup.iterator().hasNext()) {
            c.output(employeeGroup.iterator().next());
        }
    }
}));


//////////////////







import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryInsertError;
import org.apache.beam.sdk.options.Default;
import org.apache.beam.sdk.options.Description;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.PCollection;

public class DataflowBatch {
    public interface MyOptions extends PipelineOptions {
        @Description("Input BigQuery table")
        @Default.String("YOUR_PROJECT_ID:YOUR_DATASET.YOUR_TABLE")
        String getInputTable();
        void setInputTable(String value);
    }

    public static void main(String[] args) {
        MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class);

        Pipeline pipeline = Pipeline.create(options);

        PCollection<TableRow> records = pipeline
            .apply("Read from BigQuery", BigQueryIO.readTableRows().from(options.getInputTable()));

        PCollection<BigQueryInsertError> failedInserts = records.apply("Insert into BigQuery",
            BigQueryIO.writeTableRows()
                .to(options.getInputTable())
                .withWriteDisposition(Write.WriteDisposition.WRITE_APPEND)
                .withCreateDisposition(Write.CreateDisposition.CREATE_IF_NEEDED)
                .withExtendedErrorInfo()
                .withMethod(BigQueryIO.Write.Method.STORAGE_WRITE_API)
        );

        PCollection<Long> totalRecords = records.apply("Count Total Records", Count.globally());

        PCollection<Long> successfulInsertsCount = failedInserts
            .apply("Filter Successful Inserts",
                ParDo.of(new DoFn<BigQueryInsertError, Long>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        c.output(1L);
                    }
                }))
            .apply("Count Successful Inserts", Sum.longsGlobally());

        PCollection<Long> missingRecordsCount = totalRecords
            .apply("Calculate Missing Records",
                ParDo.of(new DoFn<Long, Long>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        long totalRecords = c.element();
                        long successfulInserts = c.sideInput(successfulInsertsCount);
                        c.output(totalRecords - successfulInserts);
                    }
                }).withSideInputs(successfulInsertsCount));

        missingRecordsCount.apply("Log Missing Records",
            ParDo.of(new DoFn<Long, Void>() {
                @ProcessElement
                public void processElement(ProcessContext c) {
                    long missingRecords = c.element();
                    System.out.println("Missing records: " + missingRecords);
                }
            }));

        pipeline.run().waitUntilFinish();
    }
}

///////////////////

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;
import org.apache.beam.sdk.options.Default;
import org.apache.beam.sdk.options.Description;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PCollectionView;
import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
import org.apache.beam.sdk.transforms.windowing.FixedWindows;
import org.apache.beam.sdk.transforms.windowing.Window;
import org.joda.time.Duration;

public class DataflowBatch {
    public interface MyOptions extends PipelineOptions {
        @Description("Input Pub/Sub topic")
        @Default.String("projects/YOUR_PROJECT_ID/topics/YOUR_TOPIC")
        String getInputTopic();
        void setInputTopic(String value);

        @Description("Output BigQuery table")
        @Default.String("YOUR_PROJECT_ID:YOUR_DATASET.YOUR_TABLE")
        String getOutputTable();
        void setOutputTable(String value);
    }

    public static void main(String[] args) {
        MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class);

        Pipeline pipeline = Pipeline.create(options);

        PCollection<Employee> employees = pipeline
            .apply("Read Avro Messages", AvroIO.read(Employee.class).from(options.getInputTopic()));

        // Apply windowing to the employees collection
        PCollection<Employee> windowedEmployees = employees.apply(
            "Apply Windowing", Window.into(FixedWindows.of(Duration.standardMinutes(1))));

        PCollection<Long> totalRecords = windowedEmployees.apply("Count Total Records", Count.globally());

        PCollection<TableRow> successfulInserts = windowedEmployees.apply("Insert into BigQuery",
            BigQueryIO.<Employee>write()
                .to(options.getOutputTable())
                .withFormatFunction((Employee element) -> {
                    return new TableRow().set("fullName", element.getFirstName() + " " + element.getLastName());
                })
                .withWriteDisposition(WriteDisposition.WRITE_APPEND)
                .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)
                .withMethod(Method.STORAGE_WRITE_API));

        PCollectionView<Long> successfulInsertCount = successfulInserts
            .apply("Count Successful Inserts", Count.globally())
            .apply(View.asSingleton());

        PCollectionView<Long> totalRecordsView = totalRecords.apply(View.asSingleton());

        // Convert successfulInsertCount to PCollection<KV<String, Long>>
        PCollection<KV<String, Long>> successfulInsertCountPCollection = successfulInsertCount.apply("Convert to KV",
            Combine.globally(Count.<Long>combineFn())
                .asSingletonView()
                .apply(View.asSingleton())
                .apply("Wrap in KV", ParDo.of(new DoFn<Long, KV<String, Long>>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        c.output(KV.of("successfulInserts", c.element()));
                    }
                })));

        // Convert totalRecordsView to PCollection<KV<String, Long>>
        PCollection<KV<String, Long>> totalRecordsViewPCollection = totalRecordsView.apply("Convert to KV",
            Combine.globally(Count.<Long>combineFn())
                .asSingletonView()
                .apply(View.asSingleton())
                .apply("Wrap in KV", ParDo.of(new DoFn<Long, KV<String, Long>>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        c.output(KV.of("totalRecords", c.element()));
                    }
                })));

        PCollection<String> status = KeyedPCollectionTuple
            .of("successfulInserts", successfulInsertCountPCollection)
            .and("totalRecords", totalRecordsViewPCollection)
            .apply(CoGroupByKey.create())
            .apply(ParDo.of(new DoFn<KV<String, CoGbkResult>, String>() {
                @ProcessElement
                public void processElement(ProcessContext c) {
                    long successfulInsertCount = c.element().getValue().getOnly("successfulInserts", 0L);
                    long totalRecords = c.element().getValue().getOnly("totalRecords", 0L);
                    if (successfulInsertCount == totalRecords) {
                        c.output("All records successfully inserted into BigQuery.");
                    } else {
                        long missingRecords = totalRecords - successfulInsertCount;
                        c.output("Missing records: " + missingRecords);
                    }
                }
            }));

        // Print the status
        status.apply(ParDo.of(new DoFn<String, Void>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                System.out.println(c.element());
            }
        }));

        pipeline.run().waitUntilFinish();
    }
}







//////////////////////////



import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;
import org.apache.beam.sdk.options.Default;
import org.apache.beam.sdk.options.Description;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PCollectionView;
import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
import org.apache.beam.sdk.transforms.windowing.FixedWindows;
import org.apache.beam.sdk.transforms.windowing.Window;
import org.joda.time.Duration;

public class DataflowBatch {
    public interface MyOptions extends PipelineOptions {
        @Description("Input Pub/Sub topic")
        @Default.String("projects/YOUR_PROJECT_ID/topics/YOUR_TOPIC")
        String getInputTopic();
        void setInputTopic(String value);

        @Description("Output BigQuery table")
        @Default.String("YOUR_PROJECT_ID:YOUR_DATASET.YOUR_TABLE")
        String getOutputTable();
        void setOutputTable(String value);
    }

    public static void main(String[] args) {
        MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class);

        Pipeline pipeline = Pipeline.create(options);

        PCollection<Employee> employees = pipeline
            .apply("Read Avro Messages", AvroIO.read(Employee.class).from(options.getInputTopic()));

        // Apply windowing to the employees collection
        PCollection<Employee> windowedEmployees = employees.apply(
            "Apply Windowing", Window.into(FixedWindows.of(Duration.standardMinutes(1))));

        PCollection<Long> totalRecords = windowedEmployees.apply("Count Total Records", Count.globally());

        PCollection<TableRow> successfulInserts = windowedEmployees.apply("Insert into BigQuery",
            BigQueryIO.<Employee>write()
                .to(options.getOutputTable())
                .withFormatFunction((Employee element) -> {
                    return new TableRow().set("fullName", element.getFirstName() + " " + element.getLastName());
                })
                .withWriteDisposition(WriteDisposition.WRITE_APPEND)
                .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)
                .withMethod(Method.STORAGE_WRITE_API));

        PCollectionView<Long> successfulInsertCount = successfulInserts
            .apply("Count Successful Inserts", Count.globally())
            .apply(View.asSingleton());

        PCollectionView<Long> totalRecordsView = totalRecords.apply(View.asSingleton());

        PCollection<String> status = KeyedPCollectionTuple
            .of("successfulInserts", successfulInsertCount)
            .and("totalRecords", totalRecordsView)
            .apply(CoGroupByKey.create())
            .apply(ParDo.of(new DoFn<KV<String, CoGbkResult>, String>() {
                @ProcessElement
                public void processElement(ProcessContext c) {
                    long successfulInsertCount = c.element().getValue().getOnly("successfulInserts", 0L);
                    long totalRecords = c.element().getValue().getOnly("totalRecords", 0L);
                    if (successfulInsertCount == totalRecords) {
                        c.output("All records successfully inserted into BigQuery.");
                    } else {
                        long missingRecords = totalRecords - successfulInsertCount;
                        c.output("Missing records: " + missingRecords);
                    }
                }
            }));

        // Print the status
        status.apply(ParDo.of(new DoFn<String, Void>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                System.out.println(c.element());
            }
        }));

        pipeline.run().waitUntilFinish();
    }
}

////////////////////


// Transform TableRow to KV<String, Long>
PCollection<KV<String, Long>> successfulInserts = result
    .apply("ExtractKey", MapElements.into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.longs()))
        .via((TableRow row) -> KV.of(row.get("key_field").toString(), 1L))); // Adjust key_field as needed

// Apply windowing
PCollection<KV<String, Long>> successfulInsertsWithWindow = successfulInserts
    .apply("AddWindow", Window.into(FixedWindows.of(Duration.standardMinutes(1)))); // Adjust window duration as needed


// Get the successful inserts with windowing
PCollection<KV<TableRow, Long>> successfulInserts = result
    .getSuccessfulInserts()
    .apply("AddWindow", Window.into(FixedWindows.of(Duration.standardMinutes(1)))); // Adjust window duration as needed

// Convert TableRow to String and Long to String
PCollection<KV<String, String>> formattedSuccessfulInserts = successfulInserts
    .apply("FormatResult", MapElements.into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.strings()))
        .via((KV<TableRow, Long> element) -> {
            TableRow row = element.getKey();
            String key = "successfulInserts"; // Assuming you want this key for all elements
            String value = String.valueOf(element.getValue());
            return KV.of(key, value);
        }));


apply("CalculateMissingPercentage", ParDo.of(new DoFn<KV<String, CoGbkResult>, KV<String, Long>>() {
    @ProcessElement
    public void processElement(ProcessContext c) {
        CoGbkResult result = c.element().getValue();
        long successfulInserts = result.getOnly("successfulInserts", 0L);
        long totalRecords = result.getOnly("totalRecords", 0L);
        double percentage = (double) successfulInserts / totalRecords * 100.0;
        c.output(KV.of("Percentage of records successfully inserted", (long) percentage));
        long missing = totalRecords - successfulInserts;
        c.output(KV.of("Missing Records", missing));
    }
}));

apply("CalculateMissingPercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, KV<String, Long>>() {
    @ProcessElement
    public void processElement(ProcessContext c) {
        CoGbkResult result = c.element().getValue();
        long successfulInserts = result.getOnly("successfulInserts", 0L);
        long totalRecords = result.getOnly("totalRecords", 0L);
        double percentage = (double) successfulInserts / totalRecords * 100.0;
        c.output(KV.of("Percentage of records successfully inserted", (long) percentage));
        long missing = totalRecords - successfulInserts;
        c.output(KV.of("Missing Records", missing));
    }
}));


apply("CalculateMissingPercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, KV<String, Long>>() {
    @ProcessElement
    public void processElement(ProcessContext c) {
        CoGbkResult result = c.element().getValue();
        long successfulInserts = result.getOnly("successfulInserts", 0L);
        long totalRecords = result.getOnly("totalRecords", 0L);
        double percentage = (double) successfulInserts / totalRecords * 100.0;
        c.output(KV.of("Percentage of records successfully inserted", (long) percentage));
        long missing = totalRecords - successfulInserts;
        c.output(KV.of("Missing Records", missing));
    }
}));


// Calculate percentage of records successfully inserted
PCollection<KV<String, Long>> missingRecords = KeyedPCollectionTuple
        .of("successfulInserts", successfulInsertsView)
        .and("totalRecords", totalRecordsView)
        .apply(CoGroupByKey.create())
        .apply("CalculateMissingPercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, KV<String, Long>>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                long successfulInserts = c.element().getValue().getOnly("successfulInserts", 0L);
                long totalRecords = c.element().getValue().getOnly("totalRecords", 0L);
                double percentage = (double) successfulInserts / totalRecords * 100.0;
                c.output(KV.of("Percentage of records successfully inserted", (long) percentage));
                long missing = totalRecords - successfulInserts;
                c.output(KV.of("Missing Records", missing));
            }
        }));

// Print the percentage of missing records
missingRecords.apply("PrintMissingPercentage", ParDo.of(new DoFn<KV<String, Long>, Void>() {
    @ProcessElement
    public void processElement(ProcessContext c) {
        System.out.println(c.element().getKey() + ": " + c.element().getValue() + "%");
    }
}));


PCollection<KV<String, Long>> totalRecordsView = input
        .apply("CountTotalRecords", Count.globally())
        .apply(ParDo.of(new DoFn<Long, KV<String, Long>>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                c.output(KV.of("total", c.element()));
            }
        }));

// Retrieve the count of successful inserts
PCollection<KV<String, String>> successfulInsertsView = result
        .getSuccessfulInserts()
        .apply("ConvertSuccessfulInserts",
               ParDo.of(new DoFn<Long, KV<String, String>>() {
                   @ProcessElement
                   public void processElement(ProcessContext c) {
                       c.output(KV.of("success", c.element().toString()));
                   }
               }));

// Get the total count of records processed
PCollection<KV<String, String>> totalRecordsView = input
        .apply("CountTotalRecords", Count.globally())
        .apply("ConvertTotalRecords",
               ParDo.of(new DoFn<Long, KV<String, String>>() {
                   @ProcessElement
                   public void processElement(ProcessContext c) {
                       c.output(KV.of("total", c.element().toString()));
                   }
               }));


-------

PCollection<KV<String, String>> successfulInsertsView = result
    .getSuccessfulInserts()
    .apply("CountSuccessfulInserts", Count.perElement())
    .apply("FormatResult", MapElements.into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.strings()))
        .via((KV<TableRow, Long> element) -> KV.of("successfulInserts", String.valueOf(element.getValue()))));

successfulInsertsView
    .apply(View.asSingleton());

import com.google.api.services.bigquery.model.TableRow;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;
import org.apache.beam.sdk.options.*;
import org.apache.beam.sdk.transforms.*;
import org.apache.beam.sdk.transforms.windowing.*;
import org.apache.beam.sdk.values.*;
import org.joda.time.Duration;

public class DataflowJob {

    public interface MyOptions extends PipelineOptions, StreamingOptions {
        @Description("Pub/Sub subscription path")
        @Validation.Required
        ValueProvider<String> getSubscriptionPath();
        void setSubscriptionPath(ValueProvider<String> value);

        @Description("BigQuery output table")
        @Validation.Required
        ValueProvider<String> getOutputTable();
        void setOutputTable(ValueProvider<String> value);
    }

    public static void main(String[] args) {
        MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class);
        options.setStreaming(true);

        Pipeline pipeline = Pipeline.create(options);

        PCollection<TableRow> input = pipeline
                .apply("ReadFromPubSub", PubsubIO.readAvros(Employee.class)
                        .fromSubscription(options.getSubscriptionPath()))
                .apply("TransformEmployeeData", ParDo.of(new DoFn<Employee, TableRow>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        Employee emp = c.element();
                        // Perform transformation
                        TableRow row = new TableRow();
                        row.set("fullName", emp.getFirstName() + " " + emp.getLastName());
                        // Add more fields as needed
                        c.output(row);
                    }
                }));

        WriteResult result = input.apply("WriteToBigQuery", BigQueryIO.writeTableRows()
                .to(options.getOutputTable())
                .withSchema(getSchema())
                .withFormatFunction(row -> row)
                .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)
                .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED));

        // Retrieve the count of successful inserts
        PCollectionView<Long> successfulInsertsView = result
                .getSuccessfulInserts()
                .apply("CountSuccessfulInserts", Combine.globally(Count.<TableRow>combineFn()).withoutDefaults())
                .apply(View.asSingleton());

        // Get the total count of records processed
        PCollectionView<Long> totalRecordsView = input
                .apply("CountTotalRecords", Count.globally())
                .apply(View.asSingleton());

        // Calculate percentage of records successfully inserted
        PCollection<KV<String, Long>> missingRecords = KeyedPCollectionTuple
                .of("successfulInserts", successfulInsertsView)
                .and("totalRecords", totalRecordsView)
                .apply(CoGroupByKey.create())
                .apply("CalculateMissingPercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, KV<String, Long>>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        long successfulInserts = c.element().getValue().getOnly("successfulInserts", 0L);
                        long totalRecords = c.element().getValue().getOnly("totalRecords", 0L);
                        double percentage = (double) successfulInserts / totalRecords * 100.0;
                        c.output(KV.of("Percentage of records successfully inserted", (long) percentage));
                        long missing = totalRecords - successfulInserts;
                        c.output(KV.of("Missing Records", missing));
                    }
                }));

        // Print the percentage of missing records
        missingRecords.apply("PrintMissingPercentage", ParDo.of(new DoFn<KV<String, Long>, Void>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                System.out.println(c.element().getKey() + ": " + c.element().getValue() + "%");
            }
        }));

        pipeline.run();
    }

    private static TableSchema getSchema() {
        return new TableSchema()
                .setFields(
                        ImmutableList.of(
                                new TableFieldSchema().setName("fullName").setType("STRING")
                                // Add more fields as needed
                        )
                );
    }
}









------------------------



import com.google.api.services.bigquery.model.TableRow;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;
import org.apache.beam.sdk.options.*;
import org.apache.beam.sdk.transforms.*;
import org.apache.beam.sdk.transforms.windowing.*;
import org.apache.beam.sdk.values.*;
import org.joda.time.Duration;

public class DataflowJob {

    public interface MyOptions extends PipelineOptions, StreamingOptions {
        @Description("Pub/Sub subscription path")
        @Validation.Required
        ValueProvider<String> getSubscriptionPath();
        void setSubscriptionPath(ValueProvider<String> value);

        @Description("BigQuery output table")
        @Validation.Required
        ValueProvider<String> getOutputTable();
        void setOutputTable(ValueProvider<String> value);
    }

    public static void main(String[] args) {
        MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class);
        options.setStreaming(true);

        Pipeline pipeline = Pipeline.create(options);

        PCollection<TableRow> input = pipeline
                .apply("ReadFromPubSub", PubsubIO.readAvros(Employee.class)
                        .fromSubscription(options.getSubscriptionPath()))
                .apply("TransformEmployeeData", ParDo.of(new DoFn<Employee, TableRow>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        Employee emp = c.element();
                        // Perform transformation
                        TableRow row = new TableRow();
                        row.set("fullName", emp.getFirstName() + " " + emp.getLastName());
                        // Add more fields as needed
                        c.output(row);
                    }
                }));

        WriteResult result = input.apply("WriteToBigQuery", BigQueryIO.writeTableRows()
                .to(options.getOutputTable())
                .withSchema(getSchema())
                .withFormatFunction(row -> row)
                .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)
                .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED));

        // Retrieve the count of successful inserts
        PCollectionView<Long> successfulInsertsView = result
                .getSuccessfulInserts()
                .apply("CountSuccessfulInserts", Combine.globally(Count.<TableRow>combineFn()).withoutDefaults())
                .apply(View.asSingleton());

        // Get the total count of records processed
        PCollectionView<Long> totalRecordsView = input
                .apply("CountTotalRecords", Count.globally())
                .apply(View.asSingleton());

        // Calculate percentage of records successfully inserted
        PCollection<String> printPercentage = KeyedPCollectionTuple.of("successfulInserts", successfulInsertsView)
                .and("totalRecords", totalRecordsView)
                .apply(CoGroupByKey.create())
                .apply("CalculatePercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, String>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        long successfulInserts = c.element().getValue().getOnly("successfulInserts", 0L);
                        long totalRecords = c.element().getValue().getOnly("totalRecords", 0L);
                        double percentage = (double) successfulInserts / totalRecords * 100.0;
                        c.output("Percentage of records successfully inserted: " + percentage + "%");
                    }
                }));

        printPercentage.apply("PrintPercentage", ParDo.of(new DoFn<String, Void>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                System.out.println(c.element());
            }
        }));

        pipeline.run();
    }

    private static TableSchema getSchema() {
        return new TableSchema()
                .setFields(
                        ImmutableList.of(
                                new TableFieldSchema().setName("fullName").setType("STRING")
                                // Add more fields as needed
                        )
                );
    }
}
In this modified version:

We retrieve the total count of records processed using Count.globally() and store it in a PCollectionView<Long>.
We then use CoGroupByKey to join the successful inserts count and total records count.
After calculating the percentage of records successfully inserted, we print the result to the console.
Ensure that you have defined the Employee class and replace it with your actual Avro message structure. Also, provide appropriate Pub/Sub subscription and BigQuery table paths in the options.







======================


// Step 3: Recovery Mechanism
// Get the total count of records processed
PCollection<Long> totalRecordsProcessed = inputData.apply(Count.globally());

// Get the count of records successfully inserted into BigQuery
PCollection<Long> successfulInserts = result.getSuccessfulInserts().apply(Count.globally());

// Convert successful inserts count into a singleton view
final PCollectionView<Long> successfulInsertsView = successfulInserts.apply(View.asSingleton());

// Identify missing records
PCollection<Long> missingRecordsCount = totalRecordsProcessed
    .apply("Calculate Missing Records", ParDo.of(new DoFn<Long, Long>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            Long totalRecords = c.element();
            Long successfulInsertsCount = c.sideInput(successfulInsertsView);
            Long missingCount = totalRecords - successfulInsertsCount;
            c.output(missingCount);
        }
    }).withSideInputs(successfulInsertsView));

// Step 4: Logging and Reporting
missingRecordsCount.apply(Sum.longs())
    .apply(ParDo.of(new DoFn<Long, Void>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            Long missingRecordCount = c.element();
            // Log or report the count of missing records
            LOG.info("Missing records count: {}", missingRecordCount);
        }
    }));

// Step 1: Record Tracking
PCollection<YourData> inputData = /* Your input data */;
PCollectionView<Map<String, Boolean>> recordIdsView = inputData
    .apply("Extract Record IDs", ParDo.of(new DoFn<YourData, KV<String, Void>>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            String recordId = /* Extract unique identifier for each record */;
            c.output(KV.of(recordId, null));
        }
    }))
    .apply(View.asMap());

// Step 2: Error Handling (Assuming WriteResult is available)
WriteResult result = processedData.apply(/* Write to BigQuery operation */);
result.getFailedInsertsWithErr()
    .apply(/* Handle failed inserts */);

// Step 3: Recovery Mechanism
// Get the total count of records processed
long totalRecordsProcessed = inputData.apply(Count.globally());
// Get the count of records successfully inserted into BigQuery
PCollectionView<Long> successfulInsertsView = result
    .apply(Count.globally())
    .apply(View.asSingleton());

// Compare counts and identify missing records
PCollection<Long> missingRecordsCount = KeyedPCollectionTuple
    .of(new TupleTag<Void>(), View.<String, Boolean>asMap())
    .and(result.getSuccessfulInserts().apply(Count.globally()))
    .apply(CoGroupByKey.create())
    .apply(ParDo.of(new DoFn<KV<String, CoGbkResult>, Long>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            KV<String, CoGbkResult> element = c.element();
            if (!element.getValue().getOnly(new TupleTag<Void>())) {
                // Record is missing
                c.output(1L);
            }
        }
    }))
    .apply(Sum.longs());

// Step 4: Logging and Reporting
missingRecordsCount.apply(Sum.longs())
    .apply(ParDo.of(new DoFn<Long, Void>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            long missingRecordCount = c.element();
            // Log or report the count of missing records
            LOG.info("Missing records count: {}", missingRecordCount);
        }
    }));

// Additional steps for logging and reporting as needed



gsutil ls -l gs://your-bucket-name/** | awk '{ if ($2 < "'$(date -d '2 days ago' +%Y-%m-%d)'" ) print $NF }' | gsutil -m rm -I

#!/bin/bash

# Get the current time in epoch format
current_time=$(date +%s)

# Calculate the time two days ago in epoch format
two_days_ago=$((current_time - 2 * 24 * 3600))

# List files in the bucket and their metadata
gsutil ls -l gs://your-bucket-name > files.txt

# Parse the output and delete files older than two days
while read -r line; do
    # Extract the modification time of the file
    modification_time=$(echo "$line" | awk '{print $2}')

    # Convert modification time to epoch format
    modification_time_epoch=$(date -d "$modification_time" +%s)

    # Check if the file is older than two days
    if ((modification_time_epoch < two_days_ago)); then
        # Extract the filename from the line
        filename=$(echo "$line" | awk '{print $NF}')

        # Delete the file
        gsutil rm "$filename"
    fi
done < files.txt

# Remove the temporary file
rm files.txt



























To efficiently handle your requirement of consuming the API for address details in batches and then storing the responses in BigQuery, let's outline a solution that uses Google Cloud Platform (GCP) services to manage the workload effectively. This involves making HTTP requests to the external API, handling the response data, and storing it in BigQuery. We can improve on the previous example by incorporating batch processing and more robust error handling.

Recommended Architecture:
Cloud Function (API Consumer): This will handle the task of sending batch requests to the API.
BigQuery: For storing the processed data.
Pub/Sub (optional): For handling asynchronous processing if the response times are long or if the data needs further processing before being stored in BigQuery.
Detailed Steps and Sample Code in Java:
Cloud Function for Making API Requests: This function will trigger API requests to the API, handle the responses, and then store the data in BigQuery.
java
Copy code
import com.google.cloud.functions.HttpFunction;
import com.google.cloud.functions.HttpRequest;
import com.google.cloud.functions.HttpResponse;
import com.google.cloud.bigquery.BigQuery;
import com.google.cloud.bigquery.BigQueryOptions;
import com.google.cloud.bigquery.TableId;
import com.google.cloud.bigquery.InsertAllRequest;
import org.apache.http.client.fluent.Request;
import org.apache.http.entity.ContentType;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import java.util.List;
import java.util.ArrayList;

public class TestApiHandler implements HttpFunction {

    private static final String TEST_API_URL = "https://api.test.example/batch";  // Change to actual TEST API batch endpoint
    private static final BigQuery bigQuery = BigQueryOptions.getDefaultInstance().getService();

    @Override
    public void service(HttpRequest request, HttpResponse response) throws Exception {
        // Assume the request body contains JSON data to be sent to TEST API
        String requestBody = request.getReader().readLine();
        
        // Making the API call to TEST API
        try {
            String apiResponse = Request.Post(TEST_API_URL)
                .bodyString(requestBody, ContentType.APPLICATION_JSON)
                .execute()
                .returnContent()
                .asString();

            // Process the API response and prepare data for BigQuery
            List<Map<String, Object>> rowsToInsert = processDataForBigQuery(apiResponse);
            if (!rowsToInsert.isEmpty()) {
                insertIntoBigQuery(rowsToInsert);
            }
            response.getWriter().write("Data processed and stored successfully.");
        } catch (IOException e) {
            response.setStatusCode(500);
            response.getWriter().write("Failed to call TEST API: " + e.getMessage());
        }
    }

    private List<Map<String, Object>> processDataForBigQuery(String apiResponse) {
        // Parse the API response and prepare data for BigQuery insertion
        List<Map<String, Object>> rows = new ArrayList<>();
        // Logic to parse and structure data from API response
        return rows;
    }

    private void insertIntoBigQuery(List<Map<String, Object>> rows) throws InterruptedException {
        TableId tableId = TableId.of("your_dataset", "your_table");
        InsertAllRequest.Builder builder = InsertAllRequest.newBuilder(tableId);
        for (Map<String, Object> row : rows) {
            builder.addRow(row);
        }
        bigQuery.insertAll(builder.build());
    }
}
Key Elements:
HTTP Post Request: Uses Apache HttpClient (included in the dependency when deploying the function) to send a POST request to the TEST API.
Data Processing: After receiving the response, the data is processed and formatted as needed for insertion into BigQuery.
BigQuery Insertion: The processed data is then batch-inserted into BigQuery.
Deployment and Configuration:
Deploy this function via the Google Cloud Console or using the gcloud command-line tool.
Make sure that your Google Cloud project has enabled the APIs for Cloud Functions and BigQuery.
Configure the necessary permissions for the Cloud Function service account to access BigQuery and to make outbound network calls to the TEST API.
This architecture and code snippet should efficiently handle your requirement of processing address details in batches using the TEST API and storing the responses in BigQuery. Remember to replace placeholders with actual data and enhance security and error handling as per your production standards.
