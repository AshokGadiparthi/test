Got it! Let’s focus purely on Airflow monitoring and metadata tracking, excluding Slack/email alerts for now. We’ll aim to replicate OpenLineage-like functionality: capturing task status, execution times, inputs/outputs, runtime statistics, and SLA, all stored in one centralized table, but fully self-contained.



Here’s a complete solution.

1️⃣ BigQuery Table: Single Source of Truth


Table Name: airflow_metadata.task_runs

Column Name

Type

Description

run_id

STRING

Unique DAG run ID

dag_id

STRING

DAG name

task_id

STRING

Task name

execution_date

TIMESTAMP

DAG scheduled execution date

start_time

TIMESTAMP

Task start time

end_time

TIMESTAMP

Task end time

status

STRING

running, success, failed

duration_seconds

FLOAT

Task runtime

sla_seconds

FLOAT

SLA threshold for the task

sla_breached

BOOLEAN

True if duration > SLA

inputs

STRING

JSON list of input datasets/files

outputs

STRING

JSON list of output datasets/files

owner

STRING

DAG owner

environment

STRING

dev/prod

inserted_at

TIMESTAMP

Metadata insert time

This is sufficient to capture everything OpenLineage tracks except lineage graphs.
2️⃣ Python Utility: airflow_metadata_utils.py
import json
from datetime import datetime
from google.cloud import bigquery

BQ_PROJECT = "my_project"
BQ_DATASET = "airflow_metadata"
BQ_TABLE = "task_runs"

bq_client = bigquery.Client(project=BQ_PROJECT)
table_id = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}"

def record_task_metadata(task_instance, sla_seconds=None):
    """
    Record Airflow task metadata to BigQuery.
    """
    start_time = task_instance.start_date
    end_time = task_instance.end_date or datetime.utcnow()
    duration = (end_time - start_time).total_seconds() if start_time else None
    sla_breached = False
    if sla_seconds and duration and duration > sla_seconds:
        sla_breached = True

    # Optional: inputs/outputs from XCom
    inputs = task_instance.xcom_pull(task_ids=task_instance.task_id, key='inputs') or []
    outputs = task_instance.xcom_pull(task_ids=task_instance.task_id, key='outputs') or []

    row = {
        "run_id": task_instance.run_id,
        "dag_id": task_instance.dag_id,
        "task_id": task_instance.task_id,
        "execution_date": task_instance.execution_date,
        "start_time": start_time,
        "end_time": end_time,
        "status": task_instance.state,
        "duration_seconds": duration,
        "sla_seconds": sla_seconds,
        "sla_breached": sla_breached,
        "inputs": json.dumps(inputs),
        "outputs": json.dumps(outputs),
        "owner": task_instance.owner,
        "environment": "prod",
        "inserted_at": datetime.utcnow()
    }

    errors = bq_client.insert_rows_json(table_id, [row])
    if errors:
        print(f"Failed to insert row: {errors}")
3️⃣ Monitoring DAG: Tag-Based (CDP Example)
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.models import DagBag, DagRun
from airflow.utils.state import State
from airflow.utils.dates import days_ago
from airflow_metadata_utils import record_task_metadata

dag = DAG(
    dag_id="cdp_airflow_monitor",
    start_date=days_ago(1),
    schedule_interval="*/5 * * * *",
    catchup=False,
    max_active_runs=1,
    default_args={"owner": "airflow_monitor"}
)

def monitor_cdp_dags(tag="cdp", default_sla_seconds=600, **kwargs):
    session = kwargs['session']
    dagbag = DagBag()
    tagged_dag_ids = [dag_id for dag_id, dag in dagbag.dags.items() if tag in dag.tags]

    dag_runs = session.query(DagRun)\
        .filter(DagRun.dag_id.in_(tagged_dag_ids))\
        .filter(DagRun.state.in_([State.RUNNING, State.SUCCESS, State.FAILED]))\
        .all()

    for dag_run in dag_runs:
        for task_instance in dag_run.get_task_instances():
            # Each task can have its own SLA
            sla_seconds = getattr(task_instance.task, 'sla_seconds', default_sla_seconds)
            record_task_metadata(task_instance, sla_seconds=sla_seconds)

monitor_task = PythonOperator(
    task_id="monitor_cdp_tagged_dags",
    python_callable=monitor_cdp_dags,
    provide_context=True,
    dag=dag
)
4️⃣ CDP DAG Example With Per-Task SLA
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

dag = DAG(
    dag_id="gcs_to_bigquery_cdp",
    start_date=datetime(2025, 9, 23),
    schedule_interval="@daily",
    catchup=False,
    tags=["cdp"]
)

def task1_fn(**kwargs):
    kwargs['ti'].xcom_push(key='inputs', value=['gs://bucket/input1.csv'])
    kwargs['ti'].xcom_push(key='outputs', value=['bq://dataset/table1'])

def task2_fn(**kwargs):
    kwargs['ti'].xcom_push(key='inputs', value=['gs://bucket/input2.csv'])
    kwargs['ti'].xcom_push(key='outputs', value=['bq://dataset/table2'])

task1 = PythonOperator(
    task_id="task1_process_data",
    python_callable=task1_fn,
    provide_context=True,
    dag=dag
)
task1.sla_seconds = 300  # 5 minutes

task2 = PythonOperator(
    task_id="task2_process_data",
    python_callable=task2_fn,
    provide_context=True,
    dag=dag
)
task2.sla_seconds = 600  # 10 minutes

task1 >> task2
5️⃣ Features of This Setup
Per-task SLA: each task defines its own SLA.

Live status monitoring: running, success, failed.

Inputs/Outputs captured: via XCom.

Runtime metrics: duration, SLA breach flags.

Tag-based DAG filtering: only monitor DAGs tagged cdp.

Single BigQuery table: central store for all metadata.

Ready for dashboards: can visualize task runtimes, SLA breaches, etc.

✅ 
Optional Enhancements Later
Add task retries and try number.

Include custom metadata facets like dataset size, partition, data quality results.

Integrate Looker/Data Studio dashboards for monitoring SLA, runtime trends, and task failures.

Extend to all DAGs or multiple tags if needed.

If you want, I can create a diagram showing exactly how this solution replicates OpenLineage functionality for Airflow — including:

Task DAG → Task Execution → Monitor → BigQuery → Dashboard / SLA.



Do you want me to do that diagram next?

