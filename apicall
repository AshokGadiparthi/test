apply("CalculateMissingPercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, KV<String, Long>>() {
    @ProcessElement
    public void processElement(ProcessContext c) {
        CoGbkResult result = c.element().getValue();
        long successfulInserts = result.getOnly("successfulInserts", 0L);
        long totalRecords = result.getOnly("totalRecords", 0L);
        double percentage = (double) successfulInserts / totalRecords * 100.0;
        c.output(KV.of("Percentage of records successfully inserted", (long) percentage));
        long missing = totalRecords - successfulInserts;
        c.output(KV.of("Missing Records", missing));
    }
}));


apply("CalculateMissingPercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, KV<String, Long>>() {
    @ProcessElement
    public void processElement(ProcessContext c) {
        CoGbkResult result = c.element().getValue();
        long successfulInserts = result.getOnly("successfulInserts", 0L);
        long totalRecords = result.getOnly("totalRecords", 0L);
        double percentage = (double) successfulInserts / totalRecords * 100.0;
        c.output(KV.of("Percentage of records successfully inserted", (long) percentage));
        long missing = totalRecords - successfulInserts;
        c.output(KV.of("Missing Records", missing));
    }
}));


// Calculate percentage of records successfully inserted
PCollection<KV<String, Long>> missingRecords = KeyedPCollectionTuple
        .of("successfulInserts", successfulInsertsView)
        .and("totalRecords", totalRecordsView)
        .apply(CoGroupByKey.create())
        .apply("CalculateMissingPercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, KV<String, Long>>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                long successfulInserts = c.element().getValue().getOnly("successfulInserts", 0L);
                long totalRecords = c.element().getValue().getOnly("totalRecords", 0L);
                double percentage = (double) successfulInserts / totalRecords * 100.0;
                c.output(KV.of("Percentage of records successfully inserted", (long) percentage));
                long missing = totalRecords - successfulInserts;
                c.output(KV.of("Missing Records", missing));
            }
        }));

// Print the percentage of missing records
missingRecords.apply("PrintMissingPercentage", ParDo.of(new DoFn<KV<String, Long>, Void>() {
    @ProcessElement
    public void processElement(ProcessContext c) {
        System.out.println(c.element().getKey() + ": " + c.element().getValue() + "%");
    }
}));


PCollection<KV<String, Long>> totalRecordsView = input
        .apply("CountTotalRecords", Count.globally())
        .apply(ParDo.of(new DoFn<Long, KV<String, Long>>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                c.output(KV.of("total", c.element()));
            }
        }));

// Retrieve the count of successful inserts
PCollection<KV<String, String>> successfulInsertsView = result
        .getSuccessfulInserts()
        .apply("ConvertSuccessfulInserts",
               ParDo.of(new DoFn<Long, KV<String, String>>() {
                   @ProcessElement
                   public void processElement(ProcessContext c) {
                       c.output(KV.of("success", c.element().toString()));
                   }
               }));

// Get the total count of records processed
PCollection<KV<String, String>> totalRecordsView = input
        .apply("CountTotalRecords", Count.globally())
        .apply("ConvertTotalRecords",
               ParDo.of(new DoFn<Long, KV<String, String>>() {
                   @ProcessElement
                   public void processElement(ProcessContext c) {
                       c.output(KV.of("total", c.element().toString()));
                   }
               }));


-------

PCollection<KV<String, String>> successfulInsertsView = result
    .getSuccessfulInserts()
    .apply("CountSuccessfulInserts", Count.perElement())
    .apply("FormatResult", MapElements.into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.strings()))
        .via((KV<TableRow, Long> element) -> KV.of("successfulInserts", String.valueOf(element.getValue()))));

successfulInsertsView
    .apply(View.asSingleton());

import com.google.api.services.bigquery.model.TableRow;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;
import org.apache.beam.sdk.options.*;
import org.apache.beam.sdk.transforms.*;
import org.apache.beam.sdk.transforms.windowing.*;
import org.apache.beam.sdk.values.*;
import org.joda.time.Duration;

public class DataflowJob {

    public interface MyOptions extends PipelineOptions, StreamingOptions {
        @Description("Pub/Sub subscription path")
        @Validation.Required
        ValueProvider<String> getSubscriptionPath();
        void setSubscriptionPath(ValueProvider<String> value);

        @Description("BigQuery output table")
        @Validation.Required
        ValueProvider<String> getOutputTable();
        void setOutputTable(ValueProvider<String> value);
    }

    public static void main(String[] args) {
        MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class);
        options.setStreaming(true);

        Pipeline pipeline = Pipeline.create(options);

        PCollection<TableRow> input = pipeline
                .apply("ReadFromPubSub", PubsubIO.readAvros(Employee.class)
                        .fromSubscription(options.getSubscriptionPath()))
                .apply("TransformEmployeeData", ParDo.of(new DoFn<Employee, TableRow>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        Employee emp = c.element();
                        // Perform transformation
                        TableRow row = new TableRow();
                        row.set("fullName", emp.getFirstName() + " " + emp.getLastName());
                        // Add more fields as needed
                        c.output(row);
                    }
                }));

        WriteResult result = input.apply("WriteToBigQuery", BigQueryIO.writeTableRows()
                .to(options.getOutputTable())
                .withSchema(getSchema())
                .withFormatFunction(row -> row)
                .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)
                .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED));

        // Retrieve the count of successful inserts
        PCollectionView<Long> successfulInsertsView = result
                .getSuccessfulInserts()
                .apply("CountSuccessfulInserts", Combine.globally(Count.<TableRow>combineFn()).withoutDefaults())
                .apply(View.asSingleton());

        // Get the total count of records processed
        PCollectionView<Long> totalRecordsView = input
                .apply("CountTotalRecords", Count.globally())
                .apply(View.asSingleton());

        // Calculate percentage of records successfully inserted
        PCollection<KV<String, Long>> missingRecords = KeyedPCollectionTuple
                .of("successfulInserts", successfulInsertsView)
                .and("totalRecords", totalRecordsView)
                .apply(CoGroupByKey.create())
                .apply("CalculateMissingPercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, KV<String, Long>>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        long successfulInserts = c.element().getValue().getOnly("successfulInserts", 0L);
                        long totalRecords = c.element().getValue().getOnly("totalRecords", 0L);
                        double percentage = (double) successfulInserts / totalRecords * 100.0;
                        c.output(KV.of("Percentage of records successfully inserted", (long) percentage));
                        long missing = totalRecords - successfulInserts;
                        c.output(KV.of("Missing Records", missing));
                    }
                }));

        // Print the percentage of missing records
        missingRecords.apply("PrintMissingPercentage", ParDo.of(new DoFn<KV<String, Long>, Void>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                System.out.println(c.element().getKey() + ": " + c.element().getValue() + "%");
            }
        }));

        pipeline.run();
    }

    private static TableSchema getSchema() {
        return new TableSchema()
                .setFields(
                        ImmutableList.of(
                                new TableFieldSchema().setName("fullName").setType("STRING")
                                // Add more fields as needed
                        )
                );
    }
}









------------------------



import com.google.api.services.bigquery.model.TableRow;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;
import org.apache.beam.sdk.options.*;
import org.apache.beam.sdk.transforms.*;
import org.apache.beam.sdk.transforms.windowing.*;
import org.apache.beam.sdk.values.*;
import org.joda.time.Duration;

public class DataflowJob {

    public interface MyOptions extends PipelineOptions, StreamingOptions {
        @Description("Pub/Sub subscription path")
        @Validation.Required
        ValueProvider<String> getSubscriptionPath();
        void setSubscriptionPath(ValueProvider<String> value);

        @Description("BigQuery output table")
        @Validation.Required
        ValueProvider<String> getOutputTable();
        void setOutputTable(ValueProvider<String> value);
    }

    public static void main(String[] args) {
        MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class);
        options.setStreaming(true);

        Pipeline pipeline = Pipeline.create(options);

        PCollection<TableRow> input = pipeline
                .apply("ReadFromPubSub", PubsubIO.readAvros(Employee.class)
                        .fromSubscription(options.getSubscriptionPath()))
                .apply("TransformEmployeeData", ParDo.of(new DoFn<Employee, TableRow>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        Employee emp = c.element();
                        // Perform transformation
                        TableRow row = new TableRow();
                        row.set("fullName", emp.getFirstName() + " " + emp.getLastName());
                        // Add more fields as needed
                        c.output(row);
                    }
                }));

        WriteResult result = input.apply("WriteToBigQuery", BigQueryIO.writeTableRows()
                .to(options.getOutputTable())
                .withSchema(getSchema())
                .withFormatFunction(row -> row)
                .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)
                .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED));

        // Retrieve the count of successful inserts
        PCollectionView<Long> successfulInsertsView = result
                .getSuccessfulInserts()
                .apply("CountSuccessfulInserts", Combine.globally(Count.<TableRow>combineFn()).withoutDefaults())
                .apply(View.asSingleton());

        // Get the total count of records processed
        PCollectionView<Long> totalRecordsView = input
                .apply("CountTotalRecords", Count.globally())
                .apply(View.asSingleton());

        // Calculate percentage of records successfully inserted
        PCollection<String> printPercentage = KeyedPCollectionTuple.of("successfulInserts", successfulInsertsView)
                .and("totalRecords", totalRecordsView)
                .apply(CoGroupByKey.create())
                .apply("CalculatePercentage", ParDo.of(new DoFn<KV<Void, CoGbkResult>, String>() {
                    @ProcessElement
                    public void processElement(ProcessContext c) {
                        long successfulInserts = c.element().getValue().getOnly("successfulInserts", 0L);
                        long totalRecords = c.element().getValue().getOnly("totalRecords", 0L);
                        double percentage = (double) successfulInserts / totalRecords * 100.0;
                        c.output("Percentage of records successfully inserted: " + percentage + "%");
                    }
                }));

        printPercentage.apply("PrintPercentage", ParDo.of(new DoFn<String, Void>() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                System.out.println(c.element());
            }
        }));

        pipeline.run();
    }

    private static TableSchema getSchema() {
        return new TableSchema()
                .setFields(
                        ImmutableList.of(
                                new TableFieldSchema().setName("fullName").setType("STRING")
                                // Add more fields as needed
                        )
                );
    }
}
In this modified version:

We retrieve the total count of records processed using Count.globally() and store it in a PCollectionView<Long>.
We then use CoGroupByKey to join the successful inserts count and total records count.
After calculating the percentage of records successfully inserted, we print the result to the console.
Ensure that you have defined the Employee class and replace it with your actual Avro message structure. Also, provide appropriate Pub/Sub subscription and BigQuery table paths in the options.







======================


// Step 3: Recovery Mechanism
// Get the total count of records processed
PCollection<Long> totalRecordsProcessed = inputData.apply(Count.globally());

// Get the count of records successfully inserted into BigQuery
PCollection<Long> successfulInserts = result.getSuccessfulInserts().apply(Count.globally());

// Convert successful inserts count into a singleton view
final PCollectionView<Long> successfulInsertsView = successfulInserts.apply(View.asSingleton());

// Identify missing records
PCollection<Long> missingRecordsCount = totalRecordsProcessed
    .apply("Calculate Missing Records", ParDo.of(new DoFn<Long, Long>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            Long totalRecords = c.element();
            Long successfulInsertsCount = c.sideInput(successfulInsertsView);
            Long missingCount = totalRecords - successfulInsertsCount;
            c.output(missingCount);
        }
    }).withSideInputs(successfulInsertsView));

// Step 4: Logging and Reporting
missingRecordsCount.apply(Sum.longs())
    .apply(ParDo.of(new DoFn<Long, Void>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            Long missingRecordCount = c.element();
            // Log or report the count of missing records
            LOG.info("Missing records count: {}", missingRecordCount);
        }
    }));

// Step 1: Record Tracking
PCollection<YourData> inputData = /* Your input data */;
PCollectionView<Map<String, Boolean>> recordIdsView = inputData
    .apply("Extract Record IDs", ParDo.of(new DoFn<YourData, KV<String, Void>>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            String recordId = /* Extract unique identifier for each record */;
            c.output(KV.of(recordId, null));
        }
    }))
    .apply(View.asMap());

// Step 2: Error Handling (Assuming WriteResult is available)
WriteResult result = processedData.apply(/* Write to BigQuery operation */);
result.getFailedInsertsWithErr()
    .apply(/* Handle failed inserts */);

// Step 3: Recovery Mechanism
// Get the total count of records processed
long totalRecordsProcessed = inputData.apply(Count.globally());
// Get the count of records successfully inserted into BigQuery
PCollectionView<Long> successfulInsertsView = result
    .apply(Count.globally())
    .apply(View.asSingleton());

// Compare counts and identify missing records
PCollection<Long> missingRecordsCount = KeyedPCollectionTuple
    .of(new TupleTag<Void>(), View.<String, Boolean>asMap())
    .and(result.getSuccessfulInserts().apply(Count.globally()))
    .apply(CoGroupByKey.create())
    .apply(ParDo.of(new DoFn<KV<String, CoGbkResult>, Long>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            KV<String, CoGbkResult> element = c.element();
            if (!element.getValue().getOnly(new TupleTag<Void>())) {
                // Record is missing
                c.output(1L);
            }
        }
    }))
    .apply(Sum.longs());

// Step 4: Logging and Reporting
missingRecordsCount.apply(Sum.longs())
    .apply(ParDo.of(new DoFn<Long, Void>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            long missingRecordCount = c.element();
            // Log or report the count of missing records
            LOG.info("Missing records count: {}", missingRecordCount);
        }
    }));

// Additional steps for logging and reporting as needed



gsutil ls -l gs://your-bucket-name/** | awk '{ if ($2 < "'$(date -d '2 days ago' +%Y-%m-%d)'" ) print $NF }' | gsutil -m rm -I

#!/bin/bash

# Get the current time in epoch format
current_time=$(date +%s)

# Calculate the time two days ago in epoch format
two_days_ago=$((current_time - 2 * 24 * 3600))

# List files in the bucket and their metadata
gsutil ls -l gs://your-bucket-name > files.txt

# Parse the output and delete files older than two days
while read -r line; do
    # Extract the modification time of the file
    modification_time=$(echo "$line" | awk '{print $2}')

    # Convert modification time to epoch format
    modification_time_epoch=$(date -d "$modification_time" +%s)

    # Check if the file is older than two days
    if ((modification_time_epoch < two_days_ago)); then
        # Extract the filename from the line
        filename=$(echo "$line" | awk '{print $NF}')

        # Delete the file
        gsutil rm "$filename"
    fi
done < files.txt

# Remove the temporary file
rm files.txt



























To efficiently handle your requirement of consuming the API for address details in batches and then storing the responses in BigQuery, let's outline a solution that uses Google Cloud Platform (GCP) services to manage the workload effectively. This involves making HTTP requests to the external API, handling the response data, and storing it in BigQuery. We can improve on the previous example by incorporating batch processing and more robust error handling.

Recommended Architecture:
Cloud Function (API Consumer): This will handle the task of sending batch requests to the API.
BigQuery: For storing the processed data.
Pub/Sub (optional): For handling asynchronous processing if the response times are long or if the data needs further processing before being stored in BigQuery.
Detailed Steps and Sample Code in Java:
Cloud Function for Making API Requests: This function will trigger API requests to the API, handle the responses, and then store the data in BigQuery.
java
Copy code
import com.google.cloud.functions.HttpFunction;
import com.google.cloud.functions.HttpRequest;
import com.google.cloud.functions.HttpResponse;
import com.google.cloud.bigquery.BigQuery;
import com.google.cloud.bigquery.BigQueryOptions;
import com.google.cloud.bigquery.TableId;
import com.google.cloud.bigquery.InsertAllRequest;
import org.apache.http.client.fluent.Request;
import org.apache.http.entity.ContentType;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import java.util.List;
import java.util.ArrayList;

public class TestApiHandler implements HttpFunction {

    private static final String TEST_API_URL = "https://api.test.example/batch";  // Change to actual TEST API batch endpoint
    private static final BigQuery bigQuery = BigQueryOptions.getDefaultInstance().getService();

    @Override
    public void service(HttpRequest request, HttpResponse response) throws Exception {
        // Assume the request body contains JSON data to be sent to TEST API
        String requestBody = request.getReader().readLine();
        
        // Making the API call to TEST API
        try {
            String apiResponse = Request.Post(TEST_API_URL)
                .bodyString(requestBody, ContentType.APPLICATION_JSON)
                .execute()
                .returnContent()
                .asString();

            // Process the API response and prepare data for BigQuery
            List<Map<String, Object>> rowsToInsert = processDataForBigQuery(apiResponse);
            if (!rowsToInsert.isEmpty()) {
                insertIntoBigQuery(rowsToInsert);
            }
            response.getWriter().write("Data processed and stored successfully.");
        } catch (IOException e) {
            response.setStatusCode(500);
            response.getWriter().write("Failed to call TEST API: " + e.getMessage());
        }
    }

    private List<Map<String, Object>> processDataForBigQuery(String apiResponse) {
        // Parse the API response and prepare data for BigQuery insertion
        List<Map<String, Object>> rows = new ArrayList<>();
        // Logic to parse and structure data from API response
        return rows;
    }

    private void insertIntoBigQuery(List<Map<String, Object>> rows) throws InterruptedException {
        TableId tableId = TableId.of("your_dataset", "your_table");
        InsertAllRequest.Builder builder = InsertAllRequest.newBuilder(tableId);
        for (Map<String, Object> row : rows) {
            builder.addRow(row);
        }
        bigQuery.insertAll(builder.build());
    }
}
Key Elements:
HTTP Post Request: Uses Apache HttpClient (included in the dependency when deploying the function) to send a POST request to the TEST API.
Data Processing: After receiving the response, the data is processed and formatted as needed for insertion into BigQuery.
BigQuery Insertion: The processed data is then batch-inserted into BigQuery.
Deployment and Configuration:
Deploy this function via the Google Cloud Console or using the gcloud command-line tool.
Make sure that your Google Cloud project has enabled the APIs for Cloud Functions and BigQuery.
Configure the necessary permissions for the Cloud Function service account to access BigQuery and to make outbound network calls to the TEST API.
This architecture and code snippet should efficiently handle your requirement of processing address details in batches using the TEST API and storing the responses in BigQuery. Remember to replace placeholders with actual data and enhance security and error handling as per your production standards.
