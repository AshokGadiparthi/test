Perfect — here’s a ready-to-deploy Airflow monitoring utility that tracks all DAGs and tasks, stores live status, runtime, and metadata in BigQuery, and functions like a mini OpenLineage replacement without installing any plugins.

1️⃣ BigQuery Schema


Table Name: airflow_metadata.task_runs

Column Name

Type

Description

run_id

STRING

Unique Airflow DAG run ID

dag_id

STRING

DAG name

task_id

STRING

Task name

execution_date

TIMESTAMP

Scheduled execution date

start_time

TIMESTAMP

Task start time

end_time

TIMESTAMP

Task end time

status

STRING

Task state (running, success, failed)

duration_seconds

FLOAT

Task runtime in seconds

inputs

STRING

JSON list of input datasets/files (optional)

outputs

STRING

JSON list of output datasets/files (optional)

owner

STRING

DAG owner/maintainer

environment

STRING

dev/prod

inserted_at

TIMESTAMP

Record insertion timestamp

2️⃣ Python Utility: airflow_metadata_utils.py
import json
from datetime import datetime
from google.cloud import bigquery

# -----------------------------
# BigQuery config
# -----------------------------
BQ_PROJECT = "my_project"
BQ_DATASET = "airflow_metadata"
BQ_TABLE = "task_runs"

bq_client = bigquery.Client(project=BQ_PROJECT)
table_id = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}"

# -----------------------------
# Function to record task metadata
# -----------------------------
def record_task_metadata(task_instance):
    start_time = task_instance.start_date
    end_time = task_instance.end_date or datetime.utcnow()
    duration = (end_time - start_time).total_seconds() if start_time else None

    # Optional: inputs/outputs from XCom
    inputs = task_instance.xcom_pull(task_ids=task_instance.task_id, key='inputs') or []
    outputs = task_instance.xcom_pull(task_ids=task_instance.task_id, key='outputs') or []

    row = {
        "run_id": task_instance.run_id,
        "dag_id": task_instance.dag_id,
        "task_id": task_instance.task_id,
        "execution_date": task_instance.execution_date,
        "start_time": start_time,
        "end_time": end_time,
        "status": task_instance.state,
        "duration_seconds": duration,
        "inputs": json.dumps(inputs),
        "outputs": json.dumps(outputs),
        "owner": task_instance.owner,
        "environment": "prod",
        "inserted_at": datetime.utcnow()
    }

    errors = bq_client.insert_rows_json(table_id, [row])
    if errors:
        print(f"Failed to insert row: {errors}")
3️⃣ DAG: Global Monitoring for All Tasks


This DAG runs every 5 minutes and records current status of all DAG runs/tasks.

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.models import DagRun, TaskInstance
from airflow.utils.state import State
from airflow.utils.dates import days_ago
from airflow_metadata_utils import record_task_metadata
from datetime import datetime, timedelta

dag = DAG(
    dag_id="global_airflow_monitor",
    start_date=days_ago(1),
    schedule_interval="*/5 * * * *",  # every 5 minutes
    catchup=False,
    max_active_runs=1,
    default_args={"owner": "airflow_monitor"}
)

def monitor_all_dags(**kwargs):
    session = kwargs['session']  # Airflow session
    # Query all active DAG runs
    dag_runs = session.query(DagRun).filter(DagRun.state.in_([State.RUNNING, State.SUCCESS, State.FAILED])).all()

    for dag_run in dag_runs:
        for task_instance in dag_run.get_task_instances():
            record_task_metadata(task_instance)

monitor_task = PythonOperator(
    task_id="monitor_all_dags",
    python_callable=monitor_all_dags,
    provide_context=True,
    dag=dag
)
4️⃣ How It Works
DAG global_airflow_monitor runs every 5 minutes.

Queries all DAG runs and their tasks in Airflow metadata DB.

Calls record_task_metadata() to store:

DAG/task status

Start/end times and duration

Inputs/outputs (from XComs)

Owner/environment

Writes the data to BigQuery immediately.

5️⃣ Features
✅ Tracks all DAGs and tasks automatically.

✅ Stores live status, runtimes, and metadata in BigQuery.

✅ No installation of OpenLineage or plugins required.

✅ Can be extended with data quality, PII flags, or custom facets.

✅ Compatible with Looker Studio / Data Studio dashboards.

6️⃣ Optional Enhancements
Push additional data quality metrics to a separate table.

Track task retries and try_number.

Add alerts (Slack/email) for failures.

Include environment tag (dev, staging, prod) per DAG.

If you want, I can also create a full example DAG for “GCS → BigQuery” with this global monitoring integrated, so that every task automatically logs metadata to BigQuery just like OpenLineage would.



Do you want me to do that next?
