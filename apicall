"""
DAG: Airflow CDP Monitoring - Simplified Console Version
Description: Lightweight monitoring of latest DAG runs (tag='cdp') with root cause and performance insights.
Author: Data Engineering Team
"""

from datetime import datetime, timedelta
import logging
import pandas as pd

from airflow import DAG
from airflow.models import DagBag, Variable
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.utils.state import State


# -----------------------
# Configuration
# -----------------------
DEFAULT_ARGS = {
    'owner': 'data_engineering',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=3),
}

LATEST_RUN_WINDOW_DAYS = int(Variable.get('airflow_monitor_latest_run_window_days', 7))

# -----------------------
# SQL: latest run metadata
# -----------------------
METADATA_QUERY = f"""
WITH latest_runs AS (
    SELECT dag_id, MAX(execution_date) AS latest_execution_date
    FROM dag_run
    WHERE execution_date >= CURRENT_DATE - INTERVAL '{LATEST_RUN_WINDOW_DAYS} days'
    GROUP BY dag_id
)
SELECT 
    dr.dag_id,
    dr.run_id,
    dr.execution_date,
    dr.state AS dag_state,
    dr.start_date,
    dr.end_date,
    ROUND(EXTRACT(EPOCH FROM (dr.end_date - dr.start_date)), 2) AS dag_duration_seconds,
    ti.task_id,
    ti.state AS task_state,
    ti.operator,
    ti.start_date AS task_start_date,
    ti.end_date AS task_end_date,
    ROUND(EXTRACT(EPOCH FROM (ti.end_date - ti.start_date)), 2) AS task_duration_seconds,
    ROUND(EXTRACT(EPOCH FROM (ti.start_date - ti.queued_dttm)), 2) AS queue_wait_seconds,
    ti.try_number,
    ti.max_tries,
    CURRENT_TIMESTAMP AT TIME ZONE 'UTC' AS extracted_at
FROM latest_runs lr
JOIN dag_run dr 
    ON lr.dag_id = dr.dag_id AND lr.latest_execution_date = dr.execution_date
JOIN task_instance ti 
    ON dr.dag_id = ti.dag_id AND dr.run_id = ti.run_id
ORDER BY dr.dag_id, ti.task_id;
"""


# -----------------------
# Metadata Extraction
# -----------------------
def extract_latest_metadata(**context) -> pd.DataFrame:
    logging.info("Connecting to Airflow metadata database (Postgres)...")
    pg_hook = PostgresHook(postgres_conn_id='airflow_db')
    conn = pg_hook.get_conn()
    df = pd.read_sql(METADATA_QUERY, conn)
    conn.close()

    # Filter only DAGs tagged with 'cdp'
    dagbag = DagBag()
    cdp_dags = [d for d in dagbag.dags.values() if 'cdp' in getattr(d, "tags", [])]
    cdp_ids = [d.dag_id for d in cdp_dags]
    df = df[df['dag_id'].isin(cdp_ids)]

    if df.empty:
        logging.warning("No DAGs found with tag='cdp' in the latest runs window.")
    else:
        logging.info(f"Extracted {len(df)} task rows from {df['dag_id'].nunique()} CDP DAGs.")
    context['ti'].xcom_push(key='metadata_df', value=df.to_json())
    return df


# -----------------------
# Root Cause Analysis
# -----------------------
def analyze_failures(**context):
    df_json = context['ti'].xcom_pull(task_ids='extract_metadata', key='metadata_df')
    if not df_json:
        logging.warning("No metadata found for RCA.")
        return

    df = pd.read_json(df_json)
    failed = df[df['task_state'].isin(['failed', 'upstream_failed'])]
    if failed.empty:
        logging.info("✅ No failed tasks found in latest CDP DAG runs.")
        return

    summary = []
    for dag_id in failed['dag_id'].unique():
        dag_failures = failed[failed['dag_id'] == dag_id]
        failed_tasks = dag_failures['task_id'].tolist()
        logging.info(f"🚨 DAG '{dag_id}' has {len(failed_tasks)} failed tasks: {failed_tasks}")

        # classify severity
        for _, row in dag_failures.iterrows():
            severity = "HIGH" if row['try_number'] > 1 else "MEDIUM"
            summary.append({
                'dag_id': dag_id,
                'task_id': row['task_id'],
                'operator': row['operator'],
                'failure_type': row['task_state'],
                'try_number': row['try_number'],
                'severity': severity,
                'duration_seconds': row['task_duration_seconds']
            })
    if summary:
        rca_df = pd.DataFrame(summary)
        logging.info("------ ROOT CAUSE SUMMARY ------")
        logging.info("\n" + rca_df.to_string(index=False))
        context['ti'].xcom_push(key='rca_summary', value=rca_df.to_json())
    else:
        logging.info("✅ All CDP DAG tasks completed successfully.")


# -----------------------
# Executive Console Report
# -----------------------
def print_executive_summary(**context):
    df_json = context['ti'].xcom_pull(task_ids='extract_metadata', key='metadata_df')
    if not df_json:
        logging.warning("No data found for executive summary.")
        return

    df = pd.read_json(df_json)
    total_dags = df['dag_id'].nunique()
    total_tasks = len(df)
    failed_tasks = len(df[df['task_state'].isin(['failed', 'upstream_failed'])])
    success_tasks = len(df[df['task_state'] == 'success'])
    success_rate = (success_tasks / total_tasks * 100) if total_tasks > 0 else 0.0

    logging.info("=" * 100)
    logging.info("AIRFLOW CDP EXECUTIVE SUMMARY (LATEST RUNS)")
    logging.info("=" * 100)
    logging.info(f"  Monitored CDP DAGs: {total_dags}")
    logging.info(f"  Total Tasks (latest run): {total_tasks}")
    logging.info(f"  Successful: {success_tasks}")
    logging.info(f"  Failed: {failed_tasks}")
    logging.info(f"  Success Rate: {success_rate:.1f}%")
    logging.info("-" * 100)

    dag_summary = (
        df.groupby(['dag_id', 'dag_state'])
          .agg(total_tasks=('task_id', 'count'),
               success=('task_state', lambda x: (x == 'success').sum()),
               failed=('task_state', lambda x: (x.isin(['failed', 'upstream_failed'])).sum()))
          .reset_index()
    )

    for _, row in dag_summary.iterrows():
        symbol = "✅" if row['dag_state'] == 'success' else "❌" if row['dag_state'] == 'failed' else "⏳"
        logging.info(f"{symbol} DAG: {row['dag_id']} | Status: {row['dag_state']} | Tasks: {row['total_tasks']} "
                     f"(✓{row['success']} ✗{row['failed']})")

    logging.info("=" * 100)
    logging.info("END OF REPORT")
    logging.info("=" * 100)


# -----------------------
# DAG Definition
# -----------------------
with DAG(
    dag_id='airflow_cdp_monitoring',
    default_args=DEFAULT_ARGS,
    description='Simplified real-time Airflow monitoring for CDP-tagged DAGs',
    schedule_interval='0 * * * *',  # hourly
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['monitoring', 'cdp'],
    max_active_runs=1,
) as dag:

    extract_metadata = PythonOperator(
        task_id='extract_metadata',
        python_callable=extract_latest_metadata,
        provide_context=True,
    )

    root_cause_analysis = PythonOperator(
        task_id='root_cause_analysis',
        python_callable=analyze_failures,
        provide_context=True,
    )

    print_report = PythonOperator(
        task_id='print_executive_summary',
        python_callable=print_executive_summary,
        provide_context=True,
    )

    extract_metadata >> root_cause_analysis >> print_report
