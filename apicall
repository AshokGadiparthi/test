import com.google.api.services.bigquery.model.TableRow;
import com.google.cloud.bigquery.*;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.SimpleFunction;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PDone;
import org.json.JSONObject;

public class SpannerToBigQueryCDC {

    public interface Options extends PipelineOptions {
    }

    public static void main(String[] args) {
        PipelineOptionsFactory.register(Options.class);
        Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);
        Pipeline pipeline = Pipeline.create(options);

        String pubSubSubscription = "projects/your-project/subscriptions/your-subscription";
        String stagingTableSpec = "your-project:your_dataset.stg_customer";
        String dimTableSpec = "your-project:your_dataset.dim_customer";

        pipeline
            .apply("ReadMessages", PubsubIO.readStrings().fromSubscription(pubSubSubscription))
            .apply("ParseMessages", ParDo.of(new ParseMessageFn()))
            .apply("WriteToBigQueryStaging", BigQueryIO.writeTableRows()
                .to(stagingTableSpec)
                .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)
                .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)
                .withSchema(getStagingSchema()))
            .apply("MergeToBigQueryDim", new MergeToBigQuery(dimTableSpec));

        pipeline.run().waitUntilFinish();
    }

    static class ParseMessageFn extends DoFn<String, TableRow> {
        @ProcessElement
        public void processElement(@Element String message, OutputReceiver<TableRow> receiver) {
            JSONObject json = new JSONObject(message);
            String modType = json.getString("modType");
            JSONObject mod = json.getJSONArray("mods").getJSONObject(0);
            JSONObject newValues = new JSONObject(mod.getString("newValuesJson"));
            JSONObject keys = new JSONObject(mod.getString("keysJson"));

            TableRow row = new TableRow()
                .set("cust_id", keys.optString("cust_id", null))
                .set("acct_num", newValues.optString("acct_num", null))
                .set("COL3", newValues.optInt("COL3", -1))
                .set("COL4", newValues.optString("COL4", null))
                .set("COL5", newValues.optInt("COL5", -1))
                .set("modType", modType);

            receiver.output(row);
        }
    }

    private static TableSchema getStagingSchema() {
        return new TableSchema().setFields(
            ImmutableList.of(
                new TableFieldSchema().setName("cust_id").setType("STRING"),
                new TableFieldSchema().setName("acct_num").setType("STRING"),
                new TableFieldSchema().setName("COL3").setType("INTEGER"),
                new TableFieldSchema().setName("COL4").setType("STRING"),
                new TableFieldSchema().setName("COL5").setType("INTEGER"),
                new TableFieldSchema().setName("modType").setType("STRING")
            )
        );
    }

    static class MergeToBigQuery extends PTransform<PCollection<TableRow>, PDone> {
        private final String dimTableSpec;

        MergeToBigQuery(String dimTableSpec) {
            this.dimTableSpec = dimTableSpec;
        }

        @Override
        public PDone expand(PCollection<TableRow> input) {
            input.apply("MergeToBigQueryDim", ParDo.of(new DoFn<TableRow, Void>() {
                @ProcessElement
                public void processElement(ProcessContext c) {
                    TableRow row = c.element();
                    String modType = (String) row.get("modType");

                    String query = "";
                    if ("INSERT".equals(modType)) {
                        query = String.format(
                            "MERGE INTO `%s` a " +
                            "USING " +
                            "(SELECT * FROM `%s`) b " +
                            "ON a.cust_id = b.cust_id " +
                            "WHEN NOT MATCHED THEN " +
                            "INSERT (cust_id, acct_num, COL3, COL4, COL5) " +
                            "VALUES (b.cust_id, b.acct_num, b.COL3, b.COL4, b.COL5)",
                            dimTableSpec, stagingTableSpec
                        );
                    } else if ("UPDATE".equals(modType)) {
                        query = String.format(
                            "MERGE INTO `%s` a " +
                            "USING " +
                            "(SELECT * FROM `%s` WHERE modType = 'UPDATE') b " +
                            "ON a.cust_id = b.cust_id " +
                            "WHEN MATCHED THEN " +
                            "UPDATE SET a.COL3 = b.COL3, a.COL4 = b.COL4, a.COL5 = b.COL5",
                            dimTableSpec, stagingTableSpec
                        );
                    } else if ("DELETE".equals(modType)) {
                        query = String.format(
                            "MERGE INTO `%s` a " +
                            "USING " +
                            "(SELECT * FROM `%s` WHERE modType = 'DELETE') b " +
                            "ON a.cust_id = b.cust_id " +
                            "WHEN MATCHED THEN " +
                            "DELETE WHERE a.cust_id = b.cust_id",
                            dimTableSpec, stagingTableSpec
                        );
                    }

                    try {
                        BigQuery bigquery = BigQueryOptions.getDefaultInstance().getService();
                        QueryJobConfiguration queryConfig = QueryJobConfiguration.newBuilder(query).build();
                        bigquery.query(queryConfig);
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }
            }));
            return PDone.in(input.getPipeline());
        }
    }
}
