import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.FileIO;
import org.apache.beam.sdk.io.fs.MatchResult;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.Create;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.PCollection;

import java.util.ArrayList;
import java.util.List;

public class DataflowBatchWrapper {

    private static final long TARGET_BATCH_SIZE_MB = 700 * 1024 * 1024; // 700 MB in bytes

    public static void main(String[] args) {
        // Create Pipeline options
        MyPipelineOptions options = PipelineOptionsFactory.fromArgs(args).as(MyPipelineOptions.class);
        Pipeline p = Pipeline.create(options);

        // List all input files
        PCollection<MatchResult.Metadata> allFiles = p.apply("MatchFiles", FileIO.match().filepattern("gs://my-bucket/path/*"));

        // Split files into batches
        PCollection<List<String>> fileBatches = allFiles.apply("BatchFiles", ParDo.of(new BatchFilesDoFn()));

        // Process each batch
        fileBatches.apply("ProcessBatches", ParDo.of(new ProcessBatchDoFn(options)));

        p.run().waitUntilFinish();
    }

    static class BatchFilesDoFn extends DoFn<MatchResult.Metadata, List<String>> {
        @ProcessElement
        public void processElement(ProcessContext c) {
            List<MatchResult.Metadata> files = c.element();
            List<String> currentBatch = new ArrayList<>();
            long currentBatchSize = 0;

            for (MatchResult.Metadata file : files) {
                long fileSize = file.resourceId().getCurrentSize();
                if (currentBatchSize + fileSize > TARGET_BATCH_SIZE_MB) {
                    c.output(currentBatch);
                    currentBatch = new ArrayList<>();
                    currentBatchSize = 0;
                }
                currentBatch.add(file.resourceId().toString());
                currentBatchSize += fileSize;
            }

            if (!currentBatch.isEmpty()) {
                c.output(currentBatch);
            }
        }
    }

    static class ProcessBatchDoFn extends DoFn<List<String>, Void> {
        private final MyPipelineOptions options;

        ProcessBatchDoFn(MyPipelineOptions options) {
            this.options = options;
        }

        @ProcessElement
        public void processElement(ProcessContext c) {
            List<String> files = c.element();

            // Trigger a new Dataflow job for this batch of files
            MyPipelineOptions newOptions = PipelineOptionsFactory.fromArgs(options.asArgs()).as(MyPipelineOptions.class);
            Pipeline batchPipeline = Pipeline.create(newOptions);

            // Add your existing Dataflow logic here, using 'files' as input
            batchPipeline.apply("CreateFileList", Create.of(files))
                         // add your custom processing pipeline here
                         .apply(...);

            batchPipeline.run().waitUntilFinish();
        }
    }
}
