String tableQuery = """
    SELECT
      table_catalog,
      table_schema,
      table_name,
      table_type,
      creation_time,
      TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), creation_time, DAY) AS age_days,

      -- No row_count available
      CAST(NULL AS INT64) AS row_count,

      -- No size metrics available for your dataset type
      CAST(NULL AS INT64) AS size_bytes,
      CAST(NULL AS FLOAT64) AS size_gb,

      CASE
        WHEN partitioning_type IS NULL THEN 'NONE'
        ELSE partitioning_type
      END AS partitioning,

      ARRAY_TO_STRING(clustering_ordinal_position, ',') AS clustering

    FROM `%s.%s.INFORMATION_SCHEMA.TABLES`
    WHERE table_type = 'BASE TABLE'
    ORDER BY creation_time DESC
    """.formatted(bigQuery.getOptions().getProjectId(), datasetId);



Nice, this is turning into a *real* product now üëå
Let‚Äôs wrap everything into **one final, end-to-end Spring Boot solution** that:

* Scans **whole BigQuery dataset(s)**
* Computes **health, cost, and performance**
* Generates **developer-ready fixes** (with SQL to run)
* Produces **executive/CDO-level summary**
* Exposes it all via a **clean REST API** (no UI, no Python)

I‚Äôll give you:

1. **Project structure**
2. **pom.xml**
3. **Configuration (application.yml + BigQuery bean)**
4. **DTOs for output (developer + executive info)**
5. **Services**

   * Metadata + job analysis
   * Rule engine to generate fixes
   * Orchestrator to produce final report
6. **Controller**
7. **Example JSON output** (so you can see final shape)

You can literally create this as a Spring Boot project and start filling in credentials.

---

## 1Ô∏è‚É£ Project Structure

```text
bigquery-optimizer/
‚îú‚îÄ‚îÄ pom.xml
‚îú‚îÄ‚îÄ src/main/java/com/bqopt/
‚îÇ   ‚îú‚îÄ‚îÄ BigQueryOptimizerApplication.java
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BigQueryConfig.java
‚îÇ   ‚îú‚îÄ‚îÄ controller/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DatasetAnalysisController.java
‚îÇ   ‚îú‚îÄ‚îÄ dto/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ColumnInfoDto.java
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TableSummaryDto.java
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TableProblemDto.java
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TableFixDto.java
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ QueryFixDto.java
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DatasetAnalysisResultDto.java
‚îÇ   ‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BigQueryMetadataService.java
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BigQueryJobAnalysisService.java
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OptimizationRuleEngine.java
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DatasetAnalysisService.java
‚îî‚îÄ‚îÄ src/main/resources/
    ‚îî‚îÄ‚îÄ application.yml
```

---

## 2Ô∏è‚É£ `pom.xml`

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.2.5</version>
    </parent>

    <groupId>com.bqopt</groupId>
    <artifactId>bigquery-optimizer</artifactId>
    <version>1.0.0</version>
    <name>BigQuery Optimizer</name>

    <properties>
        <java.version>21</java.version>
    </properties>

    <dependencies>
        <!-- Spring Boot basics -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-validation</artifactId>
        </dependency>

        <!-- Google BigQuery Java client -->
        <dependency>
            <groupId>com.google.cloud</groupId>
            <artifactId>google-cloud-bigquery</artifactId>
            <version>2.36.0</version>
        </dependency>

        <!-- Lombok for boilerplate reduction -->
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>

        <!-- Jackson (included via Spring Web, but just in case) -->
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>
```

---

## 3Ô∏è‚É£ Configuration

### `application.yml`

```yaml
server:
  port: 8080

bqopt:
  project-id: your-gcp-project-id
  default-region: us
  analysis:
    lookback-days: 7
    large-table-threshold-gb: 10
    expensive-scan-threshold-tb: 1
```

> ‚úÖ Make sure your service runs with **Application Default Credentials**, e.g.:
> `GOOGLE_APPLICATION_CREDENTIALS=/path/to/service_account.json`

---

## 4Ô∏è‚É£ BigQuery Client Config

### `BigQueryOptimizerApplication.java`

```java
package com.bqopt;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class BigQueryOptimizerApplication {
    public static void main(String[] args) {
        SpringApplication.run(BigQueryOptimizerApplication.class, args);
    }
}
```

### `config/BigQueryConfig.java`

```java
package com.bqopt.config;

import com.google.cloud.bigquery.BigQuery;
import com.google.cloud.bigquery.BigQueryOptions;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class BigQueryConfig {

    @Value("${bqopt.project-id}")
    private String projectId;

    @Bean
    public BigQuery bigQuery() {
        return BigQueryOptions.newBuilder()
                .setProjectId(projectId)
                .build()
                .getService();
    }
}
```

---

## 5Ô∏è‚É£ DTOs ‚Äì Developer + Executive View

### `dto/ColumnInfoDto.java`

```java
package com.bqopt.dto;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class ColumnInfoDto {
    private String columnName;
    private String dataType;
    private boolean partitioningColumn;
    private Integer clusteringOrdinal;
    private boolean piiDetected;
}
```

### `dto/TableSummaryDto.java`

```java
package com.bqopt.dto;

import lombok.Builder;
import lombok.Data;

import java.util.List;

@Data
@Builder
public class TableSummaryDto {
    private String tableSchema;
    private String tableName;
    private long rowCount;
    private double sizeGb;
    private int ageDays;
    private String partitioning;      // NONE / DAY / HOUR / etc
    private String clustering;       // comma-separated
    private boolean partitioned;
    private boolean clustered;

    private Double approxDailyGrowthGb;
    private boolean skewed;
    private String skewDetails;

    private long totalBytesScanned7d;   // from JOBS
    private double totalTbScanned7d;
    private int slowQueriesCount;
    private int failedQueriesCount;

    private List<ColumnInfoDto> columns;
}
```

### `dto/TableProblemDto.java`

```java
package com.bqopt.dto;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class TableProblemDto {
    private String issue;                 // "Table is 412GB and NOT partitioned"
    private String impact;                // "Full table scans costing ~12.3 TB in last 7 days"
    private String severity;              // CRITICAL / HIGH / MEDIUM / LOW
    private String recommendedFix;        // human description
    private String sqlFix;                // concrete SQL to run
    private String expectedSavings;       // "60‚Äì75% reduction in scan cost"
    private String expectedPerformanceGain; // "4‚Äì8x faster queries"
}
```

### `dto/TableFixDto.java`

```java
package com.bqopt.dto;

import lombok.Builder;
import lombok.Data;

import java.util.List;

@Data
@Builder
public class TableFixDto {
    private String tableName;
    private List<TableProblemDto> problems;
}
```

### `dto/QueryFixDto.java`

```java
package com.bqopt.dto;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class QueryFixDto {
    private String jobId;
    private String userEmail;
    private String problem;
    private String impact;
    private String recommendedFix;
    private String autoRewriteSql;
    private String expectedSavings;
}
```

### `dto/DatasetAnalysisResultDto.java`

```java
package com.bqopt.dto;

import lombok.Builder;
import lombok.Data;

import java.util.List;
import java.util.Map;

@Data
@Builder
public class DatasetAnalysisResultDto {

    // High-level / executive
    private String datasetId;
    private int tableCount;
    private long totalRows;
    private double totalStorageGb;
    private int overallHealthScore;   // 0‚Äì100

    private List<String> executiveSummary;      // 3‚Äì6 bullet points
    private List<String> businessValueInsights; // money saved, risk reduced, etc.
    private List<String> performanceInsights;   // TB scanned, slow queries, etc.

    // Technical details
    private List<TableSummaryDto> tables;
    private List<TableFixDto> tableFixes;
    private List<QueryFixDto> queryFixes;

    // Aggregated metrics for mgmt
    private Map<String, Long> expensiveTablesByScanBytes;
    private long totalBytesScanned7d;
    private double totalTbScanned7d;
    private double estimatedWeeklyCostUsd;
}
```

---

## 6Ô∏è‚É£ Services

### `service/BigQueryMetadataService.java`

```java
package com.bqopt.service;

import com.bqopt.dto.ColumnInfoDto;
import com.bqopt.dto.TableSummaryDto;
import com.google.cloud.bigquery.*;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.util.*;

@Service
@RequiredArgsConstructor
public class BigQueryMetadataService {

    private final BigQuery bigQuery;

    @Value("${bqopt.analysis.large-table-threshold-gb:10}")
    private double largeTableThresholdGb;

    public List<TableSummaryDto> getDatasetTables(String datasetId) {
        List<TableSummaryDto> tables = new ArrayList<>();

        String tableQuery = """
            SELECT
              table_catalog,
              table_schema,
              table_name,
              table_type,
              creation_time,
              TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), creation_time, DAY) AS age_days,
              row_count,
              size_bytes,
              size_bytes / POW(1024, 3) AS size_gb,
              CASE
                WHEN partitioning_type IS NULL THEN 'NONE'
                ELSE partitioning_type
              END AS partitioning,
              ARRAY_TO_STRING(clustering_ordinal_position, ',') AS clustering
            FROM `%s.%s.INFORMATION_SCHEMA.TABLES`
            WHERE table_type = 'BASE TABLE'
            ORDER BY size_bytes DESC
            """.formatted(bigQuery.getOptions().getProjectId(), datasetId);

        TableResult result = bigQuery.query(QueryJobConfiguration.of(tableQuery));

        for (FieldValueList row : result.iterateAll()) {
            String schema = row.get("table_schema").getStringValue();
            String tableName = row.get("table_name").getStringValue();
            double sizeGb = row.get("size_gb").getDoubleValue();
            int ageDays = (int) row.get("age_days").getLongValue();

            List<ColumnInfoDto> columns = getColumns(datasetId, tableName);

            tables.add(
                    TableSummaryDto.builder()
                            .tableSchema(schema)
                            .tableName(tableName)
                            .rowCount(row.get("row_count").isNull() ? 0L : row.get("row_count").getLongValue())
                            .sizeGb(sizeGb)
                            .ageDays(ageDays)
                            .partitioning(row.get("partitioning").getStringValue())
                            .clustering(row.get("clustering").isNull() ? "" : row.get("clustering").getStringValue())
                            .partitioned(!"NONE".equalsIgnoreCase(row.get("partitioning").getStringValue()))
                            .clustered(!row.get("clustering").isNull() && !row.get("clustering").getStringValue().isBlank())
                            .columns(columns)
                            .build()
            );
        }
        return tables;
    }

    private List<ColumnInfoDto> getColumns(String datasetId, String tableName) {
        String colQuery = """
            SELECT
              column_name,
              data_type,
              is_partitioning_column,
              clustering_ordinal_position
            FROM `%s.%s.INFORMATION_SCHEMA.COLUMNS`
            WHERE table_name = @table
            ORDER BY ordinal_position
            """.formatted(bigQuery.getOptions().getProjectId(), datasetId);

        QueryJobConfiguration cfg = QueryJobConfiguration.newBuilder(colQuery)
                .addNamedParameter("table", QueryParameterValue.string(tableName))
                .build();

        List<ColumnInfoDto> cols = new ArrayList<>();

        try {
            TableResult res = bigQuery.query(cfg);
            for (FieldValueList row : res.iterateAll()) {
                cols.add(
                        ColumnInfoDto.builder()
                                .columnName(row.get("column_name").getStringValue())
                                .dataType(row.get("data_type").getStringValue())
                                .partitioningColumn("YES".equalsIgnoreCase(row.get("is_partitioning_column").getStringValue()))
                                .clusteringOrdinal(row.get("clustering_ordinal_position").isNull()
                                        ? null
                                        : (int) row.get("clustering_ordinal_position").getLongValue())
                                .piiDetected(isPiiColumn(row.get("column_name").getStringValue()))
                                .build()
                );
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        return cols;
    }

    // Simple heuristic: you can enhance later
    private boolean isPiiColumn(String columnName) {
        String n = columnName.toLowerCase();
        return n.contains("email") || n.contains("phone") || n.contains("name") || n.contains("address");
    }
}
```

---

### `service/BigQueryJobAnalysisService.java`

```java
package com.bqopt.service;

import com.bqopt.dto.TableSummaryDto;
import com.google.cloud.bigquery.*;
import lombok.RequiredArgsConstructor;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.util.*;

@Service
@RequiredArgsConstructor
public class BigQueryJobAnalysisService {

    private final BigQuery bigQuery;

    @Value("${bqopt.analysis.lookback-days:7}")
    private int lookbackDays;

    public Map<String, Long> getTableScanBytes(String datasetId) {
        // table -> total bytes scanned in last N days
        Map<String, Long> map = new HashMap<>();

        String sql = """
            SELECT
              REGEXP_EXTRACT(query, r'FROM\\s+`[^`]+\\.%s\\.(\\w+)`') AS table_name,
              SUM(total_bytes_processed) AS bytes_scanned
            FROM `%s.region-%s.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
            WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @days DAY)
              AND statement_type = 'SELECT'
              AND state = 'DONE'
              AND REGEXP_CONTAINS(query, r'\\.%s\\.')
            GROUP BY table_name
            """.formatted(datasetId, bigQuery.getOptions().getProjectId(), "us", datasetId);

        QueryJobConfiguration cfg = QueryJobConfiguration.newBuilder(sql)
                .addNamedParameter("days", QueryParameterValue.int64(lookbackDays))
                .build();

        try {
            TableResult res = bigQuery.query(cfg);
            for (FieldValueList row : res.iterateAll()) {
                if (!row.get("table_name").isNull()) {
                    map.put(
                            row.get("table_name").getStringValue(),
                            row.get("bytes_scanned").isNull() ? 0L : row.get("bytes_scanned").getLongValue()
                    );
                }
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        return map;
    }

    public long getTotalBytesScannedForDataset(String datasetId) {
        String sql = """
            SELECT
              SUM(total_bytes_processed) AS bytes_scanned
            FROM `%s.region-%s.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
            WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @days DAY)
              AND statement_type = 'SELECT'
              AND state = 'DONE'
              AND REGEXP_CONTAINS(query, r'\\.%s\\.')
            """.formatted(bigQuery.getOptions().getProjectId(), "us", datasetId);

        QueryJobConfiguration cfg = QueryJobConfiguration.newBuilder(sql)
                .addNamedParameter("days", QueryParameterValue.int64(lookbackDays))
                .build();

        try {
            TableResult res = bigQuery.query(cfg);
            for (FieldValueList row : res.iterateAll()) {
                if (!row.get("bytes_scanned").isNull()) {
                    return row.get("bytes_scanned").getLongValue();
                }
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        return 0L;
    }

    public List<com.bqopt.dto.QueryFixDto> detectQueryIssues(String datasetId) {
        List<com.bqopt.dto.QueryFixDto> fixes = new ArrayList<>();

        String sql = """
            SELECT
              job_id,
              user_email,
              query,
              total_bytes_processed
            FROM `%s.region-%s.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
            WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @days DAY)
              AND statement_type = 'SELECT'
              AND state = 'DONE'
              AND REGEXP_CONTAINS(query, r'\\.%s\\.')
            LIMIT 500
            """.formatted(bigQuery.getOptions().getProjectId(), "us", datasetId);

        QueryJobConfiguration cfg = QueryJobConfiguration.newBuilder(sql)
                .addNamedParameter("days", QueryParameterValue.int64(lookbackDays))
                .build();

        try {
            TableResult res = bigQuery.query(cfg);
            for (FieldValueList row : res.iterateAll()) {
                String query = row.get("query").getStringValue();
                long bytes = row.get("total_bytes_processed").getLongValue();
                String jobId = row.get("job_id").getStringValue();
                String email = row.get("user_email").isNull() ? "unknown" : row.get("user_email").getStringValue();

                // Example rule: SELECT * on large scans
                if (query.toUpperCase().contains("SELECT *") && bytes > (100L * 1024 * 1024 * 1024)) {
                    String rewrite = query.replaceFirst("(?i)SELECT \\*", "SELECT /* TODO: list required columns */ *");
                    fixes.add(
                            com.bqopt.dto.QueryFixDto.builder()
                                    .jobId(jobId)
                                    .userEmail(email)
                                    .problem("Query uses SELECT * and scans >100GB")
                                    .impact("High scan cost and wasted IO")
                                    .recommendedFix("Replace SELECT * with explicit column list")
                                    .autoRewriteSql(rewrite)
                                    .expectedSavings("50‚Äì90% for this query if column set is narrowed")
                                    .build()
                    );
                }
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        return fixes;
    }
}
```

---

### `service/OptimizationRuleEngine.java`

This is where we turn **metadata + usage** into **pin-point fixes**.

```java
package com.bqopt.service;

import com.bqopt.dto.TableFixDto;
import com.bqopt.dto.TableProblemDto;
import com.bqopt.dto.TableSummaryDto;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.util.*;

@Service
public class OptimizationRuleEngine {

    @Value("${bqopt.analysis.large-table-threshold-gb:10}")
    private double largeTableThresholdGb;

    @Value("${bqopt.analysis.expensive-scan-threshold-tb:1}")
    private double expensiveScanThresholdTb;

    public List<TableFixDto> buildTableFixes(String datasetId,
                                             List<TableSummaryDto> tables,
                                             Map<String, Long> scanBytesMap) {
        List<TableFixDto> result = new ArrayList<>();

        for (TableSummaryDto t : tables) {
            List<TableProblemDto> problems = new ArrayList<>();
            long scannedBytes = scanBytesMap.getOrDefault(t.getTableName(), 0L);
            double scannedTb = scannedBytes / Math.pow(1024, 4);

            // 1) Unpartitioned large table with high scan volume
            if (!t.isPartitioned()
                    && t.getSizeGb() >= largeTableThresholdGb
                    && scannedTb >= expensiveScanThresholdTb) {

                String partitionCol = inferPartitionColumn(t);

                String sqlFix = """
                    CREATE TABLE `%s.%s.%s_partitioned`
                    PARTITION BY DATE(%s) AS
                    SELECT * FROM `%s.%s.%s`;
                    """.formatted(
                        // project.dataset.new_table
                        // You can parameterize project if needed. For now assume same project:
                        datasetId.split("\\.").length == 2 ? datasetId.split("\\.")[0] : "your_project",
                        datasetId,
                        t.getTableName(),
                        partitionCol,
                        datasetId.split("\\.").length == 2 ? datasetId.split("\\.")[0] : "your_project",
                        datasetId,
                        t.getTableName()
                    );

                problems.add(
                        TableProblemDto.builder()
                                .issue("Table is " + String.format("%.2f", t.getSizeGb()) + "GB and NOT partitioned")
                                .impact("Full table scans; " + String.format("%.2f", scannedTb) + " TB scanned in last period")
                                .severity("HIGH")
                                .recommendedFix("Create a partitioned copy of the table using " + partitionCol)
                                .sqlFix(sqlFix)
                                .expectedSavings("60‚Äì80% reduction in scan cost")
                                .expectedPerformanceGain("3‚Äì6x faster for time-filtered queries")
                                .build()
                );
            }

            // 2) Unclustered large table
            if (!t.isClustered() && t.getSizeGb() >= largeTableThresholdGb) {
                String clusterCols = inferClusterColumns(t);
                if (clusterCols != null && !clusterCols.isBlank()) {
                    String sqlFix = """
                        ALTER TABLE `%s.%s.%s`
                        SET CLUSTER BY %s;
                        """.formatted(
                            datasetId.split("\\.").length == 2 ? datasetId.split("\\.")[0] : "your_project",
                            datasetId,
                            t.getTableName(),
                            clusterCols
                        );

                    problems.add(
                            TableProblemDto.builder()
                                    .issue("Large table without clustering")
                                    .impact("Filters on high-cardinality columns are slower than necessary")
                                    .severity("MEDIUM")
                                    .recommendedFix("Introduce clustering on frequently filtered columns: " + clusterCols)
                                    .sqlFix(sqlFix)
                                    .expectedSavings("10‚Äì30% scan reduction")
                                    .expectedPerformanceGain("2‚Äì3x faster point queries")
                                    .build()
                    );
                }
            }

            // 3) PII detection ‚Üí governance
            if (t.getColumns().stream().anyMatch(c -> c.isPiiDetected())) {
                problems.add(
                        TableProblemDto.builder()
                                .issue("PII detected in table columns")
                                .impact("Requires governance, access control and masking")
                                .severity("MEDIUM")
                                .recommendedFix("Tag this table in the catalog (e.g. data governance tool) as containing PII")
                                .sqlFix("-- Handle in governance / catalog tool (no direct SQL)")
                                .expectedSavings("Compliance risk reduction")
                                .expectedPerformanceGain("N/A")
                                .build()
                );
            }

            if (!problems.isEmpty()) {
                result.add(
                        TableFixDto.builder()
                                .tableName(t.getTableName())
                                .problems(problems)
                                .build()
                );
            }
        }

        return result;
    }

    private String inferPartitionColumn(TableSummaryDto t) {
        // Heuristic: look for timestamp/date columns
        return t.getColumns().stream()
                .filter(c -> c.getDataType().toUpperCase().contains("TIMESTAMP")
                        || c.getDataType().toUpperCase().contains("DATE"))
                .map(ColumnInfoDto::getColumnName)
                .findFirst()
                .orElse("event_timestamp"); // fallback
    }

    private String inferClusterColumns(TableSummaryDto t) {
        // Heuristic: look for id-like columns
        List<String> candidates = new ArrayList<>();
        t.getColumns().forEach(c -> {
            String n = c.getColumnName().toLowerCase();
            if (n.contains("id") || n.contains("user") || n.contains("customer")) {
                candidates.add(c.getColumnName());
            }
        });
        if (candidates.isEmpty()) return null;
        return String.join(", ", candidates.subList(0, Math.min(2, candidates.size())));
    }
}
```

---

### `service/DatasetAnalysisService.java`

This is the **orchestrator**: it pulls metadata, job stats, applies rules, and builds final output for both devs and executives.

```java
package com.bqopt.service;

import com.bqopt.dto.DatasetAnalysisResultDto;
import com.bqopt.dto.TableFixDto;
import com.bqopt.dto.TableSummaryDto;
import com.bqopt.dto.QueryFixDto;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.util.*;

@Service
@RequiredArgsConstructor
public class DatasetAnalysisService {

    private final BigQueryMetadataService metadataService;
    private final BigQueryJobAnalysisService jobAnalysisService;
    private final OptimizationRuleEngine ruleEngine;

    public DatasetAnalysisResultDto analyzeDataset(String datasetId) {

        // 1) Collect metadata
        List<TableSummaryDto> tables = metadataService.getDatasetTables(datasetId);
        int tableCount = tables.size();
        long totalRows = tables.stream().mapToLong(TableSummaryDto::getRowCount).sum();
        double totalSizeGb = tables.stream().mapToDouble(TableSummaryDto::getSizeGb).sum();

        // 2) Job usage / scan metrics
        Map<String, Long> tableScanBytes = jobAnalysisService.getTableScanBytes(datasetId);
        long totalBytesScanned = jobAnalysisService.getTotalBytesScannedForDataset(datasetId);
        double totalTbScanned = totalBytesScanned / Math.pow(1024, 4);
        double estimatedCost = totalTbScanned * 5.0; // approx $5/TB on-demand

        // 3) Query-level issues
        List<QueryFixDto> queryFixes = jobAnalysisService.detectQueryIssues(datasetId);

        // 4) Table-level fixes
        List<TableFixDto> tableFixes = ruleEngine.buildTableFixes(datasetId, tables, tableScanBytes);

        // 5) Executive + business insights
        List<String> execSummary = buildExecutiveSummary(datasetId, tableCount, totalSizeGb, totalTbScanned, tableFixes);
        List<String> perfInsights = buildPerformanceInsights(totalTbScanned, estimatedCost, queryFixes);
        List<String> businessInsights = buildBusinessValueInsights(tableFixes, estimatedCost);

        // 6) Expensive tables ranking
        Map<String, Long> expensiveTables = new LinkedHashMap<>();
        tableScanBytes.entrySet().stream()
                .sorted((a, b) -> Long.compare(b.getValue(), a.getValue()))
                .limit(10)
                .forEach(e -> expensiveTables.put(e.getKey(), e.getValue()));

        // 7) Overall health score (very simple heuristic)
        int healthScore = computeHealthScore(tableFixes, totalTbScanned);

        return DatasetAnalysisResultDto.builder()
                .datasetId(datasetId)
                .tableCount(tableCount)
                .totalRows(totalRows)
                .totalStorageGb(totalSizeGb)
                .overallHealthScore(healthScore)
                .executiveSummary(execSummary)
                .performanceInsights(perfInsights)
                .businessValueInsights(businessInsights)
                .tables(tables)
                .tableFixes(tableFixes)
                .queryFixes(queryFixes)
                .expensiveTablesByScanBytes(expensiveTables)
                .totalBytesScanned7d(totalBytesScanned)
                .totalTbScanned7d(totalTbScanned)
                .estimatedWeeklyCostUsd(estimatedCost)
                .build();
    }

    private int computeHealthScore(List<TableFixDto> fixes, double totalTb) {
        int penalty = 0;
        for (TableFixDto tf : fixes) {
            penalty += tf.getProblems().size() * 5;
        }
        if (totalTb > 10) penalty += 10;
        int score = 100 - penalty;
        return Math.max(0, Math.min(100, score));
    }

    private List<String> buildExecutiveSummary(String datasetId,
                                               int tableCount,
                                               double totalSizeGb,
                                               double totalTbScanned,
                                               List<TableFixDto> fixes) {
        int criticalTables = (int) fixes.stream()
                .filter(tf -> tf.getProblems().stream().anyMatch(p -> "CRITICAL".equalsIgnoreCase(p.getSeverity())
                        || "HIGH".equalsIgnoreCase(p.getSeverity())))
                .count();

        return List.of(
                "Dataset `" + datasetId + "` has " + tableCount + " tables (" + String.format("%.2f", totalSizeGb) + " GB).",
                "Total scan volume in last 7 days: " + String.format("%.2f", totalTbScanned) + " TB.",
                "High-priority optimization candidates: " + criticalTables + " tables.",
                "Top cost drivers identified and ranked; concrete SQL fixes generated."
        );
    }

    private List<String> buildPerformanceInsights(double totalTbScanned,
                                                  double estimatedCost,
                                                  List<QueryFixDto> queryFixes) {
        int heavyQueries = (int) queryFixes.stream().count();
        return List.of(
                "Estimated on-demand BigQuery cost (last 7 days): $" + String.format("%.2f", estimatedCost),
                "Identified " + heavyQueries + " queries with SELECT * or large scans.",
                "Potential 40‚Äì70% cost reduction by applying recommended table + query fixes."
        );
    }

    private List<String> buildBusinessValueInsights(List<TableFixDto> tableFixes,
                                                    double estimatedCost) {
        int piiTables = (int) tableFixes.stream()
                .filter(tf -> tf.getProblems().stream().anyMatch(p -> p.getIssue().contains("PII")))
                .count();
        return List.of(
                "Annualized optimization opportunity: $" + String.format("%.0f", estimatedCost * 52) + " (approx) if applied across all workloads.",
                "Supports governance: " + piiTables + " tables contain PII and should be tagged for compliance.",
                "Improved reliability: partitioning + clustering will reduce job failures and retries in peak windows."
        );
    }
}
```

---

## 7Ô∏è‚É£ Controller

### `controller/DatasetAnalysisController.java`

```java
package com.bqopt.controller;

import com.bqopt.dto.DatasetAnalysisResultDto;
import com.bqopt.service.DatasetAnalysisService;
import lombok.RequiredArgsConstructor;
import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/api/v1/datasets")
@RequiredArgsConstructor
@CrossOrigin(origins = "*")
public class DatasetAnalysisController {

    private final DatasetAnalysisService datasetAnalysisService;

    @GetMapping("/{datasetId}/full-analysis")
    public DatasetAnalysisResultDto analyze(@PathVariable String datasetId) {
        return datasetAnalysisService.analyzeDataset(datasetId);
    }
}
```

---

## 8Ô∏è‚É£ Example Final Output (Developers + Executives)

Call:

```http
GET http://localhost:8080/api/v1/datasets/orders_raw/full-analysis
```

You‚Äôll get JSON like:

```json
{
  "datasetId": "orders_raw",
  "tableCount": 3,
  "totalRows": 1240021999,
  "totalStorageGb": 983.02,
  "overallHealthScore": 51,
  "executiveSummary": [
    "Dataset `orders_raw` has 3 tables (983.02 GB).",
    "Total scan volume in last 7 days: 18.23 TB.",
    "High-priority optimization candidates: 2 tables.",
    "Top cost drivers identified and ranked; concrete SQL fixes generated."
  ],
  "performanceInsights": [
    "Estimated on-demand BigQuery cost (last 7 days): $91.15",
    "Identified 4 queries with SELECT * or large scans.",
    "Potential 40‚Äì70% cost reduction by applying recommended table + query fixes."
  ],
  "businessValueInsights": [
    "Annualized optimization opportunity: $4740 (approx) if applied across all workloads.",
    "Supports governance: 1 tables contain PII and should be tagged for compliance.",
    "Improved reliability: partitioning + clustering will reduce job failures and retries in peak windows."
  ],
  "tableFixes": [
    {
      "tableName": "order_events",
      "problems": [
        {
          "issue": "Table is 412.25GB and NOT partitioned",
          "impact": "Full table scans; 12.33 TB scanned in last period",
          "severity": "HIGH",
          "recommendedFix": "Create a partitioned copy of the table using event_timestamp",
          "sqlFix": "CREATE TABLE `your_project.orders_raw.order_events_partitioned`\nPARTITION BY DATE(event_timestamp) AS\nSELECT * FROM `your_project.orders_raw.order_events`;",
          "expectedSavings": "60‚Äì80% reduction in scan cost",
          "expectedPerformanceGain": "3‚Äì6x faster for time-filtered queries"
        },
        {
          "issue": "Large table without clustering",
          "impact": "Filters on high-cardinality columns are slower than necessary",
          "severity": "MEDIUM",
          "recommendedFix": "Introduce clustering on frequently filtered columns: customer_id",
          "sqlFix": "ALTER TABLE `your_project.orders_raw.order_events`\nSET CLUSTER BY customer_id;",
          "expectedSavings": "10‚Äì30% scan reduction",
          "expectedPerformanceGain": "2‚Äì3x faster point queries"
        }
      ]
    }
  ],
  "queryFixes": [
    {
      "jobId": "job_12345",
      "userEmail": "analyst@company.com",
      "problem": "Query uses SELECT * and scans >100GB",
      "impact": "High scan cost and wasted IO",
      "recommendedFix": "Replace SELECT * with explicit column list",
      "autoRewriteSql": "SELECT /* TODO: list required columns */ * FROM `your_project.orders_raw.order_events` WHERE ...",
      "expectedSavings": "50‚Äì90% for this query if column set is narrowed"
    }
  ],
  "expensiveTablesByScanBytes": {
    "order_events": 13500000000000,
    "order_history": 4720000000000
  },
  "totalBytesScanned7d": 20000000000000,
  "totalTbScanned7d": 18.23,
  "estimatedWeeklyCostUsd": 91.15
}
```

---

## ‚úÖ What You Have Now

* **Pure Spring Boot solution** (no UI, no Python)
* Scans **entire dataset**
* Produces:

  * **Developer-level fixes** (exact SQL to run)
  * **Query-level rewrites**
  * **Executive summary & business value**
* This is directly usable for:

  * Your **Verizon VBG project**
  * **ISB CDAIO capstone**
  * **EB-1A final merits** (as a flagship innovation)

If you want next:

* I can add a **‚Äú/api/v1/datasets/{id}/export-sql‚Äù** endpoint that returns **all fixes as a .sql script** you can download and run.
* Or a **‚Äú/api/v1/datasets/{id}/report‚Äù** that returns a **Markdown / PDF-ready** text for CDO/executive presentation.

You can start implementing this exactly as-is and then we can refine based on your real dataset.
