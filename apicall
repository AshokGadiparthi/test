
// 1) Do your join exactly as before:
final TupleTag<TableRow> tag1 = new TupleTag<>();
final TupleTag<TableRow> tag2 = new TupleTag<>();
PCollection<KV<String,CoGbkResult>> joined = KeyedPCollectionTuple
    .of(tag1, left)
    .and(tag2, right)
    .apply("JoinTables", CoGroupByKey.create());

// 2) Now filter out any key where none of the (r1,r2) pairs pass your filter
String filterCond = dq.getFilterCondition();
String t1alias   = dq.getTable1Alias();   // e.g. "cass"
String t2alias   = dq.getTable2Alias();   // e.g. "cjcm"

// Turn "cass.col6 = 'Y'" into code, for example:
PCollection<KV<String,CoGbkResult>> filtered = joined
  .apply("FilterJoined", ParDo.of(new DoFn<KV<String,CoGbkResult>, KV<String,CoGbkResult>>() {
    @ProcessElement public void process(ProcessContext c) {
      KV<String,CoGbkResult> kv = c.element();
      CoGbkResult res = kv.getValue();
      for (TableRow r1 : res.getAll(tag1)) {
        for (TableRow r2 : res.getAll(tag2)) {
          // your actual condition: upper(cass.line_consumer_ind='Y' OR ... ) OR ...
          String lc = Optional.ofNullable(r1.get("line_consumer_ind"))
                              .map(Object::toString).map(String::toUpperCase)
                              .orElse("");
          String vt = Optional.ofNullable(r1.get("vsn_cust_type_cd"))
                              .map(Object::toString).map(String::toUpperCase)
                              .orElse("");
          String uni = Optional.ofNullable(r1.get("line_UNIVERSE_IND"))
                               .map(Object::toString).map(String::toUpperCase)
                               .orElse("#");

          boolean group1 = (lc.equals("Y") || vt.equals("ME"))
                            && List.of("A","W","F").contains(uni);
          boolean group2 = uni.equals("N");

          if (group1 || group2) {
            // this pair passes → keep the entire KV
            c.output(kv);
            return;      // break out as soon as one (r1,r2) matches
          }
        }
      }
      // if we get here, no (r1,r2) passed → drop it
    }
  })).setCoder(joined.getCoder());  // preserve the same KV coder

// 3) Now run your compare on the filtered PCollection:
PCollection<KV<String,long[]>> comparisons = filtered
  .apply("CompareFields", ParDo.of(new DoFn<KV<String,CoGbkResult>,KV<String,long[]>>() {
    @ProcessElement public void process(ProcessContext c) {
      Map<String,LegacySQLTypeName> types = c.sideInput(typeView);
      CoGbkResult res = c.element().getValue();
      for (TableRow r1 : res.getAll(tag1)) {
        for (TableRow r2 : res.getAll(tag2)) {
          for (String col : columns) {
            Object v1 = r1.get(col), v2 = r2.get(col);
            boolean eq = equalsTyped(v1, v2, types.get(col));
            String rule = col + "_" + ruleType;
            c.output(KV.of(rule, new long[]{ eq?1L:0L, eq?0L:1L }));
          }
        }
      }
    }
  }).withSideInputs(typeView));

// 4) …and then Sum, ToTableRow, Write as before.

==================


// after loading AppConfig and DataQualityConfig…
List<String> joinKeys = dq.getJoinKeys();                // e.g. ["col1","col2","col3"]
List<String> cols     = dq.getComparison().getColumns();  // your 5 (or N) comparison columns

// build the distinct list of all fields you actually need
List<String> allFields = Stream.concat(joinKeys.stream(), cols.stream())
    .distinct()
    .collect(Collectors.toList());

// format them into a SQL SELECT clause: "`col1`, `col2`, ...`"
String fieldList = allFields.stream()
    .map(f -> "`" + f + "`")
    .collect(Collectors.joining(", "));

// now build your query against the view:
String query1 = String.format(
    "SELECT %s FROM `%s`",
    fieldList,
    table1  // e.g. "my_project.my_ds.my_view1"
);

// and same for view2
String query2 = String.format(
    "SELECT %s FROM `%s`",
    fieldList,
    table2
);

// then in your Beam pipeline:
// Read & key view1
PCollection<KV<String,TableRow>> left = p
  .apply("ReadView1",
    BigQueryIO.readTableRows()
      .fromQuery(query1)
      .usingStandardSql()
      .withMethod(BigQueryIO.TypedRead.Method.DIRECT_READ)
  )
  .apply("KeyView1", /* key-by joinKeys as before */ );

// Read & key view2
PCollection<KV<String,TableRow>> right = p
  .apply("ReadView2",
    BigQueryIO.readTableRows()
      .fromQuery(query2)
      .usingStandardSql()
      .withMethod(BigQueryIO.TypedRead.Method.DIRECT_READ)
  )
  .apply("KeyView2", /* key-by joinKeys as before */ );


String[] parts = ref.split("\\.", 3);
    if (parts.length != 3) {
      throw new IllegalArgumentException(
        "Table reference must be project:dataset.table or project.dataset.table, got: " + ref);
    }
    return TableId.of(parts[0], parts[1], parts[2]);

dataQuality:
  version: "1.0"
  description: >-
    Join table1⇄table2 on col1, col2, col3 and run an equality check
    on these 10 columns
  joinKeys: [col1, col2, col3]
  comparison:
    ruleType: equality
    columns: [col1, col2, col3, col4, col5, col6, col7, col8, col9, col10]


package com.trustiq.config;

import java.util.List;

// top-level AppConfig
public class AppConfig {
  private DataQualityConfig dataQuality;
  public DataQualityConfig getDataQuality() { return dataQuality; }
  public void setDataQuality(DataQualityConfig dq) { this.dataQuality = dq; }
}

// dataQuality section
public class DataQualityConfig {
  private String version;
  private String description;
  private List<String> joinKeys;
  private ComparisonConfig comparison;
  public String getVersion() { return version; }
  public void setVersion(String v) { version = v; }
  public String getDescription() { return description; }
  public void setDescription(String d) { description = d; }
  public List<String> getJoinKeys() { return joinKeys; }
  public void setJoinKeys(List<String> j) { joinKeys = j; }
  public ComparisonConfig getComparison() { return comparison; }
  public void setComparison(ComparisonConfig c) { comparison = c; }
}

// comparison block
public class ComparisonConfig {
  private String ruleType;
  private List<String> columns;
  public String getRuleType() { return ruleType; }
  public void setRuleType(String r) { ruleType = r; }
  public List<String> getColumns() { return columns; }
  public void setColumns(List<String> c) { columns = c; }
}


package com.trustiq.config;

import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.dataformat.yaml.YAMLFactory;
import com.google.cloud.storage.*;
import java.io.InputStreamReader;
import java.io.Reader;

public class ConfigLoader {
  private static final ObjectMapper MAPPER =
    new ObjectMapper(new YAMLFactory())
      .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);

  public static AppConfig load(String gcsUri) throws Exception {
    Storage st = StorageOptions.getDefaultInstance().getService();
    Blob b = st.get( BlobId.fromGsUtilUri(gcsUri) );
    try (Reader r = new InputStreamReader(b.getContent())) {
      return MAPPER.readValue(r, AppConfig.class);
    }
  }
}


package com.trustiq.pipeline;

import com.google.api.services.bigquery.model.TableRow;
import com.google.cloud.bigquery.*;
import com.trustiq.config.AppConfig;
import com.trustiq.config.ConfigLoader;
import com.trustiq.config.DataQualityConfig;
import com.trustiq.config.ComparisonConfig;
import org.apache.beam.runners.dataflow.DataflowRunner;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.*;
import org.apache.beam.sdk.options.*;
import org.apache.beam.sdk.transforms.*;
import org.apache.beam.sdk.transforms.join.CoGbkResult;
import org.apache.beam.sdk.transforms.join.KeyedPCollectionTuple;
import org.apache.beam.sdk.transforms.join.TupleTag;
import org.apache.beam.sdk.values.*;
import java.time.Instant;
import java.time.LocalDate;
import java.util.*;
import java.util.stream.Collectors;

public class DataQualityPipeline {

  public interface Options extends PipelineOptions, DataflowPipelineOptions {
    @Description("GCS URI to your app-config.yaml")
    @Validation.Required String getConfigGcsPath();
    void setConfigGcsPath(String v);
  }

  public static void main(String[] args) throws Exception {
    Options opts = PipelineOptionsFactory.fromArgs(args)
                          .withValidation()
                          .as(Options.class);
    opts.setRunner(DataflowRunner.class);

    // 1) Load your YAML config
    AppConfig appCfg = ConfigLoader.load(opts.getConfigGcsPath());
    DataQualityConfig dq = appCfg.getDataQuality();
    List<String> joinKeys  = dq.getJoinKeys();
    ComparisonConfig comp   = dq.getComparison();
    List<String> columns    = comp.getColumns();
    String ruleType         = comp.getRuleType();

    // 2) Define your input/output tables (or pull from AppConfig.sources/sinks)
    String table1 = "PROJECT:DS.table1";
    String table2 = "PROJECT:DS.table2";
    String output = "PROJECT:DS.compare_audit";

    // 3) Fetch BigQuery schema for table1
    BigQuery bq = BigQueryOptions.getDefaultInstance().getService();
    TableId tid = parseTableId(table1);
    Schema schema = bq.getTable(tid).getDefinition().getSchema();

    // 4) Convert schema → List<KV<colName,type>>
    List<KV<String, LegacySQLTypeName>> typeKVs = schema.getFields().stream()
      .map(f -> KV.of(f.getName(), f.getType()))
      .collect(Collectors.toList());

    // 5) Build Beam pipeline
    Pipeline p = Pipeline.create(opts);

    // Side‐input: Map<String,LegacySQLTypeName>
    PCollectionView<Map<String,LegacySQLTypeName>> typeView =
      p.apply("SchemaKV", Create.of(typeKVs))
       .apply("SchemaMap", View.<String,LegacySQLTypeName>asMap());

    // Keying function
    SerializableFunction<TableRow,String> keyFn = row ->
      joinKeys.stream()
        .map(k -> Objects.toString(row.get(k),""))
        .collect(Collectors.joining("|"));

    // Read & key table1
    PCollection<KV<String,TableRow>> left = p
      .apply("ReadTable1", BigQueryIO.readTableRows()
        .from(table1)
        .withMethod(BigQueryIO.TypedRead.Method.DIRECT_READ))
      .apply("KeyTable1", MapElements.into(
        TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptor.of(TableRow.class))
      ).via(r -> KV.of(keyFn.apply(r), r)));

    // Read & key table2
    PCollection<KV<String,TableRow>> right = p
      .apply("ReadTable2", BigQueryIO.readTableRows()
        .from(table2)
        .withMethod(BigQueryIO.TypedRead.Method.DIRECT_READ))
      .apply("KeyTable2", MapElements.into(
        TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptor.of(TableRow.class))
      ).via(r -> KV.of(keyFn.apply(r), r)));

    // Join
    final TupleTag<TableRow> tag1 = new TupleTag<>();
    final TupleTag<TableRow> tag2 = new TupleTag<>();
    PCollection<KV<String,CoGbkResult>> joined = KeyedPCollectionTuple
      .of(tag1, left)
      .and(tag2, right)
      .apply("JoinTables", CoGroupByKey.create());

    // Compare & emit [equals,notEquals]
    PCollection<KV<String,long[]>> comparisons = joined
      .apply("CompareFields", ParDo.of(new DoFn<KV<String,CoGbkResult>,KV<String,long[]>>() {
        @ProcessElement public void process(ProcessContext c) {
          Map<String,LegacySQLTypeName> types = c.sideInput(typeView);
          CoGbkResult res = c.element().getValue();
          for (TableRow r1 : res.getAll(tag1)) {
            for (TableRow r2 : res.getAll(tag2)) {
              for (String col : columns) {
                Object v1 = r1.get(col), v2 = r2.get(col);
                boolean eq = equalsTyped(v1, v2, types.get(col));
                String rule = col + "_" + ruleType;
                c.output(KV.of(rule, new long[]{ eq?1L:0L, eq?0L:1L }));
              }
            }
          }
        }
      }).withSideInputs(typeView));

    // Sum per rule
    PCollection<KV<String,long[]>> summed = comparisons
      .apply("SumCounts", Combine.perKey(new Combine.CombineFn<long[],long[],long[]>() {
        @Override public long[] createAccumulator()             { return new long[]{0,0}; }
        @Override public long[] addInput(long[] acc,long[] in)   { return new long[]{acc[0]+in[0], acc[1]+in[1]}; }
        @Override public long[] mergeAccumulators(Iterable<long[]> it) {
          long e=0,n=0; for(long[] a:it){ e+=a[0]; n+=a[1]; } return new long[]{e,n};
        }
        @Override public long[] extractOutput(long[] acc)        { return acc; }
      }));

    // To TableRow & write to BigQuery
    summed
      .apply("ToTableRow", MapElements.into(TypeDescriptor.of(TableRow.class))
        .via(kv -> {
          long[] ct = kv.getValue();
          return new TableRow()
            .set("rule",           kv.getKey())
            .set("equals_count",   ct[0])
            .set("not_equals_count", ct[1])
            .set("run_date",       Instant.now().toString());
        }))
      .apply("WriteAudit", BigQueryIO.writeTableRows()
        .to(output)
        .withSchema(new TableSchema().setFields(Arrays.asList(
          new TableFieldSchema().setName("rule").setType("STRING"),
          new TableFieldSchema().setName("equals_count").setType("INTEGER"),
          new TableFieldSchema().setName("not_equals_count").setType("INTEGER"),
          new TableFieldSchema().setName("run_date").setType("TIMESTAMP")
        )))
        .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)
        .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE)
      );

    p.run().waitUntilFinish();
  }

  private static TableId parseTableId(String ref) {
    String[] parts = ref.split(":");
    String proj = parts[0];
    String[] dsTbl = parts[1].split("\\.");
    return TableId.of(proj, dsTbl[0], dsTbl[1]);
  }

  private static boolean equalsTyped(Object o1, Object o2, LegacySQLTypeName type) {
    if (o1 == null && o2 == null) return true;
    if (o1 == null || o2 == null) return false;
    try {
      switch(type) {
        case STRING:    return o1.toString().equals(o2.toString());
        case INTEGER:   return ((Number)o1).longValue() == ((Number)o2).longValue();
        case FLOAT:     return Float.compare(
                             ((Number)o1).floatValue(),
                             ((Number)o2).floatValue()) == 0;
        case DOUBLE:    return Double.compare(
                             ((Number)o1).doubleValue(),
                             ((Number)o2).doubleValue()) == 0;
        case BOOLEAN:   return Boolean.parseBoolean(o1.toString())
                              == Boolean.parseBoolean(o2.toString());
        case DATE:      return LocalDate.parse(o1.toString())
                              .equals(LocalDate.parse(o2.toString()));
        case TIMESTAMP: return Instant.parse(o1.toString())
                              .equals(Instant.parse(o2.toString()));
        default:        return o1.toString().equals(o2.toString());
      }
    } catch(Exception e) {
      return false;
    }
  }
}


-- 1) Create (or replace) the audit table
CREATE OR REPLACE TABLE `your_project.your_dataset.compare_audit` (
  rule             STRING,
  equals_count     INT64,
  not_equals_count INT64,
  run_date         TIMESTAMP
)
PARTITION BY DATE(run_date)       -- keep one partition per day
CLUSTER BY rule;                  -- cluster on rule for fast filtering

-- 2) Example: top mismatches in the most recent run
SELECT
  rule,
  equals_count,
  not_equals_count,
  run_date
FROM
  `your_project.your_dataset.compare_audit`
WHERE
  run_date = (
    SELECT MAX(run_date)
    FROM `your_project.your_dataset.compare_audit`
  )
ORDER BY
  not_equals_count DESC
LIMIT 20;

-- 3) Example: trend of mismatch rates over time for one column
SELECT
  DATE(run_date)          AS run_date,
  equals_count,
  not_equals_count,
  SAFE_DIVIDE(not_equals_count, equals_count + not_equals_count) AS mismatch_rate
FROM
  `your_project.your_dataset.compare_audit`
WHERE
  rule = 'col4_equality'  -- or any other rule name
ORDER BY
  run_date;


