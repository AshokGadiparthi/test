/*
 * A production-ready Dataflow pipeline that reads JSON messages from a Pub/Sub topic,
 * aggregates records by composite key (cust_id and mtn) within a 30-second window,
 * and then for each grouped key performs a lookup in Spanner to either update or insert
 * a record with the aggregated payload.
 *
 * Required Maven dependencies (versions may vary):
 *
 *   - org.apache.beam:beam-sdks-java-core
 *   - org.apache.beam:beam-runners-google-cloud-dataflow-java
 *   - com.google.cloud:google-cloud-spanner
 *   - com.google.code.gson:gson
 *
 * Adjust the package name, imports, and dependency versions as necessary.
 */

package com.example.dataflow;

import com.google.api.gax.rpc.ApiException;
import com.google.cloud.Timestamp;
import com.google.cloud.spanner.DatabaseClient;
import com.google.cloud.spanner.DatabaseId;
import com.google.cloud.spanner.Mutation;
import com.google.cloud.spanner.ResultSet;
import com.google.cloud.spanner.Spanner;
import com.google.cloud.spanner.SpannerOptions;
import com.google.cloud.spanner.Statement;
import com.google.gson.Gson;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Collections;
import java.util.UUID;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.options.Description;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.GroupByKey;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.windowing.FixedWindows;
import org.apache.beam.sdk.transforms.windowing.Window;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.TypeDescriptor;
import org.apache.beam.sdk.values.TypeDescriptors;
import org.joda.time.Duration;

/** Main class for the Pub/Sub to Spanner aggregator pipeline. */
public class PubSubToSpannerAggregator {

  /**
   * Pipeline options for the job.
   */
  public interface PubSubToSpannerOptions extends PipelineOptions {
    @Description("Input Pub/Sub topic of the form projects/<PROJECT>/topics/<TOPIC>")
    String getInputTopic();
    void setInputTopic(String value);

    @Description("Google Cloud Spanner Project ID")
    String getSpannerProjectId();
    void setSpannerProjectId(String value);

    @Description("Google Cloud Spanner Instance ID")
    String getSpannerInstanceId();
    void setSpannerInstanceId(String value);

    @Description("Google Cloud Spanner Database ID")
    String getSpannerDatabaseId();
    void setSpannerDatabaseId(String value);

    @Description("Spanner Table Name")
    String getSpannerTable();
    void setSpannerTable(String value);
  }

  /**
   * POJO for incoming transaction messages.
   *
   * Expected JSON format:
   * <pre>
   * {
   *   "cust_id": "customer123",
   *   "mtn": "mtn_value",
   *   "payload": "{ ... }",
   *   "ops_ts": 1672500000000
   * }
   * </pre>
   */
  public static class TransactionMessage {
    // Field names should match your JSON
    String cust_id;
    String mtn;
    String payload;
    long ops_ts; // timestamp in milliseconds

    // Getters (and setters if needed)
    public String getCustId() { return cust_id; }
    public String getMtn() { return mtn; }
    public String getPayload() { return payload; }
    public long getOpsTs() { return ops_ts; }

    /**
     * Utility to parse JSON into a TransactionMessage.
     */
    public static TransactionMessage fromJson(String json) {
      return new Gson().fromJson(json, TransactionMessage.class);
    }
  }

  /**
   * DoFn to aggregate a set of TransactionMessages for the same key.
   *
   * Aggregation logic:
   * - Sort records by their ops_ts.
   * - Concatenate their payloads (separated by a space).
   * - Use the cust_id and mtn from the first (or any) record.
   */
  public static class AggregateMessagesFn
      extends DoFn<KV<String, Iterable<TransactionMessage>>, KV<String, TransactionMessage>> {

    @ProcessElement
    public void processElement(ProcessContext c) {
      String compositeKey = c.element().getKey();
      Iterable<TransactionMessage> messagesIterable = c.element().getValue();

      List<TransactionMessage> messages = new ArrayList<>();
      for (TransactionMessage msg : messagesIterable) {
        messages.add(msg);
      }
      // Sort the messages by ops_ts in ascending order
      Collections.sort(messages, Comparator.comparingLong(TransactionMessage::getOpsTs));

      // Aggregate payloads (simple concatenation; adjust as needed)
      StringBuilder aggregatedPayload = new StringBuilder();
      for (TransactionMessage msg : messages) {
        aggregatedPayload.append(msg.getPayload()).append(" ");
      }

      // Use the cust_id and mtn from the first record (they are all the same for this key)
      TransactionMessage aggregatedMessage = new TransactionMessage();
      aggregatedMessage.cust_id = messages.get(0).getCustId();
      aggregatedMessage.mtn = messages.get(0).getMtn();
      aggregatedMessage.payload = aggregatedPayload.toString().trim();
      // Optionally, use the latest ops_ts
      aggregatedMessage.ops_ts = messages.get(messages.size() - 1).getOpsTs();

      c.output(KV.of(compositeKey, aggregatedMessage));
    }
  }

  /**
   * DoFn that performs a lookup in Spanner (using cust_id and mtn) and then upserts the aggregated
   * record. If the record already exists (i.e. a UUID is found), it uses that UUID; otherwise, a new
   * UUID is generated.
   */
  public static class SpannerUpsertFn extends DoFn<KV<String, TransactionMessage>, Void> {

    private transient Spanner spanner;
    private transient DatabaseClient dbClient;
    private final String projectId;
    private final String instanceId;
    private final String databaseId;
    private final String tableName;

    public SpannerUpsertFn(String projectId, String instanceId, String databaseId, String tableName) {
      this.projectId = projectId;
      this.instanceId = instanceId;
      this.databaseId = databaseId;
      this.tableName = tableName;
    }

    @Setup
    public void setup() {
      // Create the Spanner client.
      SpannerOptions options = SpannerOptions.newBuilder().build();
      spanner = options.getService();
      DatabaseId db = DatabaseId.of(projectId, instanceId, databaseId);
      dbClient = spanner.getDatabaseClient(db);
    }

    @Teardown
    public void teardown() {
      if (spanner != null) {
        spanner.close();
      }
    }

    @ProcessElement
    public void processElement(ProcessContext c) {
      TransactionMessage msg = c.element().getValue();

      // Build a query to check if the record exists.
      // It is assumed that the table has columns: cust_id, mtn, UUID, payload, and ops_ts.
      String query = String.format(
          "SELECT UUID FROM %s WHERE cust_id = @cust_id AND mtn = @mtn", tableName);
      Statement statement =
          Statement.newBuilder(query)
              .bind("cust_id")
              .to(msg.getCustId())
              .bind("mtn")
              .to(msg.getMtn())
              .build();

      String existingUUID = null;
      try (ResultSet resultSet = dbClient.singleUse().executeQuery(statement)) {
        if (resultSet.next()) {
          existingUUID = resultSet.getString("UUID");
        }
      } catch (ApiException e) {
        // Log and rethrow or handle appropriately in production.
        System.err.println("Error during Spanner lookup: " + e.getMessage());
        return;
      }

      // Prepare an upsert mutation. If the record exists, reuse its UUID; otherwise, generate one.
      Mutation.WriteBuilder mutationBuilder =
          Mutation.newInsertOrUpdateBuilder(tableName)
              .set("cust_id")
              .to(msg.getCustId())
              .set("mtn")
              .to(msg.getMtn())
              .set("payload")
              .to(msg.getPayload())
              // Convert ops_ts from milliseconds to Spanner Timestamp (which expects microseconds).
              .set("ops_ts")
              .to(Timestamp.ofTimeMicroseconds(msg.getOpsTs() * 1000L));

      if (existingUUID != null) {
        mutationBuilder.set("UUID").to(existingUUID);
      } else {
        mutationBuilder.set("UUID").to(UUID.randomUUID().toString());
      }

      Mutation mutation = mutationBuilder.build();

      // Write the mutation to Spanner.
      try {
        dbClient.write(Collections.singletonList(mutation));
      } catch (ApiException e) {
        System.err.println("Error during Spanner upsert: " + e.getMessage());
        // In production, consider using retry logic or dead-letter handling.
      }
    }
  }

  public static void main(String[] args) {
    // Parse pipeline options from the command line.
    PubSubToSpannerOptions options =
        PipelineOptionsFactory.fromArgs(args).withValidation().as(PubSubToSpannerOptions.class);
    Pipeline p = Pipeline.create(options);

    p.apply("ReadFromPubSub", PubsubIO.readStrings().fromTopic(options.getInputTopic()))
        // Parse each JSON message into a TransactionMessage POJO.
        .apply("ParseMessages", ParDo.of(new DoFn<String, TransactionMessage>() {
          @ProcessElement
          public void processElement(ProcessContext c) {
            try {
              TransactionMessage msg = TransactionMessage.fromJson(c.element());
              c.output(msg);
            } catch (Exception e) {
              // In production, log the error and optionally output to a dead-letter queue.
              System.err.println("Failed to parse message: " + e.getMessage());
            }
          }
        }))
        // Map each message to a KV pair with a composite key: cust_id + "_" + mtn.
        .apply("MapToKV",
            MapElements.into(
                    TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptor.of(TransactionMessage.class)))
                .via(msg -> KV.of(msg.getCustId() + "_" + msg.getMtn(), msg)))
        // Apply fixed windowing of 30 seconds.
        .apply("WindowIntoFixed", Window.<KV<String, TransactionMessage>>into(FixedWindows.of(Duration.standardSeconds(30))))
        // Group messages by their composite key.
        .apply("GroupByKey", GroupByKey.create())
        // Aggregate the messages for each key (ordering them by ops_ts and concatenating payloads).
        .apply("AggregateMessages", ParDo.of(new AggregateMessagesFn()))
        // For each aggregated record, perform a Spanner lookup and then upsert (insert/update) the record.
        .apply("UpsertToSpanner",
            ParDo.of(new SpannerUpsertFn(
                options.getSpannerProjectId(),
                options.getSpannerInstanceId(),
                options.getSpannerDatabaseId(),
                options.getSpannerTable())));

    p.run().waitUntilFinish();
  }
}
