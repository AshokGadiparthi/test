package com.example.dataflow;

import com.google.api.services.bigquery.model.TableRow;
import com.google.cloud.Timestamp;
import com.google.cloud.spanner.Struct;
import com.google.cloud.spanner.Type;
import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.io.gcp.spanner.SpannerConfig;
import org.apache.beam.sdk.io.gcp.spanner.SpannerIO;
import org.apache.beam.sdk.options.Default;
import org.apache.beam.sdk.options.Description;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;

/**
 * FULL BACKFILL JOB:
 *   Cloud Spanner  ->  BigQuery
 *   Handles Spanner TIMESTAMP -> BigQuery DATETIME conversion.
 */
public class SpannerToBigQueryBatch {

    public interface Options extends DataflowPipelineOptions {

        @Description("Cloud Spanner Instance ID")
        String getSpannerInstanceId();
        void setSpannerInstanceId(String id);

        @Description("Cloud Spanner Database ID")
        String getSpannerDatabaseId();
        void setSpannerDatabaseId(String id);

        @Description("Query to read Spanner data (SELECT * FROM table)")
        String getSpannerQuery();
        void setSpannerQuery(String sql);

        @Description("Output BigQuery table (project:dataset.table)")
        String getOutputTable();
        void setOutputTable(String value);

        @Description("Truncate output table first?")
        @Default.Boolean(true)
        Boolean getTruncateFirst();
        void setTruncateFirst(Boolean value);
    }

    public static void main(String[] args) {

        PipelineOptionsFactory.register(Options.class);
        Options options =
                PipelineOptionsFactory.fromArgs(args)
                        .withValidation()
                        .as(Options.class);

        Pipeline p = Pipeline.create(options);

        // Spanner connection config
        SpannerConfig spannerConfig =
                SpannerConfig.create()
                        .withInstanceId(options.getSpannerInstanceId())
                        .withDatabaseId(options.getSpannerDatabaseId());

        // 1) READ FROM SPANNER (full batch)
        var spannerRows =
                p.apply("ReadSpanner",
                        SpannerIO.read()
                                .withSpannerConfig(spannerConfig)
                                .withQuery(options.getSpannerQuery())
                                .withBatching(true) // critical for large tables
                );

        // 2) CONVERT Struct → TableRow with TYPE HANDLING
        var bqRows =
                spannerRows.apply("ConvertToBQ",
                        ParDo.of(new StructToBQFn()));

        // 3) WRITE TO BIGQUERY
        BigQueryIO.Write<TableRow> write =
                BigQueryIO.writeTableRows()
                        .to(options.getOutputTable())
                        .withMethod(BigQueryIO.WriteMethod.STORAGE_WRITE_API)
                        .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED);

        if (options.getTruncateFirst()) {
            write = write.withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE);
        } else {
            write = write.withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND);
        }

        bqRows.apply("WriteBQ", write);

        p.run().waitUntilFinish();
    }

    /**
     * Converts Spanner Struct → BigQuery TableRow with proper type mapping.
     *
     * Special logic:
     *   TIMESTAMP → BigQuery DATETIME (strip timezone)
     */
    public static class StructToBQFn extends DoFn<Struct, TableRow> {

        @ProcessElement
        public void processElement(ProcessContext c) {
            Struct record = c.element();
            TableRow output = new TableRow();

            for (int i = 0; i < record.getColumnCount(); i++) {
                String col = record.getColumnName(i);
                Type type = record.getColumnType(i);

                if (record.isNull(i)) {
                    output.set(col, null);
                    continue;
                }

                switch (type.getCode()) {
                    case BOOL:
                        output.set(col, record.getBoolean(i));
                        break;

                    case INT64:
                        output.set(col, String.valueOf(record.getLong(i)));
                        break;

                    case FLOAT64:
                        output.set(col, record.getDouble(i));
                        break;

                    case STRING:
                        output.set(col, record.getString(i));
                        break;

                    case NUMERIC:
                        output.set(col, record.getNumeric(i).toString());
                        break;

                    case JSON:
                        output.set(col, record.getJson(i));
                        break;

                    case BYTES:
                        output.set(col, record.getBytes(i).toBase64());
                        break;

                    case DATE:
                        output.set(col, record.getDate(i).toString());
                        break;

                    case TIMESTAMP:
                        // ⭐️⭐️ KEY LOGIC: Convert TIMESTAMP → DATETIME ⭐️⭐️
                        Timestamp ts = record.getTimestamp(i);
                        
                        // Strip timezone; keep date+time (BigQuery DATETIME format)
                        String datetime = ts.toString().replace("Z", "").replace("T", " ");
                        output.set(col, datetime);
                        break;

                    default:
                        // fallback
                        output.set(col, record.getColumnValue(i).toString());
                }
            }

            c.output(output);
        }
    }
}
