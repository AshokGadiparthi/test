def store_airflow_summary_to_bq(df):
    from google.cloud import bigquery
    import pandas as pd
    import datetime

    # --- Metrics ---
    total_dags = df['dag_id'].nunique()
    total_tasks = len(df)
    success = len(df[df['task_state'] == 'success'])
    failed = len(df[df['task_state'] == 'failed'])
    success_rate = (success / total_tasks * 100) if total_tasks > 0 else 0.0

    # --- Create summary DataFrame ---
    summary_df = pd.DataFrame([{
        "report_date": pd.Timestamp.utcnow(),
        "total_dags": total_dags,
        "total_tasks": total_tasks,
        "success_count": success,
        "failed_count": failed,
        "success_rate": round(success_rate, 2),
        "generated_at": pd.Timestamp.utcnow()
    }])

    # --- BigQuery Schema ---
    schema = [
        bigquery.SchemaField("report_date", "TIMESTAMP"),
        bigquery.SchemaField("total_dags", "INTEGER"),
        bigquery.SchemaField("total_tasks", "INTEGER"),
        bigquery.SchemaField("success_count", "INTEGER"),
        bigquery.SchemaField("failed_count", "INTEGER"),
        bigquery.SchemaField("success_rate", "FLOAT"),
        bigquery.SchemaField("generated_at", "TIMESTAMP"),
    ]

    # --- Load to BigQuery ---
    bq_hook = BigQueryHook(gcp_conn_id='google_cloud_default', use_legacy_sql=False)
    client = bq_hook.get_client()
    table_id = f"{client.project}.dbtdatamesh.cdp_monitor_summary"

    job_config = bigquery.LoadJobConfig(
        schema=schema,
        write_disposition="WRITE_APPEND"
    )

    logging.info(f"Storing Airflow summary metrics into {table_id} ...")
    job = client.load_table_from_dataframe(summary_df, table_id, job_config=job_config)
    job.result()
    logging.info("âœ… Airflow summary metrics successfully stored in BigQuery.")
