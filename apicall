Ashok — here is a best-in-class, production-grade solution pattern that will stop repeats, create a single consolidated table, route to the right stakeholders, and add real intelligence (noise suppression + new/ongoing detection + thresholds). I’m giving you a complete blueprint + copy-paste code you can implement directly in your Airflow DAG.

Target Architecture (what you will get)


Tables
RAW (traceability, optional but recommended)

vzw_cdp_prd_tbls.nrt_vzw_cdp_dataflow_error_logs_raw

SUMMARY (one row per unique issue)

vzw_cdp_prd_tbls.nrt_vzw_cdp_dataflow_error_summary

OWNERSHIP / ROUTING (who to alert)

vzw_cdp_prd_tbls.nrt_pipeline_ownership

POLICY (noise suppression + thresholds)

vzw_cdp_prd_tbls.nrt_error_signature_policy



Outputs
A consolidated table (no repeated spam rows)

Alerts sent per team (only relevant items)

“Intelligence” fields: NEW / ONGOING, severity score, category, recommended action

Key Principles (this is why your current code repeats)
Use Airflow data_interval_start/end (no overlap repeats)

Generate error_signature (dedup key)

Suppress noisy signatures (ManagedChannelImpl etc.)

Alert only when:

category is high impact OR

count crosses threshold OR

signature is NEW

Best Categories for your environment (covers your screenshot)
DATA_VALIDATION_STALE (HIGH) ✅

RESOURCE_POOL_STARVATION (HIGH) ✅

REDIS_RETRY (MED-HIGH)

BQ_QUOTA / BQ_RATE_LIMIT (HIGH)

PERMISSION_403 (HIGH)

NETWORK_GRPC (MED)

GRPC_SHUTDOWN_WARNING (NOISE) ✅

SDK_PROGRESS (LOW/NOISE unless job fails) ✅

K8S_RUNTIME_WARNING (LOW)

UNKNOWN (MED)

✅ The Complete Airflow Implementation (copy/paste)


This is a single-DAG approach with 4 tasks:

extract raw logs (within data window)

build summary (dedup + classify + suppress)

load summary to BigQuery (MERGE / upsert)

alert stakeholders



You can keep your existing structure, but replace your logic with this.
import re
import json
import hashlib
from datetime import timezone
import pandas as pd

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.email import send_email

from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import logging as cloud_logging
from google.cloud import bigquery

# ----------------------------
# 1) Normalization + Signature
# ----------------------------
NOISE_PATTERNS = [
    r"managedchannelimpl.*was not shutdown properly",   # gRPC shutdown warning spam
]
HIGH_IMPACT_PATTERNS = [
    r"stale data check condition",
    r"could not get a resource from the pool",
    r"permission|access denied|403",
    r"quota|rate limit|resources exceeded",
]

_noise_re = re.compile(r"(\b\d{4}-\d{2}-\d{2}t\d{2}:\d{2}:\d{2}(\.\d+)?z\b)|(\b\d{6,}\b)", re.IGNORECASE)
_space_re = re.compile(r"\s+")

def normalize_text(s: str, max_len: int = 600) -> str:
    if not s:
        return ""
    s = s.strip()
    s = _noise_re.sub("<N>", s)        # remove timestamps / long ids / big numbers
    s = _space_re.sub(" ", s)
    return s[:max_len].lower()

def build_signature(job_name: str, category: str, message: str, stack: str) -> str:
    first_line = (message or "").split("\n")[0]
    base = f"{job_name}|{category}|{normalize_text(first_line)}|{normalize_text((stack or '').splitlines()[0] if stack else '')}"
    return hashlib.sha256(base.encode("utf-8")).hexdigest()

# ----------------------------
# 2) Classifier (intelligence)
# ----------------------------
def classify(message: str, stack: str) -> str:
    txt = f"{message}\n{stack}".lower()

    if "stale data check condition" in txt:
        return "DATA_VALIDATION_STALE"
    if "could not get a resource from the pool" in txt:
        return "RESOURCE_POOL_STARVATION"
    if "redis operation failed" in txt:
        return "REDIS_RETRY"
    if "sdk failed progress reporting" in txt:
        return "SDK_PROGRESS"
    if "managedchannelimpl" in txt and "was not shutdown properly" in txt:
        return "GRPC_SHUTDOWN_WARNING"
    if "permission" in txt or "access denied" in txt or "403" in txt:
        return "PERMISSION_403"
    if "quota" in txt or "rate limit" in txt:
        return "BQ_QUOTA"
    if "deadline exceeded" in txt or "timeout" in txt:
        return "TIMEOUT"
    if "unavailable" in txt or "connection reset" in txt or "socket" in txt:
        return "NETWORK_GRPC"
    if "container runtime status check" in txt or "skipping pod synchronization" in txt:
        return "K8S_RUNTIME_WARNING"

    return "UNKNOWN"

def is_noise(category: str, message: str) -> bool:
    if category in {"GRPC_SHUTDOWN_WARNING"}:
        return True
    txt = (message or "").lower()
    return any(re.search(p, txt) for p in NOISE_PATTERNS)

def severity_score(category: str, is_new: bool, count: int) -> int:
    base = {
        "DATA_VALIDATION_STALE": 90,
        "RESOURCE_POOL_STARVATION": 85,
        "PERMISSION_403": 80,
        "BQ_QUOTA": 75,
        "REDIS_RETRY": 60,
        "TIMEOUT": 60,
        "NETWORK_GRPC": 55,
        "SDK_PROGRESS": 25,
        "K8S_RUNTIME_WARNING": 20,
        "UNKNOWN": 50,
        "GRPC_SHUTDOWN_WARNING": 0,
    }.get(category, 40)

    # amplify by volume
    vol = 0
    if count >= 200: vol = 20
    elif count >= 50: vol = 10
    elif count >= 10: vol = 5

    return base + (10 if is_new else 0) + vol

def rfc3339(dt):
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    dt = dt.astimezone(timezone.utc)
    return dt.isoformat().replace("+00:00", "Z")

# -------------------------------------------
# 3) Extract -> Summarize -> Load -> Alert
# -------------------------------------------
def extract_and_summarize(**context):
    # Use Airflow data interval to avoid overlap repeats
    window_start = context["data_interval_start"]
    window_end = context["data_interval_end"]
    start_ts = rfc3339(window_start)
    end_ts = rfc3339(window_end)

    # Credentials from Airflow connection
    gcp_hook = GoogleBaseHook(gcp_conn_id="sa-vz-it-hukv-cdwldo-0-app")
    creds = gcp_hook.get_credentials()
    project = "vz-it-pr-hukv-cdwldo-0"

    logging_client = cloud_logging.Client(project=project, credentials=creds)

    log_filter = (
        'resource.type="dataflow_step" '
        'AND severity>=ERROR '
        f'AND timestamp>="{start_ts}" AND timestamp<"{end_ts}"'
    )

    entries = list(logging_client.list_entries(filter_=log_filter))
    rows = []
    seen_insert = set()

    for e in entries:
        ins = getattr(e, "insert_id", None)
        if ins and ins in seen_insert:
            continue
        if ins:
            seen_insert.add(ins)

        resource_labels = getattr(e.resource, "labels", {}) or {}
        job_name = resource_labels.get("job_name") or resource_labels.get("job_id") or "Unknown"
        job_id = resource_labels.get("job_id", "")
        region = resource_labels.get("region", "")

        payload = e.payload
        if isinstance(payload, dict):
            msg = payload.get("message") or payload.get("exception") or str(payload)
            st = payload.get("stack_trace") or payload.get("stacktrace") or ""
        else:
            msg = str(payload)
            st = ""

        if "\n" in msg and not st:
            parts = msg.split("\n", 1)
            msg = parts[0]
            st = parts[1] if len(parts) > 1 else ""

        cat = classify(msg, st)
        sig = build_signature(job_name, cat, msg, st)

        rows.append({
            "job_name": job_name,
            "job_id": job_id,
            "region": region,
            "severity": str(getattr(e, "severity", "ERROR")),
            "category": cat,
            "error_signature": sig,
            "message": (msg or "")[:2000],
            "stack_trace": (st or "")[:8000],
            "log_timestamp": e.timestamp.isoformat() if e.timestamp else None,
            "insert_id": ins,
            "window_start": start_ts,
            "window_end": end_ts,
        })

    df = pd.DataFrame(rows)
    if df.empty:
        context["ti"].xcom_push(key="summary", value="[]")
        return

    df["log_timestamp"] = pd.to_datetime(df["log_timestamp"], errors="coerce")

    # Summarize to one row per (job_name, category, signature)
    summary = (df.groupby(["job_name", "job_id", "region", "category", "error_signature"], as_index=False)
                 .agg(
                    first_seen=("log_timestamp", "min"),
                    last_seen=("log_timestamp", "max"),
                    occurrence_count=("error_signature", "size"),
                    sample_message=("message", "first"),
                    sample_stack=("stack_trace", "first"),
                 ))

    # suppress noise
    summary["is_noise"] = summary.apply(lambda r: is_noise(r["category"], r["sample_message"]), axis=1)

    # Save to XCom
    context["ti"].xcom_push(key="summary", value=summary.to_json(orient="records", date_format="iso"))

def load_summary_to_bq(**context):
    summary_json = context["ti"].xcom_pull(task_ids="extract_and_summarize", key="summary")
    if not summary_json:
        return
    rows = json.loads(summary_json)
    if not rows:
        return

    gcp_hook = GoogleBaseHook(gcp_conn_id="sa-vz-it-hukv-cdwldo-0-app")
    creds = gcp_hook.get_credentials()
    project = "vz-it-pr-hukv-cdwldo-0"
    dataset = "vzw_cdp_prd_tbls"
    table = "nrt_vzw_cdp_dataflow_error_summary"

    bq = bigquery.Client(project=project, credentials=creds)
    table_id = f"{project}.{dataset}.{table}"

    df = pd.DataFrame(rows)
    df["first_seen"] = pd.to_datetime(df["first_seen"], errors="coerce")
    df["last_seen"] = pd.to_datetime(df["last_seen"], errors="coerce")

    # Load into a temp table then MERGE (upsert)
    temp_table_id = f"{project}.{dataset}._tmp_error_summary"
    job = bq.load_table_from_dataframe(df, temp_table_id, job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE"))
    job.result()

    merge_sql = f"""
    MERGE `{table_id}` T
    USING `{temp_table_id}` S
    ON  T.job_name = S.job_name
    AND T.error_signature = S.error_signature
    AND T.category = S.category
    WHEN MATCHED THEN UPDATE SET
      T.last_seen = GREATEST(T.last_seen, S.last_seen),
      T.first_seen = LEAST(T.first_seen, S.first_seen),
      T.occurrence_count = T.occurrence_count + S.occurrence_count,
      T.sample_message = S.sample_message,
      T.sample_stack = S.sample_stack,
      T.is_noise = S.is_noise
    WHEN NOT MATCHED THEN INSERT (
      job_name, job_id, region, category, error_signature,
      first_seen, last_seen, occurrence_count,
      sample_message, sample_stack, is_noise
    ) VALUES (
      S.job_name, S.job_id, S.region, S.category, S.error_signature,
      S.first_seen, S.last_seen, S.occurrence_count,
      S.sample_message, S.sample_stack, S.is_noise
    )
    """
    bq.query(merge_sql).result()

def alert_stakeholders(**context):
    summary_json = context["ti"].xcom_pull(task_ids="extract_and_summarize", key="summary")
    if not summary_json:
        return
    rows = json.loads(summary_json)
    if not rows:
        return

    df = pd.DataFrame(rows)

    # Drop noise and low-signal spam
    df = df[df["is_noise"] == False]

    if df.empty:
        return

    # --- Routing (Phase 1): simple default routing
    # Replace with BQ ownership table later
    DEFAULT_EMAILS = ["data_eng_oncall@verizon.com"]  # put your distro list

    # NEW vs ONGOING (simple version):
    # For now treat everything in window as NEW; in Phase 2, query summary table for last_seen < 7d
    df["is_new"] = True

    df["severity_score"] = df.apply(lambda r: severity_score(r["category"], r["is_new"], int(r["occurrence_count"])), axis=1)

    # Alert thresholds (example)
    def should_alert(r):
        if r["category"] in {"DATA_VALIDATION_STALE", "RESOURCE_POOL_STARVATION", "PERMISSION_403", "BQ_QUOTA"}:
            return True
        return int(r["occurrence_count"]) >= 25

    df = df[df.apply(should_alert, axis=1)]
    if df.empty:
        return

    df = df.sort_values(["severity_score", "occurrence_count"], ascending=[False, False]).head(30)

    # Build HTML table
    html = ["<h3>Dataflow Error Summary (Consolidated)</h3>"]
    html.append("<table border='1' cellpadding='6' cellspacing='0'>")
    html.append("<tr><th>Job</th><th>Category</th><th>Count</th><th>First Seen</th><th>Last Seen</th><th>Sample</th></tr>")
    for _, r in df.iterrows():
        html.append(
            f"<tr>"
            f"<td>{r['job_name']}</td>"
            f"<td>{r['category']}</td>"
            f"<td>{int(r['occurrence_count'])}</td>"
            f"<td>{r['first_seen']}</td>"
            f"<td>{r['last_seen']}</td>"
            f"<td>{str(r['sample_message'])[:200]}</td>"
            f"</tr>"
        )
    html.append("</table>")

    subject = f"[CDP][Dataflow] Consolidated Error Summary - {len(df)} issues"
    send_email(to=DEFAULT_EMAILS, subject=subject, html_content="".join(html))

# ----------------------------
# DAG
# ----------------------------
with DAG(
    dag_id="dataflow_error_intelligence",
    start_date=pd.Timestamp("2025-01-01"),
    schedule_interval="0 * * * *",  # hourly
    catchup=False,
    max_active_runs=1,
    tags=["monitoring", "dataflow", "bigquery", "cdp"]
) as dag:

    t1 = PythonOperator(
        task_id="extract_and_summarize",
        python_callable=extract_and_summarize,
    )

    t2 = PythonOperator(
        task_id="load_summary_to_bq",
        python_callable=load_summary_to_bq,
    )

    t3 = PythonOperator(
        task_id="alert_stakeholders",
        python_callable=alert_stakeholders,
    )

    t1 >> t2 >> t3
2 upgrades that make it “best ever” (Phase 2)


After you deploy the above, do these two upgrades and your solution will become truly executive-grade:



Upgrade A: Ownership Routing Table (so each team gets only their alerts)


Create a BigQuery table:



nrt_pipeline_ownership

job_name_regex (STRING)

team_name (STRING)

emails (ARRAY)



Then in alert_stakeholders(), query that table and route.



Upgrade B: NEW vs ONGOING detection (real intelligence)


In alert_stakeholders():

query summary table for error_signature last_seen in last 7 days

if not found → NEW

if found and still happening → ONGOING

alert NEW immediately, alert ONGOING only if count threshold crosses

What you should do first (fastest path)
Implement the code above (Phase 1)

Create only the summary table in BigQuery (raw table optional)

Let it run for 24 hours

Pull top 50 signatures and mark noise ones into nrt_error_signature_policy



After 1 day you’ll have a clean, consolidated, stakeholder-ready system.

If you want, paste (copy text) just 5 lines of your most repeated exception_desc values, and I will:

tune the classifier to your exact messages,

add perfect suppression rules,

and create the ownership mapping template for your CDP pipelines.

