// Step 1: Record Tracking
PCollection<YourData> inputData = /* Your input data */;
PCollectionView<Map<String, Boolean>> recordIdsView = inputData
    .apply("Extract Record IDs", ParDo.of(new DoFn<YourData, KV<String, Void>>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            String recordId = /* Extract unique identifier for each record */;
            c.output(KV.of(recordId, null));
        }
    }))
    .apply(View.asMap());

// Step 2: Error Handling (Assuming WriteResult is available)
WriteResult result = processedData.apply(/* Write to BigQuery operation */);
result.getFailedInsertsWithErr()
    .apply(/* Handle failed inserts */);

// Step 3: Recovery Mechanism
// Get the total count of records processed
long totalRecordsProcessed = inputData.apply(Count.globally());
// Get the count of records successfully inserted into BigQuery
PCollectionView<Long> successfulInsertsView = result
    .apply(Count.globally())
    .apply(View.asSingleton());

// Compare counts and identify missing records
PCollection<Long> missingRecordsCount = KeyedPCollectionTuple
    .of(new TupleTag<Void>(), View.<String, Boolean>asMap())
    .and(result.getSuccessfulInserts().apply(Count.globally()))
    .apply(CoGroupByKey.create())
    .apply(ParDo.of(new DoFn<KV<String, CoGbkResult>, Long>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            KV<String, CoGbkResult> element = c.element();
            if (!element.getValue().getOnly(new TupleTag<Void>())) {
                // Record is missing
                c.output(1L);
            }
        }
    }))
    .apply(Sum.longs());

// Step 4: Logging and Reporting
missingRecordsCount.apply(Sum.longs())
    .apply(ParDo.of(new DoFn<Long, Void>() {
        @ProcessElement
        public void processElement(ProcessContext c) {
            long missingRecordCount = c.element();
            // Log or report the count of missing records
            LOG.info("Missing records count: {}", missingRecordCount);
        }
    }));

// Additional steps for logging and reporting as needed



gsutil ls -l gs://your-bucket-name/** | awk '{ if ($2 < "'$(date -d '2 days ago' +%Y-%m-%d)'" ) print $NF }' | gsutil -m rm -I

#!/bin/bash

# Get the current time in epoch format
current_time=$(date +%s)

# Calculate the time two days ago in epoch format
two_days_ago=$((current_time - 2 * 24 * 3600))

# List files in the bucket and their metadata
gsutil ls -l gs://your-bucket-name > files.txt

# Parse the output and delete files older than two days
while read -r line; do
    # Extract the modification time of the file
    modification_time=$(echo "$line" | awk '{print $2}')

    # Convert modification time to epoch format
    modification_time_epoch=$(date -d "$modification_time" +%s)

    # Check if the file is older than two days
    if ((modification_time_epoch < two_days_ago)); then
        # Extract the filename from the line
        filename=$(echo "$line" | awk '{print $NF}')

        # Delete the file
        gsutil rm "$filename"
    fi
done < files.txt

# Remove the temporary file
rm files.txt



























To efficiently handle your requirement of consuming the API for address details in batches and then storing the responses in BigQuery, let's outline a solution that uses Google Cloud Platform (GCP) services to manage the workload effectively. This involves making HTTP requests to the external API, handling the response data, and storing it in BigQuery. We can improve on the previous example by incorporating batch processing and more robust error handling.

Recommended Architecture:
Cloud Function (API Consumer): This will handle the task of sending batch requests to the API.
BigQuery: For storing the processed data.
Pub/Sub (optional): For handling asynchronous processing if the response times are long or if the data needs further processing before being stored in BigQuery.
Detailed Steps and Sample Code in Java:
Cloud Function for Making API Requests: This function will trigger API requests to the API, handle the responses, and then store the data in BigQuery.
java
Copy code
import com.google.cloud.functions.HttpFunction;
import com.google.cloud.functions.HttpRequest;
import com.google.cloud.functions.HttpResponse;
import com.google.cloud.bigquery.BigQuery;
import com.google.cloud.bigquery.BigQueryOptions;
import com.google.cloud.bigquery.TableId;
import com.google.cloud.bigquery.InsertAllRequest;
import org.apache.http.client.fluent.Request;
import org.apache.http.entity.ContentType;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import java.util.List;
import java.util.ArrayList;

public class TestApiHandler implements HttpFunction {

    private static final String TEST_API_URL = "https://api.test.example/batch";  // Change to actual TEST API batch endpoint
    private static final BigQuery bigQuery = BigQueryOptions.getDefaultInstance().getService();

    @Override
    public void service(HttpRequest request, HttpResponse response) throws Exception {
        // Assume the request body contains JSON data to be sent to TEST API
        String requestBody = request.getReader().readLine();
        
        // Making the API call to TEST API
        try {
            String apiResponse = Request.Post(TEST_API_URL)
                .bodyString(requestBody, ContentType.APPLICATION_JSON)
                .execute()
                .returnContent()
                .asString();

            // Process the API response and prepare data for BigQuery
            List<Map<String, Object>> rowsToInsert = processDataForBigQuery(apiResponse);
            if (!rowsToInsert.isEmpty()) {
                insertIntoBigQuery(rowsToInsert);
            }
            response.getWriter().write("Data processed and stored successfully.");
        } catch (IOException e) {
            response.setStatusCode(500);
            response.getWriter().write("Failed to call TEST API: " + e.getMessage());
        }
    }

    private List<Map<String, Object>> processDataForBigQuery(String apiResponse) {
        // Parse the API response and prepare data for BigQuery insertion
        List<Map<String, Object>> rows = new ArrayList<>();
        // Logic to parse and structure data from API response
        return rows;
    }

    private void insertIntoBigQuery(List<Map<String, Object>> rows) throws InterruptedException {
        TableId tableId = TableId.of("your_dataset", "your_table");
        InsertAllRequest.Builder builder = InsertAllRequest.newBuilder(tableId);
        for (Map<String, Object> row : rows) {
            builder.addRow(row);
        }
        bigQuery.insertAll(builder.build());
    }
}
Key Elements:
HTTP Post Request: Uses Apache HttpClient (included in the dependency when deploying the function) to send a POST request to the TEST API.
Data Processing: After receiving the response, the data is processed and formatted as needed for insertion into BigQuery.
BigQuery Insertion: The processed data is then batch-inserted into BigQuery.
Deployment and Configuration:
Deploy this function via the Google Cloud Console or using the gcloud command-line tool.
Make sure that your Google Cloud project has enabled the APIs for Cloud Functions and BigQuery.
Configure the necessary permissions for the Cloud Function service account to access BigQuery and to make outbound network calls to the TEST API.
This architecture and code snippet should efficiently handle your requirement of processing address details in batches using the TEST API and storing the responses in BigQuery. Remember to replace placeholders with actual data and enhance security and error handling as per your production standards.
