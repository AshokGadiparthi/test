# GCS://â€¦/config.yaml

dataQuality:
  joinKeys:
    - cust_id
    - acct_num
  comparison:
    columns:
      - name: cust_id
        type: STRING
      - name: acct_num
        type: STRING
      - name: mtn
        type: STRING
    ruleType: "equals"

# Define each stage and its JSON payload root
# You can swap type: PUBSUB for streaming, or BIGQUERY for BQ tables

stages:
  - name: cjcm
    type: BIGQUERY
    table: "PROJECT:DS.payloads_table"
    jsonRoot: "$"
  - name: cassandra
    type: BIGQUERY
    table: "PROJECT:DS.gg_replica"
    jsonRoot: "$.after"
  - name: transformed
    type: BIGQUERY
    table: "PROJECT:DS.payloads_table"
    jsonRoot: "$"

# Output tables
outputs:
  rootCauseTable: "PROJECT:DS.data_quality_root_cause"
  parseErrorTable: "PROJECT:DS.data_quality_parse_errors"
  
  
package com.trustiq.pipeline;

import com.google.api.services.bigquery.model.*;
import com.trustiq.config.*;
import com.trustiq.util.JsonUtil;
import com.trustiq.util.ComparisonUtil;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.beam.runners.dataflow.DataflowRunner;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.options.*;
import org.apache.beam.sdk.transforms.*;
import org.apache.beam.sdk.transforms.join.*;
import org.apache.beam.sdk.values.*;

import java.time.Instant;
import java.util.*;
import java.util.stream.Collectors;

public class DataQualityRootCausePipeline {

  public interface Options extends PipelineOptions, DataflowPipelineOptions {
    @Description("GCS URI to config.yaml")
    @Validation.Required
    String getConfigGcsPath();
    void setConfigGcsPath(String v);
  }

  public static void main(String[] args) {
    Options opts = PipelineOptionsFactory.fromArgs(args)
        .withValidation()
        .as(Options.class);
    opts.setRunner(DataflowRunner.class);

    // 1) Load YAML config
    AppConfig appCfg = ConfigLoader.load(opts.getConfigGcsPath());
    DataQualityConfig dq = appCfg.getDataQuality();
    List<String> joinKeys    = dq.getJoinKeys();
    List<ColumnConfig> cols  = dq.getComparison().getColumns();
    List<StageConfig> stages = appCfg.getStages();

    Pipeline p = Pipeline.create(opts);
    ObjectMapper mapper = new ObjectMapper();

    // Side-output tag for parse errors
    final TupleTag<TableRow> parseErrorTag = new TupleTag<TableRow>("parseErrors"){};

    // 2) Generic Extract + Key DoFn
    class ExtractAndKeyFn extends DoFn<TableRow, KV<String, Payload>> {
      private final StageConfig stage;
      ExtractAndKeyFn(StageConfig stage) { this.stage = stage; }

      @ProcessElement
      public void process(ProcessContext c) {
        TableRow row = c.element();
        try {
          JsonNode root = mapper.readTree((String) row.get("DATA"));
          JsonNode data = JsonUtil.navigate(root, stage.getJsonRoot());
          Map<String,String> vals = JsonUtil.pick(data, joinKeys, cols);
          String key = joinKeys.stream()
            .map(vals::get)
            .collect(Collectors.joining("|"));
          c.output(KV.of(key, new Payload(key, vals)));
        } catch (Exception e) {
          c.output(parseErrorTag, new TableRow()
            .set("stage", stage.getName())
            .set("raw_data", row.get("DATA"))
            .set("error", e.getMessage())
            .set("ts", Instant.now().toString()));
        }
      }
    }

    // 3) Read & extract each stage
    Map<String, PCollection<KV<String,Payload>>> keyed = new LinkedHashMap<>();
    PCollectionList<TableRow> allParseErrors = PCollectionList.empty(p);
    for (StageConfig st : stages) {
      PCollectionTuple split = p
        .apply("Read" + st.getName(), BigQueryIO.readTableRows().from(st.getTable()))
        .apply("Extract" + st.getName(), ParDo.of(new ExtractAndKeyFn(st))
          .withOutputTags(new TupleTag<KV<String,Payload>>() {}, TupleTagList.of(parseErrorTag)));

      keyed.put(st.getName(), split.get(new TupleTag<KV<String,Payload>>() {}));
      allParseErrors = allParseErrors.and(split.get(parseErrorTag));
    }

    // 4) Merge & write parse errors
    allParseErrors
      .apply("FlattenErrors", Flatten.pCollections())
      .apply("WriteParseErrors", BigQueryIO.writeTableRows()
        .to(appCfg.getOutputs().getParseErrorTable())
        .withSchema(new TableSchema().setFields(Arrays.asList(
          new TableFieldSchema().setName("stage").setType("STRING"),
          new TableFieldSchema().setName("raw_data").setType("STRING"),
          new TableFieldSchema().setName("error").setType("STRING"),
          new TableFieldSchema().setName("ts").setType("TIMESTAMP")
        )))
        .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)
        .withWriteDisposition(WriteDisposition.WRITE_APPEND));

    // 5) Build TupleTags map
    Map<String, TupleTag<Payload>> tags = stages.stream()
      .collect(Collectors.toMap(
        StageConfig::getName,
        st -> new TupleTag<Payload>(st.getName()){}));

    // 6) CoGroupByKey
    KeyedPCollectionTuple<String> joinTuple =
      KeyedPCollectionTuple.of(tags.get(stages.get(0).getName()), keyed.get(stages.get(0).getName()));
    for (int i = 1; i < stages.size(); i++) {
      String nm = stages.get(i).getName();
      joinTuple = joinTuple.and(tags.get(nm), keyed.get(nm));
    }
    PCollection<KV<String,CoGbkResult>> joined =
      joinTuple.apply("JoinAll", CoGroupByKey.create());

    // 7) Apply RootCauseAndAuditFn
    PCollectionTuple results = joined.apply("AuditAndRoot", ParDo.of(
      new RootCauseAndAuditFn(stages, cols, tags))
      .withOutputTags(RootCauseAndAuditFn.AUDIT_TAG,
                      TupleTagList.of(RootCauseAndAuditFn.ROOT_CAUSE_TAG)));

    // 8a) Write audit summary
    results.get(RootCauseAndAuditFn.AUDIT_TAG)
      .apply("WriteAudit", BigQueryIO.writeTableRows()
        .to(appCfg.getOutputs().getRootCauseTable() + "_audit")
        .withSchema(RootCauseAndAuditFn.getAuditSchema())
        .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)
        .withWriteDisposition(WriteDisposition.WRITE_TRUNCATE));

    // 8b) Write detailed root causes
    results.get(RootCauseAndAuditFn.ROOT_CAUSE_TAG)
      .apply("WriteRootCauses", BigQueryIO.writeTableRows()
        .to(appCfg.getOutputs().getRootCauseTable())
        .withSchema(RootCauseAndAuditFn.getRootCauseSchema())
        .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)
        .withWriteDisposition(WriteDisposition.WRITE_APPEND));

    p.run().waitUntilFinish();
  }
}

package com.trustiq.pipeline;

import java.io.Serializable;
import java.util.Map;

public class Payload implements Serializable {
  private final String key;
  private final Map<String,String> values;

  public Payload(String key, Map<String,String> values) {
    this.key = key;
    this.values = values;
  }
  public String getKey() { return key; }
  public Map<String,String> getValues() { return values; }
}

package com.trustiq.pipeline;

import com.google.api.services.bigquery.model.*;
import com.trustiq.config.*;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.DoFn.ProcessContext;
import org.apache.beam.sdk.transforms.DoFn.ProcessElement;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.TupleTag;
import org.apache.beam.sdk.transforms.join.CoGbkResult;

import java.time.Instant;
import java.util.*;

public class RootCauseAndAuditFn extends DoFn<KV<String,CoGbkResult>,TableRow> {
  public static final TupleTag<TableRow> AUDIT_TAG = new TupleTag<TableRow>("audit"){};
  public static final TupleTag<TableRow> ROOT_CAUSE_TAG = new TupleTag<TableRow>("rootcause"){};

  private final List<StageConfig> stages;
  private final List<ColumnConfig> columns;
  private final Map<String,TupleTag<Payload>> tags;

  public RootCauseAndAuditFn(List<StageConfig> stages,
                              List<ColumnConfig> columns,
                              Map<String,TupleTag<Payload>> tags) {
    this.stages = stages;
    this.columns = columns;
    this.tags = tags;
  }

  @ProcessElement
  public void process(ProcessContext c) {
    String key = c.element().getKey();
    CoGbkResult res = c.element().getValue();

    for (ColumnConfig col : columns) {
      boolean[] present = new boolean[stages.size()];
      Object[] vals = new Object[stages.size()];

      for (int i = 0; i < stages.size(); i++) {
        Iterator<Payload> it = res.getAll(tags.get(stages.get(i).getName())).iterator();
        if (it.hasNext()) {
          present[i] = true;
          vals[i] = it.next().getValues().get(col.getName());
        }
      }
      long eq=0, ne=0;
      for (int i = 0; i < stages.size()-1; i++) {
        boolean m = ComparisonUtil.equals(vals[i], vals[i+1], col.getType(), col.getTolerance());
        if (m) eq++; else ne++;
      }
      c.output(AUDIT_TAG, new TableRow()
        .set("rule", col.getName())
        .set("equals_count", eq)
        .set("not_equals_count", ne)
        .set("ts", Instant.now().toString()));

      if (ne > 0) {
        String cause = determineRootCause(present, vals, col);
        c.output(ROOT_CAUSE_TAG, new TableRow()
          .set("key", key)
          .set("column", col.getName())
          .set("stages_present", Arrays.toString(present))
          .set("values", Arrays.toString(vals))
          .set("root_cause", cause)
          .set("ts", Instant.now().toString()));
      }
    }
  }

  private String determineRootCause(boolean[] p, Object[] v, ColumnConfig c) {
    if (!p[0]) return "missing_in_" + stages.get(0).getName();
    for (int i = 1; i < p.length; i++) {
      if (!p[i]) return "lost_between_" + stages.get(i-1).getName() +
                              "_and_" + stages.get(i).getName();
      if (!ComparisonUtil.equals(v[i-1], v[i], c.getType(), c.getTolerance()))
        return "value_mismatch_between_" + stages.get(i-1).getName() +
               "_and_" + stages.get(i).getName();
    }
    return "unknown";
  }

  public static TableSchema getAuditSchema() {
    return new TableSchema().setFields(Arrays.asList(
      new TableFieldSchema().setName("rule").setType("STRING"),
      new TableFieldSchema().setName("equals_count").setType("INTEGER"),
      new TableFieldSchema().setName("not_equals_count").setType("INTEGER"),
      new TableFieldSchema().setName("ts").setType("TIMESTAMP")
    ));
  }

  public static TableSchema getRootCauseSchema() {
    return new TableSchema().setFields(Arrays.asList(
      new TableFieldSchema().setName("key").setType("STRING"),
      new TableFieldSchema().setName("column").setType("STRING"),
      new TableFieldSchema().setName("stages_present").setType("STRING"),
      new TableFieldSchema().setName("values").setType("STRING"),
      new TableFieldSchema().setName("root_cause").setType("STRING"),
      new TableFieldSchema().setName("ts").setType("TIMESTAMP")
    ));
  }
}

