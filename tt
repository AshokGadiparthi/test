from airflow import DAG
from airflow.providers.google.cloud.operators.dataflow import DataflowTemplateOperator
from airflow.utils.dates import days_ago

# Define default args
default_args = {
    'owner': 'your_name',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'retries': 1,
}

# Define the DAG
with DAG(
    dag_id='bq_to_gcs_and_dataflow_to_redis',
    default_args=default_args,
    schedule_interval='@daily',  # or your preferred schedule
    catchup=False,
) as dag:

    # Task 2: Run Dataflow job using Classic Template to load data from GCS to Redis
    start_dataflow_job = DataflowTemplateOperator(
        task_id='start_dataflow_job',
        template='gs://your-bucket-name/templates/your-template-name',  # Path to the classic template
        parameters={
            'inputFile': 'gs://your-bucket-name/data/{{ ds }}/output_file.json',
            'outputRedisHost': 'your_redis_host',
            'outputRedisPort': 'your_redis_port',
            'otherParameter': 'value'
        },
        environment={
            'tempLocation': 'gs://your-bucket-name/temp/',
            'zone': 'your-gcp-zone',
            'network': 'your-network',
            'subnetwork': 'regions/your-region/subnetworks/your-subnetwork',
            'serviceAccountEmail': 'your-service-account@gcp-project.iam.gserviceaccount.com',
            'kmsKeyName': 'projects/your-project/locations/your-location/keyRings/your-keyring/cryptoKeys/your-key',
            'workerMachineType': 'n1-standard-4',
            'additionalExperiments': ['enable_streaming_engine']
        },
        project_id='your-gcp-project-id',
        location='your-region',  # e.g., 'us-central1'
    )

    # Other tasks and dependencies can be defined here

    # Task 1 >> start_dataflow_job

