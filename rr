package com.example.meshcdc.service;

import com.google.api.services.dataflow.Dataflow;
import com.google.api.services.dataflow.model.FlexTemplateRuntimeEnvironment;
import com.google.api.services.dataflow.model.LaunchFlexTemplateParameter;
import com.google.api.services.dataflow.model.LaunchFlexTemplateRequest;
import com.google.api.services.dataflow.model.LaunchFlexTemplateResponse;
import com.google.cloud.storage.Blob;
import com.google.cloud.storage.Storage;
import com.google.cloud.storage.StorageOptions;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.util.*;

@Service
public class DataflowLauncherService {

  private final Dataflow df;
  private final Storage storage;
  private final String projectId;
  private final String region;
  private final String flexSpecGcs;
  private final String saEmail;
  private final String tempLocation;
  private final ObjectMapper M = new ObjectMapper();

  public DataflowLauncherService(
      Dataflow df,
      @Value("${mesh.project-id}") String projectId,
      @Value("${mesh.region}") String region,
      @Value("${mesh.dataflow.containerSpecGcs}") String flexSpecGcs,
      @Value("${mesh.dataflow.service-account}") String saEmail,
      @Value("${mesh.dataflow.temp-location}") String tempLocation
  ) {
    this.df = df;
    this.storage = StorageOptions.getDefaultInstance().getService();
    this.projectId = projectId;
    this.region = region;
    this.flexSpecGcs = flexSpecGcs;
    this.saEmail = saEmail;
    this.tempLocation = tempLocation;
  }

  public String launchReplication(String jobName, String configLocation, String metricsTable, String dlqPath) throws IOException {
    // Build String-only parameters (Dataflow API requires Map<String, String>)
    Map<String, String> reqParams = new LinkedHashMap<>();
    putIfNonEmpty(reqParams, "configLocation", configLocation);
    putIfNonEmpty(reqParams, "metricsTable",  metricsTable);
    putIfNonEmpty(reqParams, "dlqPath",       dlqPath);

    // Normalize common typos/synonyms (e.g., dlqparg -> dlqPath)
    reqParams = normalizeKeys(reqParams);

    // Validate against names declared in containerSpec.json (throws if mismatch)
    validateAgainstSpec(reqParams.keySet());

    FlexTemplateRuntimeEnvironment env = new FlexTemplateRuntimeEnvironment()
        .setServiceAccountEmail(saEmail)
        .setTempLocation(tempLocation)
        .setEnableStreamingEngine(true)
        .setAdditionalUserLabels(Map.of("product", jobName));

    LaunchFlexTemplateParameter param = new LaunchFlexTemplateParameter()
        .setJobName(jobName)
        .setContainerSpecGcsPath(flexSpecGcs)
        .setEnvironment(env)
        .setParameters(reqParams);

    LaunchFlexTemplateRequest req = new LaunchFlexTemplateRequest().setLaunchParameter(param);
    LaunchFlexTemplateResponse resp = df.projects().locations().flexTemplates()
        .launch(projectId, region, req).execute();
    return resp.getJob() != null ? resp.getJob().getId() : null;
  }

  private static void putIfNonEmpty(Map<String, String> m, String k, String v) {
    if (v != null && !v.isBlank()) m.put(k, v);
  }

  private Map<String,String> normalizeKeys(Map<String,String> in) {
    Map<String,String> synonyms = Map.of(
        "dlqparg", "dlqPath",       // your typo seen in error
        "dlq",     "dlqPath",
        "metrics_table", "metricsTable",
        "config_path",  "configLocation"
    );
    Map<String,String> out = new LinkedHashMap<>();
    for (var e : in.entrySet()) {
      String k = e.getKey();
      if (synonyms.containsKey(k)) k = synonyms.get(k);
      out.put(k, e.getValue());
    }
    return out;
  }

  private void validateAgainstSpec(Set<String> keys) {
    Set<String> allowed = fetchSpecParameterNames();
    List<String> bad = new ArrayList<>();
    for (String k : keys) if (!allowed.contains(k)) bad.add(k);
    if (!bad.isEmpty()) {
      throw new IllegalArgumentException(
          "containerSpec " + flexSpecGcs + " does not declare parameters " + bad +
          ". Allowed: " + allowed);
    }
  }

  private Set<String> fetchSpecParameterNames() {
    // Parse gs://bucket/path.json
    String gs = flexSpecGcs.replaceFirst("^gs://", "");
    int slash = gs.indexOf('/');
    if (slash < 0) throw new IllegalArgumentException("Invalid GCS URI for containerSpec: " + flexSpecGcs);
    String bucket = gs.substring(0, slash);
    String object = gs.substring(slash + 1);

    Blob blob = storage.get(bucket, object);
    if (blob == null) throw new IllegalArgumentException("containerSpec not found: " + flexSpecGcs);
    String json = new String(blob.getContent(), StandardCharsets.UTF_8);

    try {
      JsonNode root = M.readTree(json);
      JsonNode params = root.get("parameters");
      if (params == null || !params.isArray()) return Set.of(); // no parameters declared
      Set<String> names = new LinkedHashSet<>();
      for (JsonNode p : params) {
        JsonNode n = p.get("name");
        if (n != null && n.isTextual()) names.add(n.asText());
      }
      return names;
    } catch (Exception e) {
      throw new RuntimeException("Failed to parse containerSpec.json: " + e.getMessage(), e);
    }
  }
}



==================

CREATE TABLE Orders (
  order_id   STRING(36) NOT NULL,
  user_id    STRING(36) NOT NULL,
  status     STRING(32),
  amount     NUMERIC,
  email      STRING(320),
  created_at TIMESTAMP NOT NULL OPTIONS (allow_commit_timestamp=true),
  updated_at TIMESTAMP NOT NULL OPTIONS (allow_commit_timestamp=true),
) PRIMARY KEY (order_id);

=======================================

awesome — let’s test your backend end-to-end, one tiny step at a time. copy–paste in order.

# 0) One-time prerequisites

```bash
# pick your project/region
export GCP_PROJECT=<your-project-id>
export REGION=us-central1

# config bucket for per-product JSON + DLQ + temp
export CFG_BUCKET=${GCP_PROJECT}-mesh-config

# BigQuery location (must match your target dataset)
export BQ_LOCATION=US

# Service account the Dataflow job will run as
export SA_EMAIL=dp-mesh@${GCP_PROJECT}.iam.gserviceaccount.com

# Flex Template container spec in GCS (we'll upload in step 4)
export FLEX_SPEC_GCS=gs://${CFG_BUCKET}/flex/containerSpec.json
```

# 1) Start MariaDB and the app

```bash
docker compose -f docker-compose.mariadb.yml up -d
```

Check DB is up (optional):

```bash
docker exec -it $(docker ps -qf name=db) mysql -umesh -pmesh -e "SELECT 1;"
```

Run the app (from project root):

```bash
mvn -q spring-boot:run
```

You should see `Started MeshCdcApplication` on port `8080`.

# 2) Authenticate to Google & set project

```bash
gcloud auth application-default login
gcloud config set project $GCP_PROJECT
```

# 3) Enable required APIs

```bash
gcloud services enable \
  dataflow.googleapis.com storage.googleapis.com bigquery.googleapis.com \
  spanner.googleapis.com
```

# 4) Create the config bucket & upload Flex containerSpec

```bash
gsutil mb -l $REGION gs://$CFG_BUCKET  # OK if it already exists

# Create a containerSpec.json that points at YOUR Dataflow Docker image
cat > containerSpec.json <<'JSON'
{
  "image": "gcr.io/<your-project>/<your-repo>/spanner-bq-repl:1.0.0",
  "sdk_info": {"language": "JAVA"},
  "metadata": {"name": "Spanner→BQ CDC"},
  "default_environment": {"num_workers": 3},
  "parameters": [
    {"name": "configLocation", "label": "Replication config GCS path", "is_optional": false},
    {"name": "metricsTable",  "label": "BQ sidecar table", "is_optional": false},
    {"name": "dlqPath",       "label": "GCS DLQ path",    "is_optional": true}
  ]
}
JSON
gsutil cp containerSpec.json $FLEX_SPEC_GCS
```

# 5) Create BQ dataset + sidecar table

```bash
bq --location=$BQ_LOCATION mk -d ${GCP_PROJECT}:dp_orders || true
bq query --location=$BQ_LOCATION --use_legacy_sql=false "
CREATE TABLE IF NOT EXISTS \`${GCP_PROJECT}.dp_orders.mesh_sync_status` (
  src_database STRING, src_table STRING,
  last_seen_commit_ts TIMESTAMP, last_applied_commit_ts TIMESTAMP,
  backlog_seconds INT64,
  window_start TIMESTAMP, window_end TIMESTAMP,
  inserts INT64, updates INT64, deletes INT64,
  error_count INT64, last_error STRING,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
) PARTITION BY DATE(updated_at);"
```

# 6) Create the Dataflow service account & grant roles

```bash
gcloud iam service-accounts create dp-mesh --display-name="Data Mesh SA" || true

gcloud projects add-iam-policy-binding $GCP_PROJECT \
  --member="serviceAccount:$SA_EMAIL" --role="roles/dataflow.worker"
gcloud projects add-iam-policy-binding $GCP_PROJECT \
  --member="serviceAccount:$SA_EMAIL" --role="roles/dataflow.viewer"
gcloud projects add-iam-policy-binding $GCP_PROJECT \
  --member="serviceAccount:$SA_EMAIL" --role="roles/storage.admin"
gcloud projects add-iam-policy-binding $GCP_PROJECT \
  --member="serviceAccount:$SA_EMAIL" --role="roles/bigquery.admin"
gcloud projects add-iam-policy-binding $GCP_PROJECT \
  --member="serviceAccount:$SA_EMAIL" --role="roles/spanner.viewer"
gcloud projects add-iam-policy-binding $GCP_PROJECT \
  --member="serviceAccount:$SA_EMAIL" --role="roles/spanner.databaseReader"
```

# 7) Ensure Spanner change stream exists

If you already have it, skip. Otherwise:

```bash
# Example: create change stream on all tables
gcloud spanner databases ddl update <INSTANCE> --database=<DB> \
  --ddl="CREATE CHANGE STREAM cs_orders FOR ALL OPTIONS (value_capture_type = 'NEW_VALUES')"
```

# 8) Create a product

```bash
PID=$(curl -sX POST localhost:8080/v2/products \
 -H 'content-type: application/json' \
 -d '{"productKey":"orders-transactions","name":"Orders","ownerEmail":"you@x.com"}' \
 | jq -r .id)
echo "Product ID: $PID"
```

# 9) Post the contract (edit the source & sink fields to your env)

```bash
cat > contract.json <<JSON
{
  "version": "v1",
  "replication": {
    "source": {
      "type": "SPANNER_CHANGE_STREAM",
      "instance": "<INSTANCE>",
      "database": "<DB>",
      "changeStream": "cs_orders",
      "tables": [
        {"name": "orders", "pk": ["order_id"], "commitTsCol": "_commit_ts"}
      ]
    },
    "sink": {
      "type": "BIGQUERY",
      "project": "$GCP_PROJECT",
      "dataset": "dp_orders",
      "table": "curated",
      "partition": {"field": "_commit_ts", "type": "DAY"},
      "cluster": ["order_id","status"],
      "writeMode": "UPSERT",
      "dlq": {"type": "GCS", "bucket": "$CFG_BUCKET"}
    },
    "privacy": {"policyTags": {"email": "PII_EMAIL"}},
    "slos": {"freshnessMinutes": 15, "completenessPct": 99.5},
    "sidecar": {"statusTable": "$GCP_PROJECT.dp_orders.mesh_sync_status"},
    "backfill": {"enabled": false}
  }
}
JSON

curl -sX PUT localhost:8080/v2/products/$PID/contract \
  -H 'content-type: application/json' --data-binary @contract.json | jq
```

# 10) Provision the product surface

```bash
curl -sX POST localhost:8080/v2/products/$PID/provision:apply | jq
```

Confirm the per-product config exists:

```bash
gsutil cat gs://$CFG_BUCKET/replications/$PID.json | jq
```

# 11) Create the pipeline & deploy (launch Dataflow Flex job)

```bash
PLID=$(curl -sX POST localhost:8080/v2/pipelines \
 -H 'content-type: application/json' \
 -d '{"productId":"'"$PID"'","name":"orders_spanner_to_bq","template":"DATAFLOW_FLEX"}' \
 | jq -r .id)
echo "Pipeline ID: $PLID"

curl -sX POST localhost:8080/v2/pipelines/$PLID:deploy | jq
```

You’ll get a JSON with `"jobId"`. Validate the job exists:

```bash
gcloud dataflow jobs list --region=$REGION --filter="id:<paste-jobId>"
```

# 12) Check health

```bash
curl -s localhost:8080/v1/observability/products/$PID/health | jq
```

You’ll see fields like:

```json
{"status":"GREEN|YELLOW|RED","freshnessMinutes":..., "systemLagMinutes":..., "backlogSeconds":...}
```

# 13) Quick functional checks

**A) Verify target table exists**

```bash
bq ls ${GCP_PROJECT}:dp_orders
```

**B) Verify sidecar updates**

```bash
bq query --use_legacy_sql=false "
SELECT * FROM \`${GCP_PROJECT}.dp_orders.mesh_sync_status\`
ORDER BY updated_at DESC LIMIT 10"
```

**C) Access workflow (optional)**

```bash
REQ=$(curl -sX POST localhost:8080/v2/access/requests \
 -H 'content-type: application/json' \
 -d '{"productId":"'"$PID"'","datasetRef":"'"$GCP_PROJECT"'.dp_orders.curated","columns":["email"],"principal":"user:x@example.com","purpose":"analysis","durationDays":7}' \
 | jq -r .id)

curl -sX POST "localhost:8080/v2/access/requests/$REQ:approve?by=steward@example.com" | jq
```

# 14) Common issues & exact fixes

* **`PERMISSION_DENIED` on Flex launch**
  Ensure `$SA_EMAIL` has the roles in step 6, and your app has ADC (`gcloud auth application-default login`).
* **`Not found: containerSpec`**
  Check `mesh.dataflow.containerSpecGcs` in `application.yml` points to your `gs://…/containerSpec.json` uploaded in step 4.
* **`BigQuery location mismatch`**
  Your dataset must be in `$BQ_LOCATION` and the `ProvisionerService` uses that location. Set `BQ_LOCATION` correctly before creating the dataset.
* **No rows in sidecar**
  Your Dataflow pipeline must implement the **heartbeat** that writes to `mesh_sync_status`. (I included the writer pattern in the canvas; wire it into your job.)
* **Spanner stream errors**
  Confirm the change stream name exists on the right DB and that the Dataflow SA has `roles/spanner.databaseReader` and `roles/spanner.viewer`.

# 15) What “100% Data Mesh” looks like here (acceptance)

* **Contract-first**: your posted contract drives provisioning + runtime config (GCS JSON).
* **Self-service**: users create products/pipelines via API (your Angular can call these).
* **Observability**: deterministic SLOs via sidecar + Dataflow metrics exposed at `/health`.
* **Governance**: access request flow included; add Policy Tags/column IAM next if needed.
* **Change mgmt**: contracts are versioned in DB; you can add an approvals endpoint later.

---

If any step returns an error or a response you didn’t expect, paste the exact command and output — I’ll pinpoint and fix it.
